<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>spark源码-BlockManager</title>
    <url>/2020/01/11/spark/spark%E6%BA%90%E7%A0%81-BlockManager/</url>
    <content><![CDATA[<p>每台节点（driver and executors）的block的总管理者，主要功能就是在本地或者远程的store中(堆内内存、磁盘、堆外内存) put、get、 block。</p>
<p>作为总管理者，BlockManager 依赖众多对象。</p>
<a id="more"></a>
<h1>重要成员</h1>
<p>从成员可以大体4类：RPC、传输、内存磁盘的 get or put（本文侧重）、shuffle</p>
<p><strong>RPC 相关</strong>（todo）</p>
<ul>
<li>master：BlockManagerMaster，RPC相关</li>
<li>rpcEnv</li>
<li>blockManagerId：该blockManager的Id，分布式系统</li>
<li>slaveEndpoint</li>
</ul>
<p><strong>块及其传输相关</strong>（todo）</p>
<ul>
<li>serializerManager：序列化管理者</li>
<li>mapOutputTracker：跟踪 shuffle write 的输出</li>
<li>blockTransferService：块传输服务（netty）</li>
<li>securityManager：块加密</li>
<li>remoteReadNioBufferConversion：<code>spark.network.remoteReadNioBufferConversion</code></li>
<li>futureExecutionContext</li>
<li>maxFailuresBeforeLocationRefresh</li>
<li>remoteBlockTempFileManager</li>
</ul>
<p><strong>内存磁盘相关</strong>：</p>
<ul>
<li>
<p><strong>memoryManager</strong></p>
</li>
<li>
<p><strong>memoryStore</strong></p>
</li>
<li>
<p><strong>diskBlockManager</strong></p>
</li>
<li>
<p><strong>diskStore</strong></p>
</li>
<li>
<p><strong>blockInfoManager</strong></p>
</li>
</ul>
<p><strong>shuffle</strong> 相关</p>
<ul>
<li>
<p>shuffleManager：使用的shuffleManager</p>
</li>
<li>
<p>externalShuffleServiceEnabled：<code>spark.shuffle.service.enabled</code>，是否使用外部排序(todo)，默认false</p>
</li>
<li>
<p>externalShuffleServicePort</p>
</li>
<li>
<p>shuffleServerId</p>
</li>
<li>
<p>shuffleClient：</p>
</li>
</ul>
<h1>重点功能</h1>
<h2 id="initialize"><a class="header-anchor" href="#initialize">¶</a>initialize</h2>
<p>只有掉用了initialize方法，该BlockManager才可用，看看就行，pass</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(appId: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  blockTransferService.init(<span class="keyword">this</span>)</span><br><span class="line">  shuffleClient.init(appId)</span><br><span class="line"></span><br><span class="line">  blockReplicationPolicy = &#123;</span><br><span class="line">    <span class="keyword">val</span> priorityClass = conf.get(</span><br><span class="line">      <span class="string">"spark.storage.replication.policy"</span>, classOf[<span class="type">RandomBlockReplicationPolicy</span>].getName)</span><br><span class="line">    <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(priorityClass)</span><br><span class="line">    <span class="keyword">val</span> ret = clazz.newInstance.asInstanceOf[<span class="type">BlockReplicationPolicy</span>]</span><br><span class="line">    logInfo(<span class="string">s"Using <span class="subst">$priorityClass</span> for block replication policy"</span>)</span><br><span class="line">    ret</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> id =</span><br><span class="line">    <span class="type">BlockManagerId</span>(executorId, blockTransferService.hostName, blockTransferService.port, <span class="type">None</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> idFromMaster = master.registerBlockManager(</span><br><span class="line">    id,</span><br><span class="line">    maxOnHeapMemory,</span><br><span class="line">    maxOffHeapMemory,</span><br><span class="line">    slaveEndpoint)</span><br><span class="line"></span><br><span class="line">  blockManagerId = <span class="keyword">if</span> (idFromMaster != <span class="literal">null</span>) idFromMaster <span class="keyword">else</span> id</span><br><span class="line"></span><br><span class="line">  shuffleServerId = <span class="keyword">if</span> (externalShuffleServiceEnabled) &#123;</span><br><span class="line">    logInfo(<span class="string">s"external shuffle service port = <span class="subst">$externalShuffleServicePort</span>"</span>)</span><br><span class="line">    <span class="type">BlockManagerId</span>(executorId, blockTransferService.hostName, externalShuffleServicePort)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    blockManagerId</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Register Executors' configuration with the local shuffle service, if one should exist.</span></span><br><span class="line">  <span class="keyword">if</span> (externalShuffleServiceEnabled &amp;&amp; !blockManagerId.isDriver) &#123;</span><br><span class="line">    registerWithExternalShuffleServer()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  logInfo(<span class="string">s"Initialized BlockManager: <span class="subst">$blockManagerId</span>"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Put"><a class="header-anchor" href="#Put">¶</a>Put</h2>
<p>block就是文件，所以只要有spark计算有写文件操作（内存和磁盘），那么就要用到 put。</p>
<p>内容较多，我的学习目标是：先<strong>思考清楚spark计算流程</strong>，至于BlockManager内部弄明白<strong>调用流程和store的选择</strong>即可。能力有限，下面的方式并不是全部。</p>
<h3 id="RDD-cache"><a class="header-anchor" href="#RDD-cache">¶</a>RDD cache</h3>
<p>RDD 的缓存 参见我写的spark cache：第一次执行时，自然是存。并且存完后也要读，因为该函数就是要get，这里就不提读步骤。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManager</span></span><br><span class="line"><span class="type">ShuffleMapTask</span>/<span class="type">ResultTask</span>.runTask  -&gt; <span class="type">RDD</span>.iterator -&gt; <span class="type">RDD</span>.getOrCompute -&gt; getOrElseUpdate</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager内部</span></span><br><span class="line">getOrElseUpdate -&gt; doPutIterator -&gt; doput</span><br><span class="line"></span><br><span class="line"><span class="comment">// 细看下doPutIterator的流程</span></span><br><span class="line">level.useMemory  -<span class="literal">true</span>--&gt; level.deserialized -<span class="literal">true</span>--&gt; memoryStore.putIteratorAsValues 注意</span><br><span class="line">                                             -<span class="literal">false</span>-&gt; memoryStore.putIteratorAsBytes  注意</span><br><span class="line">                 -<span class="literal">false</span>-&gt; diskStore.put</span><br><span class="line"><span class="comment">//注意： 当level为内存和磁盘时，会先存内存，内存不足再存磁盘</span></span><br></pre></td></tr></table></figure>
<h3 id="Broadcast"><a class="header-anchor" href="#Broadcast">¶</a>Broadcast</h3>
<p>广播变量的存储</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManager</span></span><br><span class="line"><span class="type">TorrentBroadcast</span>.writeBlocks -&gt; putSingle</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager内部</span></span><br><span class="line">putSingle -&gt; putIterator -&gt; doPutIterator -&gt; doput</span><br><span class="line"></span><br><span class="line"><span class="comment">// doPutIterator 同理</span></span><br></pre></td></tr></table></figure>
<h3 id="RDD-block-Replication"><a class="header-anchor" href="#RDD-block-Replication">¶</a>RDD block Replication</h3>
<blockquote>
<p>当RDD的storage level中的_replication大于1时，BlockManager需要将block数据发到另一个远程结点以备份，此时BlockManager会向远程结点发送UploadBlock消息，远程结点在收到该消息后会申请存储内存以存放收到的block数据。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManage，分2类</span></span><br><span class="line"><span class="type">NettyBlockRpcServer</span>.receive -&gt; <span class="keyword">match</span> uploadBlock -&gt; putBlockData</span><br><span class="line"><span class="type">NettyBlockRpcServer</span>.receiveStream -&gt; putBlockDataAsStream</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager内部</span></span><br><span class="line">putBlockData -&gt; putBytes -&gt; doPutBytes -&gt; doPut</span><br><span class="line">putBlockDataAsStream -&gt; 吧啦吧啦的看不懂 -&gt; putBytes -&gt; doPutBytes -&gt; doPut</span><br><span class="line"></span><br><span class="line"><span class="comment">// doPutBytes 存储逻辑和 doPutIterator 基本一样</span></span><br><span class="line"><span class="comment">// 区别是 doPutIterator 输入是 Iterator[T]；doPutBytes 输入是 ChunkedByteBuffer，它需要先反序列化</span></span><br><span class="line"><span class="comment">// 分析：doPutIterator 是缓存步骤计算得到的自然是java对象； doPutBytes 是节点传自然是序列化的</span></span><br></pre></td></tr></table></figure>
<h3 id="Shuffle-相关"><a class="header-anchor" href="#Shuffle-相关">¶</a>Shuffle 相关</h3>
<p>Shuffle时有许多写磁盘的操作，它们都是使用了一个专门的直接将数据写入磁盘的类：</p>
<p>DiskWriter：<code>DiskBlockObjectWriter</code>，而它通过BlockManaged得到。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getDiskWriter</span></span>(</span><br><span class="line">  blockId: <span class="type">BlockId</span>,</span><br><span class="line">  file: <span class="type">File</span>,</span><br><span class="line">  serializerInstance: <span class="type">SerializerInstance</span>,</span><br><span class="line">  bufferSize: <span class="type">Int</span>,</span><br><span class="line">  writeMetrics: <span class="type">ShuffleWriteMetrics</span>): <span class="type">DiskBlockObjectWriter</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> syncWrites = conf.getBoolean(<span class="string">"spark.shuffle.sync"</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">DiskBlockObjectWriter</span>(file, serializerManager, serializerInstance, bufferSize,</span><br><span class="line">                            syncWrites, writeMetrics, blockId)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 通用流程</span></span><br><span class="line"><span class="comment">// 1. 得到 DiskWriter</span></span><br><span class="line"><span class="keyword">val</span> writer = blockManager.getDiskWriter(xxx)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 将数据写入DiskWriter</span></span><br><span class="line">writer.write(xxx)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 提交并返回偏移和大小</span></span><br><span class="line"><span class="keyword">val</span> fileSegment = writer.commitAndGet()</span><br></pre></td></tr></table></figure>
<p>主要有下列情况</p>
<ul>
<li>
<p><strong>ShuffleSpill</strong>：BlockId：<code>temp_shuffle_ + “randomUUID”</code></p>
</li>
<li>
<p><strong>BypassShuffle</strong><a href="https://zouxxyy.github.io/2019/11/30/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BBypassMergeSortShuffleWriter/" target="_blank" rel="noopener">前3步</a>：BlockId：<code>temp_shuffle_ + “randomUUID”</code></p>
</li>
<li>
<p><strong>ShuffleWrite</strong>生成的合并大文件：BlockId：<code>&quot;shuffle_&quot; + shuffleId + &quot;_&quot; + mapId + &quot;_&quot; + reduceId</code></p>
</li>
</ul>
<h2 id="get"><a class="header-anchor" href="#get">¶</a>get</h2>
<p>有写就有读，与写不同的是：<strong>写只能写本地，读可以读本地和远程</strong>，这个我想应该很好理解。</p>
<h3 id="RDD-cache-v2"><a class="header-anchor" href="#RDD-cache-v2">¶</a>RDD cache</h3>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManager</span></span><br><span class="line"><span class="type">ShuffleMapTask</span>/<span class="type">ResultTask</span>.runTask  -&gt; <span class="type">RDD</span>.iterator -&gt; <span class="type">RDD</span>.getOrCompute -&gt; getOrElseUpdate</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager内部</span></span><br><span class="line">getOrElseUpdate -&gt; get -&gt; getLocalValues</span><br><span class="line">											 -&gt; getRemoteValues -&gt; getRemoteBytes -&gt; 反序列</span><br><span class="line"></span><br><span class="line"><span class="comment">// 细看下 getLocalValues 的流程</span></span><br><span class="line">level.useMemory  -<span class="literal">true</span>--&gt; level.deserialized -<span class="literal">true</span>--&gt; memoryStore.getValues </span><br><span class="line">                                             -<span class="literal">false</span>-&gt; memoryStore.getBytes  -&gt; 反序列</span><br><span class="line">                 -<span class="literal">false</span>-&gt; level.useDisk      -<span class="literal">true</span>--&gt; diskStore.getBytes    -&gt; 反序列</span><br></pre></td></tr></table></figure>
<h3 id="Broadcast-v2"><a class="header-anchor" href="#Broadcast-v2">¶</a>Broadcast</h3>
<p>广播变量的读取比较简单</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManager</span></span><br><span class="line"><span class="type">TorrentBroadcast</span>.readBroadcastBlock -&gt; getLocalValues</span><br></pre></td></tr></table></figure>
<h3 id="RDD-block-Replication-v2"><a class="header-anchor" href="#RDD-block-Replication-v2">¶</a>RDD block Replication</h3>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManage，只有一类</span></span><br><span class="line"><span class="type">NettyBlockRpcServer</span>.receive -&gt; <span class="keyword">match</span> openBlocks -&gt; getBlockData</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager内部</span></span><br><span class="line">getBlockData -&gt; getLocalBytes</span><br></pre></td></tr></table></figure>
<h3 id="Shuffle-相关-v2"><a class="header-anchor" href="#Shuffle-相关-v2">¶</a>Shuffle 相关</h3>
<p>shuffle read 阶段：<code>BlockStoreShuffleReader</code> 核心是<code>ShuffleBlockFetcherIterator</code></p>
<p>现在可以理解它们名字的含义了，就是去fetch write阶段写下的block，</p>
<p>分本地<code>localBlocks</code> 和远程  <code>remoteBlocks</code></p>
<p>这部分内容较复杂，需结合rpc，先pass</p>
<h1>小结</h1>
<p>BlockManager 块管理者，使用它时，直接get、put即可；而之前对其底层结构的了解这一过程还是很有意思的，同时使我对spark有了更深的认识。</p>
<p>至此，spark文件系统告一段落，以后学了rpc再回来看文件系统的rpc。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark源码-cache和persist</title>
    <url>/2020/01/11/spark/spark%E6%BA%90%E7%A0%81-cache%E5%92%8Cpersist/</url>
    <content><![CDATA[<h1>简介</h1>
<p>复杂的任务中，某个中间转换结果可能会被多次调用，此时可以使用 spark 的缓存功能，将计算的中间过程缓存在内存或者磁盘中，以便再次使用，减少不必要的计算。</p>
<a id="more"></a>
<h1>特点</h1>
<ul>
<li>懒加载，只有RDD触发action时才会进行计算并且缓存</li>
<li>5个参数控制存储级别：是否用内存缓存、 是否用磁盘缓存、是否用堆外内存缓存、是否序列化、缓存个数</li>
<li>cache() 是 persist() 也是 persist(StorageLevel.MEMORY_ONLY)</li>
<li>api:   <code>rdd.cache()</code> or <code>rdd.persist()</code>、<code>rdd.unpersist()</code>、<code>sc.getPersistentRDDs</code></li>
</ul>
<p>接着，以MEMORY_ONLY模式为例，从源码中验证这一切，同时加深对 <strong>spark内存管理</strong> 的理解</p>
<h1>流程</h1>
<h2 id="cache-or-persist-…"><a class="header-anchor" href="#cache-or-persist-…">¶</a>cache() or persist(…)</h2>
<p>解决一个疑问：<code>rdd1.cache()  rdd1 -&gt; rdd2 rdd2.cahce()</code> ，此时 rdd1 和 rdd2 都会被缓存</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// cache() 等同于 persist() 等同于 persist(StorageLevel.MEMORY_ONLY) ，也就是仅缓存于存储内存中。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 缓存级别，由5个参数组成</span></span><br><span class="line"><span class="keyword">new</span> <span class="type">StorageLevel</span>(useDisk, useMemory, useOffHeap, deserialized, replication))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  <span class="comment">// isLocallyCheckpointed 方法 判断该RDD是否已经标记为 checkpoint，注意不是cache</span></span><br><span class="line">  <span class="keyword">if</span> (isLocallyCheckpointed) &#123;</span><br><span class="line">    persist(<span class="type">LocalRDDCheckpointData</span>.transformStorageLevel(newLevel), allowOverride = <span class="literal">true</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    persist(newLevel, allowOverride = <span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>, allowOverride: <span class="type">Boolean</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  <span class="comment">// 可以发现当前版本并不支持改变已缓存 RDD 的 StorageLevel (注意RDD1转成RDD2后自然可以改变)</span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span> &amp;&amp; newLevel != storageLevel &amp;&amp; !allowOverride) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(</span><br><span class="line">      <span class="string">"Cannot change storage level of an RDD after it was already assigned a level"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 仅第一次缓存时触发 </span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">    sc.cleaner.foreach(_.registerRDDForCleanup(<span class="keyword">this</span>)) <span class="comment">// 用于 cleanups</span></span><br><span class="line">    <span class="comment">// 写入 persistentRdds，它是一个map（rdd.id, rdd），可调用 sc.getPersistentRDDs 得到</span></span><br><span class="line">    sc.persistRDD(<span class="keyword">this</span>) </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 设置 storageLevel，之前默认为StorageLevel.NONE</span></span><br><span class="line">  storageLevel = newLevel</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="getOrCompute-…"><a class="header-anchor" href="#getOrCompute-…">¶</a>getOrCompute(…)</h2>
<p>cache() 给 RDD埋了一个属性<code>storageLevel</code>，只有执行行动操作才会真正执行缓存</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// RDD的iterator，由于RDD本身懒加载，只要行动操作才会执行</span></span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="comment">// 计算前先检查 storageLevel 是否不为NONE</span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">    getOrCompute(split, context) <span class="comment">// 核心</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    computeOrReadCheckpoint(split, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">getOrCompute</span></span>(partition: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> blockId = <span class="type">RDDBlockId</span>(id, partition.index)</span><br><span class="line">  <span class="keyword">var</span> readCachedBlock = <span class="literal">true</span></span><br><span class="line">  <span class="comment">// 调用blockManager 的 getOrElseUpdate方法，取出或者生成该blockId对应的block数据，返回blockResult</span></span><br><span class="line">  <span class="type">SparkEnv</span>.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () =&gt; &#123;</span><br><span class="line">    <span class="comment">// 该函数变量仅生成block时执行</span></span><br><span class="line">    readCachedBlock = <span class="literal">false</span></span><br><span class="line">    computeOrReadCheckpoint(partition, context)</span><br><span class="line">  &#125;) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Left</span>(blockResult) =&gt;</span><br><span class="line">    <span class="comment">// 读 cache ：会计数</span></span><br><span class="line">    <span class="keyword">if</span> (readCachedBlock) &#123;</span><br><span class="line">      <span class="keyword">val</span> existingMetrics = context.taskMetrics().inputMetrics</span><br><span class="line">      existingMetrics.incBytesRead(blockResult.bytes)</span><br><span class="line">      <span class="comment">// blockResul.data 得到迭代器，封装成 InterruptibleIterator，结束！</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>[<span class="type">T</span>](context, blockResult.data.asInstanceOf[<span class="type">Iterator</span>[<span class="type">T</span>]]) &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">T</span> = &#123;</span><br><span class="line">          existingMetrics.incRecordsRead(<span class="number">1</span>)</span><br><span class="line">          delegate.next()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 生成 cache：compute 内有自己的计数，这里就不用处理</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, blockResult.data.asInstanceOf[<span class="type">Iterator</span>[<span class="type">T</span>]])</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Right</span>(iter) =&gt;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, iter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">T</span>]])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="getOrElseUpdate-…"><a class="header-anchor" href="#getOrElseUpdate-…">¶</a>getOrElseUpdate(…)</h2>
<p><code>getOrElseUpdate</code> 是<code>BlockManager </code>中的方法</p>
<p>函数参数<code>makeIterator</code>就是我们的<code>computeOrReadCheckpoint</code>方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOrElseUpdate</span></span>[<span class="type">T</span>](</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    level: <span class="type">StorageLevel</span>,</span><br><span class="line">    classTag: <span class="type">ClassTag</span>[<span class="type">T</span>],</span><br><span class="line">    makeIterator: () =&gt; <span class="type">Iterator</span>[<span class="type">T</span>]): <span class="type">Either</span>[<span class="type">BlockResult</span>, <span class="type">Iterator</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">  <span class="comment">// 调用 get[T](blockId) 方法，从各种 store 中找该 block</span></span><br><span class="line">  get[<span class="type">T</span>](blockId)(classTag) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="comment">// 找到了表明有缓存，直接返回找到的 BlockResult</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(block) =&gt;</span><br><span class="line">      <span class="keyword">return</span> <span class="type">Left</span>(block)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="comment">// 没缓存，继续执行下一步</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Initially we hold no locks on this block.</span></span><br><span class="line">  <span class="comment">// 调用 doPutIterator，将 makeIterator 计算的结果存入Block（逻辑概念）中（其实是存入各种store中）</span></span><br><span class="line">  <span class="comment">// 写数据时加 读锁</span></span><br><span class="line">  doPutIterator(blockId, makeIterator, level, classTag, keepReadLock = <span class="literal">true</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      <span class="comment">// doPut() 正常情况返回None，接着调用 getLocalValues 读取刚写的 block，返回 blockResult</span></span><br><span class="line">      <span class="keyword">val</span> blockResult = getLocalValues(blockId).getOrElse &#123;</span><br><span class="line">        releaseLock(blockId)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"get() failed for block <span class="subst">$blockId</span> even though we held a lock"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 放锁</span></span><br><span class="line">      releaseLock(blockId)</span><br><span class="line">      <span class="type">Left</span>(blockResult)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(iter) =&gt;</span><br><span class="line">     <span class="type">Right</span>(iter)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>补充</h1>
<h2 id="persistentRdds"><a class="header-anchor" href="#persistentRdds">¶</a>persistentRdds</h2>
<p>一个map ：key 为 <a href="http://rdd.id" target="_blank" rel="noopener">rdd.id</a> ，value 为 rdd</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">val</span> persistentRdds = &#123;</span><br><span class="line">    <span class="keyword">val</span> map: <span class="type">ConcurrentMap</span>[<span class="type">Int</span>, <span class="type">RDD</span>[_]] = <span class="keyword">new</span> <span class="type">MapMaker</span>().weakValues().makeMap[<span class="type">Int</span>, <span class="type">RDD</span>[_]]()</span><br><span class="line">    map.asScala</span><br><span class="line">  &#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPersistentRDDs</span></span>: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">RDD</span>[_]] = persistentRdds.toMap</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用户使用该方法得到它</span></span><br><span class="line"><span class="keyword">val</span> ds: collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">RDD</span>[_]] = sc.getPersistentRDDs</span><br></pre></td></tr></table></figure>
<h2 id="unpersist"><a class="header-anchor" href="#unpersist">¶</a>unpersist()</h2>
<p>取消缓存</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">unpersistRDD</span></span>(rddId: <span class="type">Int</span>, blocking: <span class="type">Boolean</span> = <span class="literal">true</span>) &#123;</span><br><span class="line">  <span class="comment">// 通知blockManager删掉属于该RDD的全部block</span></span><br><span class="line">  env.blockManager.master.removeRdd(rddId, blocking)</span><br><span class="line">  <span class="comment">// 从map中移掉它</span></span><br><span class="line">  persistentRdds.remove(rddId)</span><br><span class="line">  listenerBus.post(<span class="type">SparkListenerUnpersistRDD</span>(rddId))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>小结</h1>
<p>当你对spark的存储有一点理解时，本节相对简单。缓存就是将RDD的<code>storageLevel</code>属性改写，并把该RDD加入<code>persistentRdds</code>这个map中。当执行到<code>iterator</code>时触发，如果没有缓存过，则进行计算并写入BLock中，有缓存直接从BLock中提取即可。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark源码-block</title>
    <url>/2020/01/11/spark/spark%E6%BA%90%E7%A0%81-block/</url>
    <content><![CDATA[<p><strong>首先明确spark有个自己文件系统，block就是里面的一个文件</strong>。如：缓存后<strong>的RDD的一个分区是一个block；计算产生的</strong>临时文件<strong>也是block</strong>。<strong>任何你要存的东西都是block</strong>。</p>
<p>因此既然它是文件，它就有<strong>文件名</strong>、<strong>元信息</strong>、<strong>锁</strong>、<strong>数据</strong>，理解它们，你就理解了block！</p>
<a id="more"></a>
<h1>BlockId</h1>
<p>每个Block都有个ID与之一一对应，这个<strong>BlockId也就是该文件的文件名</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">sealed</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BlockId</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// RDDBlockId、ShuffleBlockId什么的都是它的子类</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">asRDDId</span></span>: <span class="type">Option</span>[<span class="type">RDDBlockId</span>] = <span class="keyword">if</span> (isRDD) <span class="type">Some</span>(asInstanceOf[<span class="type">RDDBlockId</span>]) <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isRDD</span></span>: <span class="type">Boolean</span> = isInstanceOf[<span class="type">RDDBlockId</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isShuffle</span></span>: <span class="type">Boolean</span> = isInstanceOf[<span class="type">ShuffleBlockId</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isBroadcast</span></span>: <span class="type">Boolean</span> = isInstanceOf[<span class="type">BroadcastBlockId</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = name</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>理解了取名的规则，你就对spark的计算流程理解了。例如：</p>
<p>todo：理解剩余的 BlockId</p>
<p><strong>普通RDD</strong>：</p>
<ul>
<li>RDDBlockId <code>&quot;rdd_&quot; + rddId + &quot;_&quot; + splitIndex</code></li>
</ul>
<p>简单的按 rddId 和 分区ID 划分</p>
<p><strong>Shuffle过程</strong>：</p>
<ul>
<li>ShuffleBlockId <code>&quot;shuffle_&quot; + shuffleId + &quot;_&quot; + mapId + &quot;_&quot; + reduceId</code></li>
<li>ShuffleDataBlockId <code>&quot;shuffle_&quot; + shuffleId + &quot;_&quot; + mapId + &quot;_&quot; + reduceId + &quot;.data&quot;</code></li>
<li>ShuffleIndexBlockId <code>&quot;shuffle_&quot; + shuffleId + &quot;_&quot; + mapId + &quot;_&quot; + reduceId + &quot;.index&quot;</code></li>
</ul>
<p><strong>ShuffleBlockId 就是ShuffleWrite时单个task生成的那个大文件，ShuffleDataBlockId是这个大文件的临时叫法，ShuffleIndexBlockId是索引文件</strong>。而它们由<strong>shuffleId、mapId、reduceId</strong> 3者共同确定。</p>
<p>注意：<strong>reduceId是0（<code>NOOP_REDUCE_ID</code>）</strong>，因为<strong>SortShuffle最后生成的是1个大文件</strong>，所以这个reduceid没什么软用。理解清楚SortShuffleManager和被淘汰的HashShuffleManager的区别，骚年。</p>
<p><strong>临时文件</strong>：</p>
<ul>
<li>
<p>TempLocalBlockId <code>temp_local_ + “randomUUID”</code></p>
</li>
<li>
<p>TempShuffleBlockId <code>temp_shuffle_ + “randomUUID”</code></p>
</li>
</ul>
<p><strong>TempLocalBlockId是本地计算中间临时文件</strong></p>
<p><strong>TempShuffleBlockId是shuffle计算中间临时文件</strong>：1、BypassShuffle 单分区临时文件；2、shuffle排序时如果内存不够发生spill到磁盘的文件。</p>
<h1>BlockInfo</h1>
<p>block的元信息如下</p>
<p>构造参数：</p>
<ul>
<li><strong>StorageLevel</strong>：存储等级</li>
<li><strong>classTag</strong>：类名，用于序列化需求</li>
<li><strong>tellMaster</strong>：是否需要通知master block改变，大多数都要，但是broadcast blocks不用</li>
</ul>
<p>内部属性（可get、set）后面两参数用于实现锁机制：</p>
<ul>
<li><strong>size</strong>: block的大小（bytes）</li>
<li><strong>readerCount</strong>： 目前有多少个Task在读它</li>
<li><strong>writerTask</strong>：<strong>持有该块写锁的Task ID</strong>。默认值：BlockInfo.NO_WRITER，代表没有人写</li>
</ul>
<p>在set值后，会调用checkInvariants方法，检查值是否合格。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkInvariants</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 可多人读</span></span><br><span class="line">  assert(_readerCount &gt;= <span class="number">0</span>)</span><br><span class="line">  <span class="comment">// 读写互斥</span></span><br><span class="line">  assert(_readerCount == <span class="number">0</span> || _writerTask == <span class="type">BlockInfo</span>.<span class="type">NO_WRITER</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>BlockInfoManager</h1>
<p>它用于控制块的元信息，以<strong>实现块的锁机制</strong>，一个经典的文件读写锁机制。</p>
<p>需求：</p>
<ul>
<li>同一个文件可以多个人读，前提没人写；且读锁可重入</li>
<li>只能有一个人写文件，且没人读 ；且写锁不可重入</li>
<li>当拿不到锁，可以选择一直等待（实现方法：拿不到锁wait，放锁notify）</li>
</ul>
<h2 id="成员"><a class="header-anchor" href="#成员">¶</a>成员</h2>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 换个别名</span></span><br><span class="line"><span class="comment">// 理解：一个task一个线程，锁的就它</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">type</span> <span class="title">TaskAttemptId</span> </span>= <span class="type">Long</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockId 和 BlockInfo的映射</span></span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> infos = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">BlockId</span>, <span class="type">BlockInfo</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 任务ID 和 它拥有的写锁</span></span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> writeLocksByTask =</span><br><span class="line"><span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TaskAttemptId</span>, mutable.<span class="type">Set</span>[<span class="type">BlockId</span>]]</span><br><span class="line"><span class="keyword">with</span> mutable.<span class="type">MultiMap</span>[<span class="type">TaskAttemptId</span>, <span class="type">BlockId</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 任务ID 和 它拥有的读锁</span></span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> readLocksByTask =</span><br><span class="line"><span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TaskAttemptId</span>, <span class="type">ConcurrentHashMultiset</span>[<span class="type">BlockId</span>]]</span><br></pre></td></tr></table></figure>
<p>writeLocksByTask的数据结构的写法真是<a href="https://www.scala-lang.org/api/current/scala/collection/mutable/MultiMap.html" target="_blank" rel="noopener">活久见</a>，使用<strong>有重复值的set</strong>实现<strong>可重入锁</strong>。</p>
<h2 id="难点方法"><a class="header-anchor" href="#难点方法">¶</a>难点方法</h2>
<h3 id="registerTask"><a class="header-anchor" href="#registerTask">¶</a>registerTask</h3>
<p>给<code>readLocksByTask</code>添加映射，值自然是空的</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">registerTask</span></span>(taskAttemptId: <span class="type">TaskAttemptId</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">  require(!readLocksByTask.contains(taskAttemptId),</span><br><span class="line">          <span class="string">s"Task attempt <span class="subst">$taskAttemptId</span> is already registered"</span>)</span><br><span class="line">  readLocksByTask(taskAttemptId) = <span class="type">ConcurrentHashMultiset</span>.create()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意一个特殊的TaskId: <code>NON_TASK_WRITER</code>，它在<strong>BlockInfoManager初始化时被注册</strong>，非任务线程，如driver线程。</p>
<h3 id="lockNewBlockForWriting"><a class="header-anchor" href="#lockNewBlockForWriting">¶</a>lockNewBlockForWriting</h3>
<p>创建新的block时，自然需要<strong>加写锁</strong>，它用到了<code>lockForReading</code>和<code>lockForWriting</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lockNewBlockForWriting</span></span>(</span><br><span class="line">  blockId: <span class="type">BlockId</span>,</span><br><span class="line">  newBlockInfo: <span class="type">BlockInfo</span>): <span class="type">Boolean</span> = synchronized &#123;</span><br><span class="line">  logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> trying to put <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  <span class="comment">// 由于有多线程的存在不能直接执行操作，而是先判断读锁，防止别的兄弟先一步已经lockNewBlockForWriting了</span></span><br><span class="line">  lockForReading(blockId) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(info) =&gt;</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">    <span class="comment">// 给 infos 加映射</span></span><br><span class="line">    infos(blockId) = newBlockInfo</span><br><span class="line">    <span class="comment">// 上写锁</span></span><br><span class="line">    lockForWriting(blockId)</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="lockForWriting"><a class="header-anchor" href="#lockForWriting">¶</a>lockForWriting</h3>
<p>加写锁</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lockForWriting</span></span>(</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    blocking: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">Option</span>[<span class="type">BlockInfo</span>] = synchronized &#123;</span><br><span class="line">  logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> trying to acquire write lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  do &#123;</span><br><span class="line">    infos.get(blockId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">return</span> <span class="type">None</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(info) =&gt;</span><br><span class="line">      	<span class="comment">// 当该block没有人读，且没有人写时才能锁它，并返回</span></span><br><span class="line">        <span class="keyword">if</span> (info.writerTask == <span class="type">BlockInfo</span>.<span class="type">NO_WRITER</span> &amp;&amp; info.readerCount == <span class="number">0</span>) &#123;</span><br><span class="line">          info.writerTask = currentTaskAttemptId</span><br><span class="line">          writeLocksByTask.addBinding(currentTaskAttemptId, blockId)</span><br><span class="line">          logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> acquired write lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">          <span class="keyword">return</span> <span class="type">Some</span>(info)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果之前没拿到锁，就wait()，blocking控制是否等待</span></span><br><span class="line">    <span class="keyword">if</span> (blocking) &#123;</span><br><span class="line">      wait()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">while</span> (blocking)</span><br><span class="line">  <span class="type">None</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="lockForReading"><a class="header-anchor" href="#lockForReading">¶</a>lockForReading</h3>
<p>加读锁</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lockForReading</span></span>(</span><br><span class="line">  blockId: <span class="type">BlockId</span>,</span><br><span class="line">  blocking: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">Option</span>[<span class="type">BlockInfo</span>] = synchronized &#123;</span><br><span class="line">  logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> trying to acquire read lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  do &#123;</span><br><span class="line">    infos.get(blockId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">return</span> <span class="type">None</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(info) =&gt;</span><br><span class="line">      <span class="comment">// 没有人写，即可加读锁，并返回</span></span><br><span class="line">      <span class="keyword">if</span> (info.writerTask == <span class="type">BlockInfo</span>.<span class="type">NO_WRITER</span>) &#123;</span><br><span class="line">        info.readerCount += <span class="number">1</span></span><br><span class="line">        readLocksByTask(currentTaskAttemptId).add(blockId)</span><br><span class="line">        logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> acquired read lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="type">Some</span>(info)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 同上</span></span><br><span class="line">    <span class="keyword">if</span> (blocking) &#123;</span><br><span class="line">      wait()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">while</span> (blocking)</span><br><span class="line">  <span class="type">None</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="unlock"><a class="header-anchor" href="#unlock">¶</a>unlock</h3>
<p>放锁</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unlock</span></span>(blockId: <span class="type">BlockId</span>, taskAttemptId: <span class="type">Option</span>[<span class="type">TaskAttemptId</span>] = <span class="type">None</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">  <span class="keyword">val</span> taskId = taskAttemptId.getOrElse(currentTaskAttemptId)</span><br><span class="line">  logTrace(<span class="string">s"Task <span class="subst">$taskId</span> releasing lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  <span class="keyword">val</span> info = get(blockId).getOrElse &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Block <span class="subst">$blockId</span> not found"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 放写锁</span></span><br><span class="line">  <span class="keyword">if</span> (info.writerTask != <span class="type">BlockInfo</span>.<span class="type">NO_WRITER</span>) &#123;</span><br><span class="line">    info.writerTask = <span class="type">BlockInfo</span>.<span class="type">NO_WRITER</span></span><br><span class="line">    writeLocksByTask.removeBinding(taskId, blockId)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    assert(info.readerCount &gt; <span class="number">0</span>, <span class="string">s"Block <span class="subst">$blockId</span> is not locked for reading"</span>)</span><br><span class="line">    <span class="comment">// 放读锁</span></span><br><span class="line">    info.readerCount -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">val</span> countsForTask = readLocksByTask(taskId)</span><br><span class="line">    <span class="comment">// remove(blockId, n) 删除重复set中指定元素 n 个 。此处自然只删一个</span></span><br><span class="line">    <span class="keyword">val</span> newPinCountForTask: <span class="type">Int</span> = countsForTask.remove(blockId, <span class="number">1</span>) - <span class="number">1</span></span><br><span class="line">    assert(newPinCountForTask &gt;= <span class="number">0</span>,</span><br><span class="line">      <span class="string">s"Task <span class="subst">$taskId</span> release lock on block <span class="subst">$blockId</span> more times than it acquired it"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  notifyAll()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="downgradeLock"><a class="header-anchor" href="#downgradeLock">¶</a>downgradeLock</h3>
<p>由写锁降为读锁。todo：了解这玩意有啥用</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">downgradeLock</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">  logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> downgrading write lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  <span class="keyword">val</span> info = get(blockId).get</span><br><span class="line">  require(info.writerTask == currentTaskAttemptId,</span><br><span class="line">    <span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> tried to downgrade a write lock that it does not hold on"</span> +</span><br><span class="line">      <span class="string">s" block <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  <span class="comment">// 放锁</span></span><br><span class="line">  unlock(blockId)</span><br><span class="line">  <span class="comment">// 加读锁，注意blocking为false</span></span><br><span class="line">  <span class="keyword">val</span> lockOutcome = lockForReading(blockId, blocking = <span class="literal">false</span>)</span><br><span class="line">  assert(lockOutcome.isDefined)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>BlockResult</h1>
<p>调用 <code>getLocalValues(blockId)</code> 或者 <code>getRemoteValues(blockId)</code> 时，以java对象的形式返回block。</p>
<p>注意，<strong>数据是存在store中的</strong>，如<code>memoryStore</code>，调它的API取出数据封装成BlockResult。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">BlockResult</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val data: <span class="type">Iterator</span>[<span class="type">Any</span>],    // 数据，从memoryStore中取出</span></span></span><br><span class="line"><span class="class"><span class="params">    val readMethod: <span class="type">DataReadMethod</span>.<span class="type">Value</span>, // <span class="type">Memory</span>, <span class="type">Disk</span>, <span class="type">Hadoop</span>, <span class="type">Network</span></span></span></span><br><span class="line"><span class="class"><span class="params">    val bytes: <span class="type">Long</span></span>)            <span class="title">//</span> <span class="title">大小，从info中取出</span></span></span><br></pre></td></tr></table></figure>
<h1>BlockData</h1>
<p>调用 <code>getLocalBytes(blockId)</code> 或者 <code>getRemoteBytes(blockId)</code> 时，以序列化字节的形式返回block。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">BlockData</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toInputStream</span></span>(): <span class="type">InputStream</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toNetty</span></span>(): <span class="type">Object</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toChunkedByteBuffer</span></span>(allocator: <span class="type">Int</span> =&gt; <span class="type">ByteBuffer</span>): <span class="type">ChunkedByteBuffer</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toByteBuffer</span></span>(): <span class="type">ByteBuffer</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">size</span></span>: <span class="type">Long</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(): <span class="type">Unit</span>   <span class="comment">// 多了一个销毁</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它有3个实现类，<code>ByteBufferBlockData</code>、<code>DiskBlockData</code>，以及<code>EncryptedBlockData</code>。</p>
<p>以<code>ByteBufferBlockData</code>为例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ByteBufferBlockData</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val buffer: <span class="type">ChunkedByteBuffer</span>,     // 数据，从memoryStore中取出</span></span></span><br><span class="line"><span class="class"><span class="params">    val shouldDispose: <span class="type">Boolean</span></span>) <span class="keyword">extends</span> <span class="title">BlockData</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 下面4个是选择以什么形式提取</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toInputStream</span></span>(): <span class="type">InputStream</span> = buffer.toInputStream(dispose = <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toNetty</span></span>(): <span class="type">Object</span> = buffer.toNetty</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toChunkedByteBuffer</span></span>(allocator: <span class="type">Int</span> =&gt; <span class="type">ByteBuffer</span>): <span class="type">ChunkedByteBuffer</span> = &#123;</span><br><span class="line">    buffer.copy(allocator)&#125;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toByteBuffer</span></span>(): <span class="type">ByteBuffer</span> = buffer.toByteBuffer</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">size</span></span>: <span class="type">Long</span> = buffer.size</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (shouldDispose) &#123;</span><br><span class="line">      buffer.dispose()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>小结</h1>
<p>我觉得对Block的认识<strong>非常非常重要</strong>，它是spark内部文件系统的基石。</p>
<p>对知识梳理一波：</p>
<p>Store：真正存数据，如MemoryStore、DiskStore</p>
<p>MemoryManager: 划分内存</p>
<p>Block：文件，有<strong>文件名</strong>（blockId）、<strong>元信息</strong>(blockInfo)、<strong>锁</strong>(blockInfoManager)、<strong>数据</strong>（从Store中取）</p>
<p>BlockManager：对Block进行管理（下一节）</p>
<p>RPC：各种master、manager（以后再学）</p>
<p>这当真是一个完整的<strong>分布式多人文件系统</strong>！</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark源码-DiskBlockManager和DiskStore</title>
    <url>/2020/01/11/spark/spark%E6%BA%90%E7%A0%81-DiskBlockManager%E5%92%8CDiskStore/</url>
    <content><![CDATA[<h1>DiskBlockManager</h1>
<p>建立BlockID和磁盘中真实文件的一一映射，同时负责创建文件夹与删除文件夹</p>
<a id="more"></a>
<h2 id="成员"><a class="header-anchor" href="#成员">¶</a>成员</h2>
<ul>
<li>
<p>subDirsPerLocalDir：每个LOCAL_DIRS下目录的最大个数，<code>spark.diskStore.subDirectories</code>（默认64）</p>
</li>
<li>
<p>localDirs：本地目录数组，由<code>createLocalDirs()</code>创建。</p>
</li>
<li>
<p>subDirs：2维数组<code>[localDirs个数][subDirsPerLocalDir 64]</code>，相当于人为把上面的目录按hash散列开，2级目录用于防止出现顶层inodes个数过多。（block文件就在该目录下）</p>
</li>
</ul>
<p>下面以我的spark on yarn集群为例：</p>
<p>localDirs ：<code>root + &quot;blockmgr&quot; + &quot;-&quot; + UUID.randomUUID</code>，其中 root 由<code>spark.local.dir</code>指定（可多个），默认是 <code>/tmp</code>。总算知道这玩意是干嘛的了，很明显该目录对应的<strong>磁盘对性能要求高！</strong></p>
<p>subDirs：可以看到散列了5个子文件夹</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[xxx tmp]$ tree ./blockmgr*</span><br><span class="line">./blockmgr-37c233af-359c-4bcd-8dcc-5f216e55ce7d</span><br><span class="line">├── 0c</span><br><span class="line">├── 0d</span><br><span class="line">├── 0e</span><br><span class="line">├── 11</span><br><span class="line">└── 13</span><br></pre></td></tr></table></figure>
<ul>
<li>shutdownHook：给<code>ShutdownHookManager</code>添加一个钩子，调用doStop，递归删除localDirs下的文件</li>
</ul>
<h2 id="难点方法"><a class="header-anchor" href="#难点方法">¶</a>难点方法</h2>
<h3 id="getFile"><a class="header-anchor" href="#getFile">¶</a>getFile</h3>
<p>根据名字或者blockID返回磁盘中的File(subDir, filename)，如果没有就创建</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getFile</span></span>(filename: <span class="type">String</span>): <span class="type">File</span> = &#123;</span><br><span class="line">  <span class="comment">// 先hash取余 dirId， 再hash取余 subDirId</span></span><br><span class="line">  <span class="keyword">val</span> hash = <span class="type">Utils</span>.nonNegativeHash(filename)</span><br><span class="line">  <span class="keyword">val</span> dirId = hash % localDirs.length</span><br><span class="line">  <span class="keyword">val</span> subDirId = (hash / localDirs.length) % subDirsPerLocalDir</span><br><span class="line">  <span class="comment">// 如果</span></span><br><span class="line">  <span class="keyword">val</span> subDir = subDirs(dirId).synchronized &#123;</span><br><span class="line">    <span class="keyword">val</span> old = subDirs(dirId)(subDirId)</span><br><span class="line">    <span class="keyword">if</span> (old != <span class="literal">null</span>) &#123;</span><br><span class="line">      old</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 子目录格式：16进制2位符</span></span><br><span class="line">      <span class="keyword">val</span> newDir = <span class="keyword">new</span> <span class="type">File</span>(localDirs(dirId), <span class="string">"%02x"</span>.format(subDirId))</span><br><span class="line">      <span class="keyword">if</span> (!newDir.exists() &amp;&amp; !newDir.mkdir()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">s"Failed to create local dir in <span class="subst">$newDir</span>."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      subDirs(dirId)(subDirId) = newDir</span><br><span class="line">      newDir</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">File</span>(subDir, filename)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getFile</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">File</span> = getFile(blockId.name)</span><br></pre></td></tr></table></figure>
<h3 id="createTempXXXBlock"><a class="header-anchor" href="#createTempXXXBlock">¶</a>createTempXXXBlock</h3>
<ul>
<li>
<p><code>createTempLocalBlock</code>：创建存放本地计算中间临时文件的Block</p>
<p>blockId =  temp_local_ + “randomUUID”</p>
</li>
<li>
<p><code>createTempShuffleBlock</code>：创建存放shuffle计算中间临时文件（如shuffle排序内存不够发生spill）的Block，blockId =  temp_shuffle_ + “randomUUID”</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTempLocalBlock</span></span>(): (<span class="type">TempLocalBlockId</span>, <span class="type">File</span>) = &#123;</span><br><span class="line">  <span class="keyword">var</span> blockId = <span class="keyword">new</span> <span class="type">TempLocalBlockId</span>(<span class="type">UUID</span>.randomUUID())</span><br><span class="line">  <span class="keyword">while</span> (getFile(blockId).exists()) &#123;</span><br><span class="line">    blockId = <span class="keyword">new</span> <span class="type">TempLocalBlockId</span>(<span class="type">UUID</span>.randomUUID())</span><br><span class="line">  &#125;</span><br><span class="line">  (blockId, getFile(blockId)) <span class="comment">// getFile创建目录</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTempShuffleBlock</span></span>(): (<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>) = &#123;</span><br><span class="line">  <span class="keyword">var</span> blockId = <span class="keyword">new</span> <span class="type">TempShuffleBlockId</span>(<span class="type">UUID</span>.randomUUID())</span><br><span class="line">  <span class="keyword">while</span> (getFile(blockId).exists()) &#123;</span><br><span class="line">    blockId = <span class="keyword">new</span> <span class="type">TempShuffleBlockId</span>(<span class="type">UUID</span>.randomUUID())</span><br><span class="line">  &#125;</span><br><span class="line">  (blockId, getFile(blockId)) <span class="comment">// getFile创建目录</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="stop"><a class="header-anchor" href="#stop">¶</a>stop</h3>
<p>调用<code>stop()</code>，删除文件</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>() &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="type">ShutdownHookManager</span>.removeShutdownHook(shutdownHook)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">      logError(<span class="string">s"Exception while removing shutdown hook."</span>, e)</span><br><span class="line">  &#125;</span><br><span class="line">  doStop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// deleteFilesOnStop：DiskBlockManager 的 构造参数，决定是否递归删除localDirs下的文件</span></span><br><span class="line">  <span class="comment">// 一般只有使用外部shuffle服务时，它才为false</span></span><br><span class="line">  <span class="keyword">if</span> (deleteFilesOnStop) &#123;</span><br><span class="line">    localDirs.foreach &#123; localDir =&gt;</span><br><span class="line">      <span class="keyword">if</span> (localDir.isDirectory() &amp;&amp; localDir.exists()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (!<span class="type">ShutdownHookManager</span>.hasRootAsShutdownDeleteDir(localDir)) &#123;</span><br><span class="line">            <span class="type">Utils</span>.deleteRecursively(localDir)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">            logError(<span class="string">s"Exception while deleting local spark dir: <span class="subst">$localDir</span>"</span>, e)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>DiskStore</h1>
<p>负责将blocks写入磁盘及删改查操作</p>
<h2 id="成员-v2"><a class="header-anchor" href="#成员-v2">¶</a>成员</h2>
<ul>
<li>minMemoryMapBytes:    使用内存映射（java nio 功能）的最小值<code>spark.storage.memoryMapThreshold</code>默认2M</li>
<li>maxMemoryMapBytes：使用内存映射的最大值</li>
<li>blockSizes：一个ConcurrentHashMap，BlockId  -&gt; blockSize</li>
</ul>
<h2 id="难点方法-v2"><a class="header-anchor" href="#难点方法-v2">¶</a>难点方法</h2>
<h3 id="put"><a class="header-anchor" href="#put">¶</a>put</h3>
<p>增</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">put</span></span>(blockId: <span class="type">BlockId</span>)(writeFunc: <span class="type">WritableByteChannel</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (contains(blockId)) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Block <span class="subst">$blockId</span> is already present in the disk store"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  logDebug(<span class="string">s"Attempting to put block <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  <span class="keyword">val</span> startTime = <span class="type">System</span>.currentTimeMillis</span><br><span class="line">  <span class="keyword">val</span> file = diskManager.getFile(blockId) <span class="comment">// 创建文件夹，并返回file</span></span><br><span class="line">  <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">CountingWritableChannel</span>(openForWrite(file)) <span class="comment">// 开channel</span></span><br><span class="line">  <span class="keyword">var</span> threwException: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    writeFunc(out) <span class="comment">// 写入文件夹中</span></span><br><span class="line">    blockSizes.put(blockId, out.getCount) <span class="comment">// 计数size，加入blockSizes map中</span></span><br><span class="line">    threwException = <span class="literal">false</span></span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      out.close()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> ioe: <span class="type">IOException</span> =&gt;</span><br><span class="line">        <span class="keyword">if</span> (!threwException) &#123;</span><br><span class="line">          threwException = <span class="literal">true</span></span><br><span class="line">          <span class="keyword">throw</span> ioe</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">       <span class="keyword">if</span> (threwException) &#123;</span><br><span class="line">        remove(blockId)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> finishTime = <span class="type">System</span>.currentTimeMillis</span><br><span class="line">  logDebug(<span class="string">"Block %s stored as %s file on disk in %d ms"</span>.format(</span><br><span class="line">    file.getName,</span><br><span class="line">    <span class="type">Utils</span>.bytesToString(file.length()),</span><br><span class="line">    finishTime - startTime))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以下是一种调用它的方式</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">putBytes</span></span>(blockId: <span class="type">BlockId</span>, bytes: <span class="type">ChunkedByteBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  put(blockId) &#123; channel =&gt;</span><br><span class="line">    bytes.writeFully(channel)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="getBytes"><a class="header-anchor" href="#getBytes">¶</a>getBytes</h3>
<p>查，根据blockId，返回BlockData。</p>
<p>有两种：<code>DiskBlockData</code>（普通） 以 及<code>EncryptedBlockData</code> （加密）</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBytes</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">BlockData</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> file = diskManager.getFile(blockId.name)</span><br><span class="line">  <span class="keyword">val</span> blockSize = getSize(blockId)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 分两种，一种是加密的一种是非加密的</span></span><br><span class="line">  securityManager.getIOEncryptionKey() <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(key) =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">EncryptedBlockData</span>(file, blockSize, conf, key)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">DiskBlockData</span>(minMemoryMapBytes, maxMemoryMapBytes, file, blockSize)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="remove"><a class="header-anchor" href="#remove">¶</a>remove</h3>
<p>删</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  blockSizes.remove(blockId)</span><br><span class="line">  <span class="keyword">val</span> file = diskManager.getFile(blockId.name)</span><br><span class="line">  <span class="keyword">if</span> (file.exists()) &#123;</span><br><span class="line">    <span class="keyword">val</span> ret = file.delete()</span><br><span class="line">    <span class="keyword">if</span> (!ret) &#123;</span><br><span class="line">      logWarning(<span class="string">s"Error deleting <span class="subst">$&#123;file.getPath()&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    ret</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>小结</h1>
<p>DiskBlockManager 用于建立BlockID和磁盘中真实文件的一一映射，同时负责创建文件夹与删除文件夹，学习时可以找到那个文件夹进入看看。</p>
<p>DiskStore 负责将数据写入磁盘以及后续的读取与删除，相比MemoryStore它更简单。因为MemoryStore是真正的仓库，它需要定义数据结构；而DiskStore的数据是在磁盘的。</p>
<p>todo: 未来学习下java nio，再学习<code>BlockData</code>。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark源码-MemoryManager和MemoryStore</title>
    <url>/2020/01/11/spark/spark%E6%BA%90%E7%A0%81-MemoryManager%E5%92%8CMemoryStore/</url>
    <content><![CDATA[<p>它俩的对象和方法较多，需要耐心反复看。</p>
<a id="more"></a>
<h1>MemoryManager</h1>
<p>限制存储内存（<code>storage</code>）和执行内存（<code>execution</code>）的大小的管理器，有两种实现：<code>StaticMemoryManager</code> 和 <code>UnifiedMemoryManager</code>。</p>
<img src="https://i.loli.net/2020/01/11/blD2MIJj1AkOQGv.png" alt="image001.png" style="zoom:67%;">
<ul>
<li>
<p>一个Executor，一个JVM，一个MemoryManager，多个Task</p>
</li>
<li>
<p>同一Executor的Task可共享堆内堆外内存，而同一个节点的不同Executor的Task只能共享堆外内存</p>
</li>
<li>
<p>只能共享存储内存，计算内存肯定无法共享</p>
</li>
</ul>
<h2 id="核心依赖对象——MemoryPool"><a class="header-anchor" href="#核心依赖对象——MemoryPool">¶</a>核心依赖对象——MemoryPool</h2>
<p>它用于统计<strong>每个Executor</strong>内存使用情况的类（它并不是去查内存，只是记录，比如要用内存时就把数字增加，减时就把数字减少），具体实现分两种：<code>StorageMemoryPool</code> 和 <code>ExecutionMemoryPool</code>，它们每个又分堆内和堆外内存。基本方法有：<code>poolSize</code>、<code>memoryUsed</code>、<code>memoryFree(_poolSize - memoryUsed)</code>，看名知意。</p>
<ul>
<li>
<p><code>StorageMemoryPool</code>：除了有统计功能，并且还会在需要的内存不够时，通知<code>MemoryStore</code>调用<code>evictBlocksToFreeSpace</code>驱逐内存（发现不够，会先向执行内存借，如果借不到再驱逐。这部分代码在MemoryManager中）。</p>
</li>
<li>
<p><code>ExecutionMemoryPool</code>：相对内存池复杂点，由于存储内存对于同一个Executor的多个Task来说是<strong>共享</strong>的，而执行内存是<strong>不共享</strong>的。因此，其内部需要<strong>维护一个<code>HashMap</code></strong>，存储每个Task使用的执行内存。</p>
<p>细节1:  内部的<code>acquireMemory</code> 和 <code>releaseMemory</code> 之间采用 <code>wait()</code> 和 <code>notifyAll()</code>机制 ：在内存不够时<code>acquireMemory</code>会触发 wait，而当<code>releaseMemory</code>时，notifyall。</p>
<p>细节2:  每个Task<strong>最少到分执行内存的 1 / 2N</strong>，当超过1 / N时，不再允许申请。</p>
<p>细节3:  <code>acquireMemory</code> 内部有个回调函数<code>maybeGrowPool</code>：当执行内存不足时，从存储内存中借它剩余的，或者要它归还从执行内存中借走的。</p>
</li>
</ul>
<p>从这里也可以发现，执行内存可以要回被借走的，而存储内存不可以，这就是后面要说的<strong>动态占用</strong>规则之一。</p>
<p>注意：<code>MemoryPool</code>里的操作，都<strong>加了object lock</strong>(<code>MemoryManager</code>)</p>
<h2 id="UnifiedMemoryManager"><a class="header-anchor" href="#UnifiedMemoryManager">¶</a>UnifiedMemoryManager</h2>
<p>spark 1.6 后统一使用<code>UnifiedMemoryManager</code>，它的进步是<strong>存储内存和执行内存可以动态占用</strong>。</p>
<h3 id="内存划分"><a class="header-anchor" href="#内存划分">¶</a>内存划分</h3>
<p><img src="https://i.loli.net/2020/01/11/sBvxNCqUHnKa61c.png" alt="屏幕快照 2020-01-03 下午8.47.52.png"></p>
<p>堆内内存（<code>spark.executor.memory</code>  JVM内存）</p>
<ul>
<li>存储（storage）内存：（堆内内存 - 预留） * 0.6 <code>spark.memory.fraction</code> * 0.5 <code>spark.memory.storageFraction</code>，用于 <strong>cache、broadcast</strong> 等</li>
<li>执行（executor）内存：（堆内内存 - 预留） * 0.6 <code>spark.memory.fraction</code> * (1 - 0.5 <code>spark.memory.storageFraction</code>，用于<strong>计算，如shuffles、join、sorts and aggregations</strong></li>
<li>其它内存：（堆内内存 - 预留） * （1 - 0.6 <code>spark.memory.fraction</code>）</li>
<li>预留内存：<code>RESERVED_SYSTEM_MEMORY_BYTES</code> 规定 300M</li>
</ul>
<p>堆外内存（<code>spark.memory.offHeap.size</code>）</p>
<ul>
<li>存储内存：堆外内存 * 0.5 <code>spark.memory.storageFraction</code></li>
<li>执行内存：堆外内存 * （1 - 0.5 <code>spark.memory.storageFraction</code>）</li>
</ul>
<h3 id="动态占用"><a class="header-anchor" href="#动态占用">¶</a>动态占用</h3>
<p><img src="https://i.loli.net/2020/01/11/FHQD3Y1LohgG8Sf.png" alt="image006.png"></p>
<p>存储内存可以尽可能多地借用执行内存中的free内存，但是当执行池需要这部分内存时，<strong>会把该部分内存池中的对象从内存中驱逐出</strong>，直到满足执行池的内存需求。</p>
<p>执行内存也可以尽可能多地借用存储内存中的free内存，不同的是，<strong>执行内存不会被存储池驱逐出内存</strong>。</p>
<p>也就是说，缓存block时可能会因为执行池占用了大量的内存池，不能释放导致<strong>缓存block失败</strong>，在这种情况下，新的block会<strong>根据StorageLevel做相应处理</strong>（如 spill 磁盘或干脆丢弃以前的）。</p>
<p>注意 <code>MemoryManager </code>里的<code>acquireMemory</code>操作都加了<code>synchronized</code></p>
<h1>MemoryStore</h1>
<p>负责将blocks写入内存及删改查操作，写入结构为反序列化的Java对象数组，或者序列化的ByteBuffers</p>
<h2 id="核心依赖对象——MemoryEntry"><a class="header-anchor" href="#核心依赖对象——MemoryEntry">¶</a>核心依赖对象——MemoryEntry</h2>
<p>块在内存中的抽象表示，也就是数据在存储内存中存储方式，有两种实现：<code>DeserializedMemoryEntry</code>和<code>SerializedMemoryEntry</code></p>
<p><code>DeserializedMemoryEntry</code>：存储的数据结构是Array[T]，只适用于堆内</p>
<p><code>SerializedMemoryEntry</code>：存储的数据结构是<code>ChunkedByteBuffer</code>（物理上存储为多个块而不是单个块上的连续数组），适用于堆内和堆外</p>
<p>采用数组的数据结构很好的<strong>解决存储内存碎片</strong>问题：通常RDD 的 record 是 在 other内存中的是不连续空间，当要缓存时（也就是存到存储内存中），会将其由不连续的存储空间转换为连续的存储空间（数组）。</p>
<h2 id="核心成员"><a class="header-anchor" href="#核心成员">¶</a>核心成员</h2>
<p><strong>entries</strong>：一个 LinkedHashMap （BlockId  -&gt;  MemoryEntry），blocks 存入 MemoryEntry，同时建立 BlockId 和 MemoryEntry 的映射关系。注意LinkedHashMap本身不是线程安全的，因此<strong>对其并发访问都要加锁</strong>。<strong>MemoryStore的核心其实就是对entries的增删改查</strong>。</p>
<p><strong>onHeapUnrollMemoryMap 和 offHeapUnrollMemoryMap</strong>：（BlockId  -&gt;  使用的 UnrollMemory 的大小）</p>
<h2 id="核心方法："><a class="header-anchor" href="#核心方法：">¶</a><strong>核心方法</strong>：</h2>
<h3 id="putBytes"><a class="header-anchor" href="#putBytes">¶</a><strong>putBytes</strong></h3>
<p>序列化ByteBuffer的写入：在写入前，先调用<strong>MemoryManager.acquireStorageMemory()<strong>申请所需的内存，再将</strong>ChunkedByteBuffer</strong>封装进<strong>SerializedMemoryEntry</strong>，最后将该MemoryEntry放入entries映射。</p>
<h3 id="putIterator-T"><a class="header-anchor" href="#putIterator-T">¶</a><strong>putIterator[T]</strong></h3>
<p>迭代器对象的写入，根据入参 <strong>valuesHolder</strong>的不同，决定写成对象还是字节</p>
<p>开始时，先初始化（申请）一部分内存（spark.storage.unrollMemoryThreshold 默认1M），将迭代器对象逐渐存入<strong>valuesHolder</strong>，并且每添加<code>memoryCheckPeriod</code>个元素就<strong>检查一次valuesHolder中数据占用的内存</strong>，如果超过了申请的内存就继续申请<code>UnrollMemory</code>（由<code>memoryGrowthFactor</code>参数控制申请大小，这种递增的方式避免OOM）。将全部元素存入valuesHolder后，使用它的<strong>build方法</strong>生成<code>MemoryEntry</code>（如果是堆内模式内部就是将SizeTrackingVector转成数组），接着release掉申请的unroll内存，最后申请整体大小的Storage内存。化零为整，这个零它就是UnrollMemory，现在知道UnrollMemory是啥了吧，它就是个工具人。</p>
<p>该函数，由下面两个函数调用。</p>
<h3 id="putIteratorAsValues"><a class="header-anchor" href="#putIteratorAsValues">¶</a>putIteratorAsValues</h3>
<p>将迭代器对象 写成对象，使用的<strong>valuesHolder</strong>是 <code>DeserializedValuesHolder</code></p>
<p>该方法的重点把握DeserializedValuesHolder，其内部有两个数据结构：SizeTrackingVector 和 Array；SizeTrackingVector 实现了SizeTracker接口，通过采样估计的方式得到其占用大小（<strong>估计值</strong>）。数据存入DeserializedValuesHolder其实是写入到SizeTrackingVector中，等待全部写完，再使用<strong>getBuilder</strong>方法，将SizeTrackingVector转成Array，再转成DeserializedMemoryEntry。</p>
<h3 id="putIteratorAsBytes"><a class="header-anchor" href="#putIteratorAsBytes">¶</a>putIteratorAsBytes</h3>
<p>将迭代器对象 写成字节，使用的<strong>valuesHolder</strong>是 <code>SerializedValuesHolder</code></p>
<p>SerializedValuesHolder 内部是各种流，且用到了<strong>serializerManager</strong>，暂时不想了解过深，只要知道每次write时流内部会记录size，因此得到的总size是精确的。其内部的build方法，将数据封装成SerializedMemoryEntry。</p>
<h3 id="getBytes、getValues、remove"><a class="header-anchor" href="#getBytes、getValues、remove">¶</a>getBytes、getValues、remove</h3>
<p>读删操作就很简单了，直接根据blockId 对<strong>entries</strong>中的Map进行读删即可</p>
<p>getBytes 的返回值是 ChunkedByteBuffer ，getValues 的返回值是 Iterator[T]，分别对应entry的2种数据结构</p>
<h3 id="evictBlocksToFreeSpace"><a class="header-anchor" href="#evictBlocksToFreeSpace">¶</a>evictBlocksToFreeSpace</h3>
<p>是不是很熟悉这名字，驱除存储内存时用的就是它。</p>
<p><strong>调用时机</strong></p>
<ul>
<li>存储内存不足（自己的不够并且借执行内存的也不够 或者 被执行内存占用很多）</li>
<li>执行内存不足时，需要回收被存储内存占用的</li>
</ul>
<p><strong>回收限制</strong></p>
<ul>
<li>不能是同一个 RDD的 block，避免循环淘汰</li>
<li>memoryMode一致，即同属堆内或者堆外</li>
<li>Block  不能处于被读状态</li>
<li>LRU（最近最少使用）：这是 entries 也就是 LinkedHashMap自带的功能</li>
</ul>
<h1>小结</h1>
<p>MemoryManager 用于对内存划分，同时实现执行内存和存储内存的动态占用。这一切都是逻辑上的，它其实就是计数员，MemoryPool 就是它的账本。当你需要使用执行内存或者存储内存时，你要向它汇报，它会对下帐本告诉你可不可分到，接着对账本修改。</p>
<p>MemoryStore 负责将数据写入<strong>存储内存</strong>以及后续的读取与删除，数据结构就是一个LinkedHashMap。说白了它就是个仓库，存缓存，存广播等等。</p>
<p>你可能会有个疑问，MemoryManager不是划了执行内存和存储内存嘛，都是关于存储内存的，那么执行内存的操作去哪啦？<strong>它与TaskMemoryManager和MemConsumer有关</strong>。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>我的2019</title>
    <url>/2019/12/31/%E9%9A%8F%E7%AC%94/%E6%88%91%E7%9A%842019/</url>
    <content><![CDATA[<p>2019小结，多图慎点</p>
<a id="more"></a>
<h1>github</h1>
<p>一度执着commit，一度觉得太水，一度又执着，一度又… <strong>希望明年加油，争取贡献PR</strong></p>
<p><img src="https://i.loli.net/2019/12/31/UtnVwE4IDSa1cZe.png" alt></p>
<h1>Hexo</h1>
<p>2019年，在小破站写(shui)了<strong>49篇文(shui)章(wen)</strong>。学习上，打60分吧，勉强及格，明年要更加努力！</p>
<p><img src="https://i.loli.net/2019/12/31/ZuN1XwA7nH8ylsf.png" alt></p>
<h1>读书</h1>
<p>欣赏来自豆瓣的蜜汁黑色散点图，一次更新后变成了这样，实在无力吐槽</p>
<p><img src="https://i.loli.net/2019/12/31/PLiUoA8uN4nfsBb.png" alt></p>
<p>微信读书的2019成绩单，<strong>勉强超过平均水平?!</strong> 今年相比去年热情降低了不少，感觉很难遇到让人眼前一亮的书了。</p>
<p>今年看过最好看的3本是：<strong>《平凡的世界》 《霍乱时期的爱情》 《追风筝的人》</strong>。经典不愧是经典～</p>
<p>夸夸<strong>良心</strong>的微信读书：开局一条狗，<strong>无线卡全靠白嫖</strong>。倘若收费，我必奉陪。</p>
<p><img src="https://i.loli.net/2019/12/31/IKZmn3JNlhv8BT7.jpg" alt></p>
<h1>billbill</h1>
<p>9月开始玩的，王者时刻生成后再自己剪一下，投了几篇，自娱自乐加朋友圈形成完美闭环。奈何画(A)质(V)不忍直视，真想放弃，希望<strong>王者的自动剪辑画质能提升点</strong>！</p>
<p>也夸夸B站，在B站上学习了很多免费学习资源，而且用户体验也挺不错的，果然程(si)序(fei)员(zai)懂二次元，奥利给！至于大会员嘛，下次一定，ヾﾉ≧∀≦)o</p>
<p><img src="https://i.loli.net/2019/12/31/EzojgVJNAnXhrCY.jpg" alt></p>
<h1>音乐</h1>
<p>2020年，也要<strong>将非主流贯彻到底</strong>！年度听的最多竟然是嵩哥的，是不是计算错了？？</p>
<p><img src="https://i.loli.net/2019/12/31/Oq9WUB1KQ3gMj7s.jpg" alt></p>
<h1>王者荣耀</h1>
<p>人老了反应跟不上了，混混王者就行，以后玩的时间越来越少了。话说，我的<strong>李白荣耀典藏又跳票了</strong>，<strong>韩信的鼠年传说被伽罗抢了</strong>。🐶 策划，我要转战隔壁lol手游！</p>
<p>貌似没找到王者的2019年报，也太偷懒了吧？？</p>
<h1>2020 加油！</h1>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title>分布式版本控制系统-Git</title>
    <url>/2019/12/31/%E5%B7%A5%E5%85%B7/%E5%88%86%E5%B8%83%E5%BC%8F%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F-Git/</url>
    <content><![CDATA[<p>本文主要包括Git的用法加底层</p>
<h1>使用</h1>
<h2 id="初始化相关"><a class="header-anchor" href="#初始化相关">¶</a>初始化相关</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># --system 操作系统级别； --global 用户级别； 不加： 项目级别 </span></span><br><span class="line">$ git config --global user.name <span class="string">"zouxxyy"</span></span><br><span class="line">$ git config --global user.email <span class="string">"xxxxxxxxxxxxxx"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看配置</span></span><br><span class="line">$ git config --list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建git对象，会生成 .git 目录</span></span><br><span class="line">$ git init</span><br><span class="line">$ ls</span><br><span class="line">HEAD                 指示当前分支的文件</span><br><span class="line">config               配置文件</span><br><span class="line">hooks.  （目录）      存放钩子脚本，比如提交代码前或者后的自动执行操作</span><br><span class="line">objects （目录）      存放所有历史记录的</span><br><span class="line">branches             指示目前被检测出的分支</span><br><span class="line">description          仓库描述性信息的文件</span><br><span class="line">info    （目录）      存放全局性的排除文件。比如 mac 进去查看就有.DS_Store</span><br><span class="line">refs    （目录）      存放分支与tags的提交对象的指针</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="文件相关"><a class="header-anchor" href="#文件相关">¶</a>文件相关</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加新文件（由未跟踪到跟踪状态，同时也是暂存状态）或者添加修改文件（由修改状态到暂存状态）（粗略这样说，其实底层做了更多事）</span></span><br><span class="line">$ git add test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交文件（由暂存状态到提交状态）；可加参数 -a 跳过 add 步骤（未跟踪的不能提交）</span></span><br><span class="line">$ git commit -m <span class="string">"commit message"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件的状态</span></span><br><span class="line">$ git status</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看未被 add 的修改</span></span><br><span class="line">$ git diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看已 add 但未被 commit 的修改</span></span><br><span class="line">$ git diff --staged</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件 （同时让文件由跟踪变成未跟踪状态）（注意它 和手动删除文件再执行 git add 效果一样）</span></span><br><span class="line">$ git rm test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重命名</span></span><br><span class="line">$ git mv test.txt new.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 commit log；可加 --oneline 参数简写</span></span><br><span class="line">$ git <span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 完整 commit log（只要改动了HEAD）</span></span><br><span class="line">$ git reflog</span><br></pre></td></tr></table></figure>
<h2 id="分支相关"><a class="header-anchor" href="#分支相关">¶</a>分支相关</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建分支</span></span><br><span class="line">$ git branch branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换分支 (切换前保证 status 干净)</span></span><br><span class="line">$ git checkout branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并切换</span></span><br><span class="line">$ git checkout -b branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示所有分支 可加参数 -v 查看它们的最后一次提交</span></span><br><span class="line">$ git branch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除分支（需先切到另一个分支）可加参数 -D 强制删除</span></span><br><span class="line">$ git branch -d branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建分支，并指向指定的提交对象 (时光机)</span></span><br><span class="line">$ git branch branchname commitHash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分支合并 （合并前切回主分支，再合并需要合并的分支）</span></span><br><span class="line"><span class="comment"># 合并可能会产生冲突，需要手动修改冲突的文件，再提交</span></span><br><span class="line">$ git merge otherBranch</span><br></pre></td></tr></table></figure>
<h2 id="stash相关"><a class="header-anchor" href="#stash相关">¶</a>stash相关</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 有时想切分支，但又不想提交，可以用到 stash 功能，是一种暂存的栈。这个栈是针对分支的～</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前分支的 stash</span></span><br><span class="line">$ git stash list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将未提交的改动 入栈（接着可以安全执行切分支操作）</span></span><br><span class="line">$ git stash</span><br><span class="line"></span><br><span class="line"><span class="comment"># （切回分支后）弹出 改动</span></span><br><span class="line">$ git pop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后面两个一般不用，我们栈里只放一个元素，git stash 入栈，git pop 出栈</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只取指定改动，不删除</span></span><br><span class="line">$ git stash apply stashName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定的栈里的东西</span></span><br><span class="line">$ git stash drop stashName</span><br></pre></td></tr></table></figure>
<h2 id="回退相关"><a class="header-anchor" href="#回退相关">¶</a>回退相关</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 撤回工作目录的修改（未add -&gt; 未修改）</span></span><br><span class="line">$ git checkout -- fileName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回暂存区的修改 （未commit -&gt; 未add）</span></span><br><span class="line">$ git reset [HEAD] fileName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可用于修改刚刚提交的message（相当于后退一步，再重新提交）</span></span><br><span class="line">$ git commit --amend</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意下面3个命令都可以撤回到指定提交对象（把 HEAD~ 改为指定的 commitHash 即可）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回上一次提交 （commit -&gt; 未commit）</span></span><br><span class="line">$ git reset --soft HEAD~</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回上一次提交同时撤回暂存区的修改 （commit -&gt; 未add）</span></span><br><span class="line">$ git reset [--mix] HEAD~</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回上一次提交同时撤回暂存区的修改同时撤回工作目录的修改 （commit -&gt; 未修改，危险！）</span></span><br><span class="line">$ git reset --hard HEAD~</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回少用 reset --hard，最好使用 branch </span></span><br><span class="line">$ git branch recoverBranchName commitHash</span><br></pre></td></tr></table></figure>
<h2 id="标签相关"><a class="header-anchor" href="#标签相关">¶</a>标签相关</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 列出所有tag</span></span><br><span class="line">$ git tag</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打tag</span></span><br><span class="line">$ git tag tagName commitHash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定tag</span></span><br><span class="line">$ git tag -d tagName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切到指定tag，然后创新分支，防止头部分离</span></span><br><span class="line">$ git checkout tagName</span><br><span class="line">$ git checkout -b branchName</span><br></pre></td></tr></table></figure>
<h2 id="远程相关"><a class="header-anchor" href="#远程相关">¶</a>远程相关</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看全部远程分支</span></span><br><span class="line">$ git remote -v</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加远程分支别名(注意：orgin 只个别名)</span></span><br><span class="line">$ git remote add orgin https://xxxx.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 克隆远程仓库到本地（此时自动创建全部远程跟踪分支，同时生成一个关联远程master的本地分支（注意））</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://xxxx.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由本地分支创建远程跟踪分支，再推到远程分支，但并不关联（注意）</span></span><br><span class="line"><span class="comment"># 推荐仅当创建一个新分支，并想把它推到远程时，才使用它；并且接着执行关联操作，也就下一行，之后仅用 git push 即可</span></span><br><span class="line">$ git push orgin branchName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本地分支（已创建）关联远程跟踪分支</span></span><br><span class="line">$ git branch -u orgin/branchName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取远程分支（全部分支）到远程跟踪分支中</span></span><br><span class="line"><span class="comment"># 推荐仅当远程有了新分支时，才使用它（注意）</span></span><br><span class="line">$ git fetch orgin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个本地分支，同时关联远程跟踪分支</span></span><br><span class="line">$ git branch -b branchName orgin/branchName</span><br><span class="line">$ git branch --track orgin/branchName (效果一样，快捷写法)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉去远程分支到远程跟踪分支和本地分支（本地分支已经关联远程跟踪分支），相当于执行 git fetch + git merge</span></span><br><span class="line">$ git pull</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除远程分支</span></span><br><span class="line">$ git push orgin --delete branchName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出没用远程跟踪分支，并删除</span></span><br><span class="line">$ git remote prune orgin --dry-run</span><br><span class="line">$ git remote orgin</span><br></pre></td></tr></table></figure>
<p>看起来比较复杂，但只要记住一个逻辑链就好，也就是按以下3步走：</p>
<ol>
<li>
<p><strong>建立远程跟踪分支</strong></p>
<ul>
<li><code>git clone https://xxxx.git</code></li>
<li><code>git push orgin branchName</code> 推<strong>新分支</strong>到远程</li>
<li><code>git fetch orgin</code> 拉远程的<strong>新分支</strong></li>
</ul>
</li>
<li>
<p><strong>本地分支关联远程跟踪分支</strong></p>
<ul>
<li><code>git checkout branchName</code> 创建本地新分支（和远程分支同名） 并关联 (还是它简单)</li>
<li><code>git branch -u orgin/branchName</code> 当前分支 关联</li>
<li><code>git branch --track orgin/branchName</code> 创建本地新分支 关联</li>
</ul>
</li>
<li>
<p><strong>愉快使用</strong></p>
<ul>
<li><code>git pull</code> 拉到本地</li>
<li><code>git push</code> 推到远程（先 pull，解决冲突再 push）</li>
</ul>
</li>
</ol>
<p>上面是自己团队的项目（<strong>有权限</strong>），当想为开源项目做贡献（<strong>没权限</strong>）时，使用 <strong>pull request</strong>， 大致流程（用到再研究）为：</p>
<ol>
<li>
<p><strong>folk 一份</strong></p>
</li>
<li>
<p><strong>在 folk 的库中，作出改动并提交，注意提交前先拉远程库，保持最新。</strong></p>
</li>
<li>
<p><strong>进网站（如github），点 pullRequest</strong></p>
</li>
</ol>
<h1>底层</h1>
<p>git 的底层数据结构是一种在 <code>object</code> 目录中存放的 k-v 类型的数据。key 是 hash值 (前两位是子文件夹名，后面是文件名)，value 是 数据内容。</p>
<p>数据内容 分为 <strong>git 对象</strong>（<code>blob</code>）、<strong>树对象</strong>（<code>tree</code>）、<strong>提交对象</strong>（<code>commit</code>）</p>
<h2 id="git-对象"><a class="header-anchor" href="#git-对象">¶</a>git 对象</h2>
<p><strong>代表文件，它是真正存数据的</strong>。它是一对一的（注意：<strong>一个文件的历史文件都算一个单独的文件</strong>）。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 向数据库中写数据，并返回hash键值。存的是一个blob对象</span></span><br><span class="line">$ git <span class="built_in">hash</span>-object -w test.txt</span><br><span class="line">915c628f360b2d8c3edbe1ac65cf575b69029b61</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据是经过压缩的，可以用cat-file查看该数据</span></span><br><span class="line"><span class="comment"># -p 显示内容； -t 查看类型</span></span><br><span class="line">$ git cat-file -p 915c628f360b2d8c3edbe1ac65cf575b69029b61</span><br><span class="line">hello git</span><br></pre></td></tr></table></figure>
<h2 id="树对象"><a class="header-anchor" href="#树对象">¶</a>树对象</h2>
<p><strong>代表版本</strong>，可以理解成一个树对象指向多个 git 对象，形成一个版本。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 暂存区 写入 test.txt 的首个版本（指针信息）</span></span><br><span class="line"><span class="comment"># --add 文件首次添加；--cacheinfo 表示添加的文件位于git数据库；100644 文件类型； test.txt 文件名</span></span><br><span class="line">$ git update-index --add --cacheinfo 100644 915c628f360b2d8c3edbe1ac65cf575b69029b61 test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看暂存区</span></span><br><span class="line">$ git ls-files -s</span><br><span class="line">100644 915c628f360b2d8c3edbe1ac65cf575b69029b61 0 test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成树对象。暂存区未清空</span></span><br><span class="line">$ git write-tree	</span><br><span class="line">72203871fa4668ad777833634034dcd3426879db</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看树对象内容</span></span><br><span class="line">$ git cat-file -p 72203871fa4668ad777833634034dcd3426879db</span><br><span class="line">100644 blob 915c628f360b2d8c3edbe1ac65cf575b69029b61 test.txt</span><br></pre></td></tr></table></figure>
<h2 id="提交对象"><a class="header-anchor" href="#提交对象">¶</a>提交对象</h2>
<p><strong>相当于对版本添加描述信息</strong>。一个提交对象封装一个树对象，而且它是<strong>链式</strong>的，里面指向上一个提交对象。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成提交对象，后接树对象hash值</span></span><br><span class="line"><span class="comment"># 这里是第一次提交。以后可以加 -p 指定父提交对象</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">'first commit'</span> | git commit-tree 72203871fa4668ad777833634034dcd3426879db</span><br><span class="line">5402d3560f35d66f7f1e4e4665dd63bf3d786495</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看提交对象的内容</span></span><br><span class="line">$ git cat-file -p 5402d3560f35d66f7f1e4e4665dd63bf3d786495</span><br><span class="line">tree 72203871fa4668ad777833634034dcd3426879db</span><br><span class="line">author zouxxyy &lt;xxxxxxxxx@qq.com&gt; 1575450654 +0800</span><br><span class="line">committer zouxxyy &lt;xxxxxxxxx@qq.com&gt; 1575450654 +0800</span><br><span class="line"></span><br><span class="line">first commit</span><br></pre></td></tr></table></figure>
<h2 id="文件相关-v2"><a class="header-anchor" href="#文件相关-v2">¶</a>文件相关</h2>
<ul>
<li>
<p><code>git add test.txt</code> 相当于先将文件做成<code>git对象</code>存入<code>object</code> ，再将它（指针信息）放入暂存区，等待提交。该操作是<strong>绝对安全</strong>的，因为数据已经写入文件中。所以可以理解为啥只有 <code>git add</code>，而没有什么<code>git change</code>之类的，因为修改也会加<code>git对象</code>。</p>
</li>
<li>
<p><code>git commit -m &quot;xx&quot;</code> 由暂存区生成树对象，再生成提交对象。<strong>注意暂存区的东西不删除</strong></p>
</li>
<li>
<p><code>git rm test.txt</code> <strong>相当于将该文件对应的（指针信息）从暂存区中删除，同时删除工作目录中的文件</strong></p>
</li>
</ul>
<h2 id="分支相关-v2"><a class="header-anchor" href="#分支相关-v2">¶</a>分支相关</h2>
<p><strong>分支切换会切换工作目录</strong>，所以切换前，先把当前分支该提交的提交了，<strong>保证工作目录干净</strong>。</p>
<ul>
<li>
<p>2个重要的东西：</p>
<ul>
<li><code>refs</code> 存放各个分支（和tags）的提交对象的指针（hashkey）</li>
<li><code>HEAD</code> 指明当前分支的指针位置</li>
</ul>
</li>
<li>
<p><code>git branch branchname</code> 相当于在 <code>refs/heads</code> 目录中新建以<strong>分支名</strong>命名的当前分支的提交对象的指针</p>
</li>
<li>
<p><code>git checkout branchname</code> 更改<code>HEAD</code>文件；<strong>切换工作目录</strong>；<strong>影响暂存区</strong></p>
</li>
<li>
<p><code>git commit -m &quot;commit message&quot;</code> 更新 <code>refs</code> 中对应分支的提交对象</p>
</li>
</ul>
<h2 id="回退相关-v2"><a class="header-anchor" href="#回退相关-v2">¶</a>回退相关</h2>
<p><code>git checkout branchName</code> 和 <code>git reset --hard commitHash</code> 的区别 ：</p>
<ul>
<li>
<p><code>checkout</code> 只改HEAD ；<code>reset</code> HEAD 和 分支指针的一起改</p>
</li>
<li>
<p><code>checkout</code> 对工作目录是安全的；<code>reset --hard</code> 会<strong>完全强制覆盖工作目录</strong></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>spark源码-shuffle之BlockStoreShuffleReader</title>
    <url>/2019/12/30/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BBlockStoreShuffleReader/</url>
    <content><![CDATA[<p>spark的唯一 ShuffleReader ：BlockStoreShuffleReader</p>
<a id="more"></a>
<h1>引入</h1>
<p><a href="https://zouxxyy.github.io/2019/11/29/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BShuffleManager/" target="_blank" rel="noopener">之前</a>提过，<code>getShuffleWrite</code> 只在 <code>ShuffleMapTask</code> 中出现。</p>
<p>那么<code>getShuffleRead</code>呢？由于不管哪个 Task 都需要读数据，于是就把该步骤封装在RDD的<code>computer</code>方法中。以下是<code>ShuffleRDD</code>中的<code>computer</code>方法。通过调用 <code>shuffleManager</code> 的 <code>getReader</code> 就获取了本文的主角<code>BlockStoreShuffleReader</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">  <span class="keyword">val</span> dep = dependencies.head.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]]</span><br><span class="line">  <span class="comment">// 是不是觉得 split.index, split.index + 1 很怪，后面会发现这是怎么回事</span></span><br><span class="line">  <span class="type">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class="number">1</span>, context)</span><br><span class="line">    .read()</span><br><span class="line">    .asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>大致流程</h1>
<ol>
<li>
<p><strong>获取初始迭代器<code>ShuffleBlockFetcherIterator</code></strong>（<code>Iterator[(BlockId, InputStream)</code>）</p>
</li>
<li>
<p><strong>反序列化出实例，并生成 k,v 迭代器</strong>（<code>Iterator[(key, value)]</code>）</p>
</li>
<li>
<p><strong>添加readMetrics，再封装成<code>InterruptibleIterator</code></strong></p>
</li>
<li>
<p><strong>进行聚合操作</strong>。分有无聚合，当有聚合时，又分是否执行过map聚合</p>
</li>
<li>
<p><strong>进行排序操作</strong>。分有无排序。</p>
</li>
</ol>
<p><code>BlockStoreShuffleReader </code> 要干的事其实很容易理解，就是把<strong>不同 block 中同一分区的record</strong>，拉到指定的 reducer 中，再对它进行<strong>聚合和排序</strong>即可。一个 reducer 处理一个分区。</p>
<h1>源码</h1>
<h2 id="获取初始迭代器ShuffleBlockFetcherIterator"><a class="header-anchor" href="#获取初始迭代器ShuffleBlockFetcherIterator">¶</a>获取初始迭代器<code>ShuffleBlockFetcherIterator</code></h2>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取初始迭代器 ShuffleBlockFetcherIterator</span></span><br><span class="line"><span class="keyword">val</span> wrappedStreams = <span class="keyword">new</span> <span class="type">ShuffleBlockFetcherIterator</span>(</span><br><span class="line">  context,</span><br><span class="line">  blockManager.shuffleClient, <span class="comment">// 默认是 NettyBlockTransferService，如果使用外部shuffle系统则使用 ExternalShuffleClient</span></span><br><span class="line">  blockManager,</span><br><span class="line">  <span class="comment">// 通过它得到 2元tuple : (BlockManagerId, Seq[(BlockId, BlockSize)])</span></span><br><span class="line">  mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),</span><br><span class="line">  serializerManager.wrapStream,</span><br><span class="line">  <span class="comment">// Note: we use getSizeAsMb when no suffix is provided for backwards compatibility</span></span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.getSizeAsMb(<span class="string">"spark.reducer.maxSizeInFlight"</span>, <span class="string">"48m"</span>) * <span class="number">1024</span> * <span class="number">1024</span>,</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.getInt(<span class="string">"spark.reducer.maxReqsInFlight"</span>, <span class="type">Int</span>.<span class="type">MaxValue</span>),</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.get(config.<span class="type">REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS</span>),</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.get(config.<span class="type">MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM</span>),</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.getBoolean(<span class="string">"spark.shuffle.detectCorrupt"</span>, <span class="literal">true</span>))</span><br></pre></td></tr></table></figure>
<p>这是 <code>mapOutputTracker.getMapSizesByExecutorId</code> 里的关键步骤</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convertMapStatuses</span></span>(</span><br><span class="line">    shuffleId: <span class="type">Int</span>,</span><br><span class="line">    startPartition: <span class="type">Int</span>,</span><br><span class="line">    endPartition: <span class="type">Int</span>,</span><br><span class="line">    statuses: <span class="type">Array</span>[<span class="type">MapStatus</span>]): <span class="type">Iterator</span>[(<span class="type">BlockManagerId</span>, <span class="type">Seq</span>[(<span class="type">BlockId</span>, <span class="type">Long</span>)])] = &#123;</span><br><span class="line">  assert (statuses != <span class="literal">null</span>)</span><br><span class="line">  <span class="keyword">val</span> splitsByAddress = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">BlockManagerId</span>, <span class="type">ListBuffer</span>[(<span class="type">BlockId</span>, <span class="type">Long</span>)]]</span><br><span class="line">  <span class="comment">// mapTask 的个数决定了 Seq[(BlockId, BlockSize)] 内元素的个数，很好理解</span></span><br><span class="line">  <span class="keyword">for</span> ((status, mapId) &lt;- statuses.iterator.zipWithIndex) &#123;</span><br><span class="line">    <span class="keyword">if</span> (status == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> errorMessage = <span class="string">s"Missing an output location for shuffle <span class="subst">$shuffleId</span>"</span></span><br><span class="line">      logError(errorMessage)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">MetadataFetchFailedException</span>(shuffleId, startPartition, errorMessage)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 左闭右开，因此前面的 （split.index, split.index + 1）中的 split.index + 1，并没有什么软用  </span></span><br><span class="line">      <span class="keyword">for</span> (part &lt;- startPartition until endPartition) &#123;</span><br><span class="line">        <span class="keyword">val</span> size = status.getSizeForBlock(part)</span><br><span class="line">        <span class="keyword">if</span> (size != <span class="number">0</span>) &#123;</span><br><span class="line">          splitsByAddress.getOrElseUpdate(status.location, <span class="type">ListBuffer</span>()) +=</span><br><span class="line">              ((<span class="type">ShuffleBlockId</span>(shuffleId, mapId, part), size))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  splitsByAddress.iterator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>ShuffleBlockFetcherIterator  </code>这个迭代器的具体实现比较复杂，简单介绍下：</p>
<ul>
<li>
<p>分本地数据块 和 远程数据块</p>
</li>
<li>
<p>本地数据块直接调用 <code>BlockManager.getBlockData</code></p>
</li>
<li>
<p>远程数据块采用Netty通过网络获取</p>
</li>
</ul>
<p>下面是 <code>IndexShuffleBlockResolver</code> 中的 <code>getBlockData</code>。我们的索引文件（indexFile）终于派上用场</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getBlockData</span></span>(blockId: <span class="type">ShuffleBlockId</span>): <span class="type">ManagedBuffer</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> indexFile = getIndexFile(blockId.shuffleId, blockId.mapId)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> channel = <span class="type">Files</span>.newByteChannel(indexFile.toPath)</span><br><span class="line">  <span class="comment">// 根据reduceId选择索引</span></span><br><span class="line">  channel.position(blockId.reduceId * <span class="number">8</span>L)</span><br><span class="line">  <span class="keyword">val</span> in = <span class="keyword">new</span> <span class="type">DataInputStream</span>(<span class="type">Channels</span>.newInputStream(channel))</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> offset = in.readLong()</span><br><span class="line">    <span class="keyword">val</span> nextOffset = in.readLong()</span><br><span class="line">    <span class="keyword">val</span> actualPosition = channel.position()</span><br><span class="line">    <span class="keyword">val</span> expectedPosition = blockId.reduceId * <span class="number">8</span>L + <span class="number">16</span></span><br><span class="line">    <span class="keyword">if</span> (actualPosition != expectedPosition) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">s"SPARK-22982: Incorrect channel position after index file reads: "</span> +</span><br><span class="line">        <span class="string">s"expected <span class="subst">$expectedPosition</span> but actual position was <span class="subst">$actualPosition</span>."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FileSegmentManagedBuffer</span>(</span><br><span class="line">      transportConf,</span><br><span class="line">      getDataFile(blockId.shuffleId, blockId.mapId),</span><br><span class="line">      offset,</span><br><span class="line">      nextOffset - offset)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    in.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="反序列化出实例，并生成-k-v-迭代器"><a class="header-anchor" href="#反序列化出实例，并生成-k-v-迭代器">¶</a>反序列化出实例，并生成 k,v 迭代器</h2>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> serializerInstance = dep.serializer.newInstance()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a key/value iterator for each stream</span></span><br><span class="line"><span class="keyword">val</span> recordIter = wrappedStreams.flatMap &#123; <span class="keyword">case</span> (blockId, wrappedStream) =&gt;</span><br><span class="line">  serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="添加readMetrics，再封装成InterruptibleIterator"><a class="header-anchor" href="#添加readMetrics，再封装成InterruptibleIterator">¶</a>添加readMetrics，再封装成<code>InterruptibleIterator</code></h2>
<p>readMetrics 里都是些记录数据，用于监控展示</p>
<p><code>InterruptibleIterator</code> 封装了任务中断功能</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Update the context task metrics for each record read.</span></span><br><span class="line"><span class="keyword">val</span> readMetrics = context.taskMetrics.createTempShuffleReadMetrics()</span><br><span class="line"><span class="keyword">val</span> metricIter = <span class="type">CompletionIterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>), <span class="type">Iterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>)]](</span><br><span class="line">  recordIter.map &#123; record =&gt;</span><br><span class="line">    readMetrics.incRecordsRead(<span class="number">1</span>) <span class="comment">// sparkUI 里的 record 数</span></span><br><span class="line">    record</span><br><span class="line">  &#125;,</span><br><span class="line">  context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> interruptibleIter = <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>)](context, metricIter)</span><br></pre></td></tr></table></figure>
<h2 id="进行聚合操作"><a class="header-anchor" href="#进行聚合操作">¶</a>进行聚合操作</h2>
<p>分有无聚合，当有聚合时，又分是否在 mapTask 时执行过map聚合</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> aggregatedIter: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = <span class="keyword">if</span> (dep.aggregator.isDefined) &#123;</span><br><span class="line">  <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="comment">// We are reading values that are already combined</span></span><br><span class="line">    <span class="comment">// 有 mapSideCombine，就是聚合(K, C)</span></span><br><span class="line">    <span class="keyword">val</span> combinedKeyValuesIterator = interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">    dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 无 mapSideCombine，就是聚合(K, V)</span></span><br><span class="line">    <span class="keyword">val</span> keyValuesIterator = interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">Nothing</span>)]]</span><br><span class="line">    dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">// 不聚合</span></span><br><span class="line">  interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="进行排序操作"><a class="header-anchor" href="#进行排序操作">¶</a>进行排序操作</h2>
<p>分是否需要排序。如果需要，就使用<code>ExternalSorter</code>在分区内部进行排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> resultIter = dep.keyOrdering <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Some</span>(keyOrd: <span class="type">Ordering</span>[<span class="type">K</span>]) =&gt;</span><br><span class="line">    <span class="comment">// Create an ExternalSorter to sort the data.</span></span><br><span class="line">    <span class="keyword">val</span> sorter =</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">C</span>, <span class="type">C</span>](context, ordering = <span class="type">Some</span>(keyOrd), serializer = dep.serializer)</span><br><span class="line">    sorter.insertAll(aggregatedIter)</span><br><span class="line">    context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class="line">    context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class="line">    context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class="line">    <span class="comment">// Use completion callback to stop sorter if task was finished/cancelled.</span></span><br><span class="line">    context.addTaskCompletionListener[<span class="type">Unit</span>](_ =&gt; &#123;</span><br><span class="line">      sorter.stop()</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="type">CompletionIterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>], <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]](sorter.iterator, sorter.stop())</span><br><span class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">    aggregatedIter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark杂谈-指定文件在HDFS中的写入节点</title>
    <url>/2019/12/28/spark/spark%E6%9D%82%E8%B0%88-%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6%E5%9C%A8HDFS%E4%B8%AD%E7%9A%84%E5%86%99%E5%85%A5%E8%8A%82%E7%82%B9/</url>
    <content><![CDATA[<p>项目中需要指定文件写到HDFS的具体哪个节点，通过查找API，找到一种解决办法。以 <code>TextOutputFormat</code> 为例测试</p>
<a id="more"></a>
<h1>write</h1>
<p>观察 <code>TextOutputFormat</code> 里的 <code>write</code> 方法，它使用了<code>out.write</code>将 record 写入 HDFS。找到 <code>out</code> ，修改它</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> nullKey = key == <span class="keyword">null</span> || key <span class="keyword">instanceof</span> NullWritable;</span><br><span class="line">  <span class="keyword">boolean</span> nullValue = value == <span class="keyword">null</span> || value <span class="keyword">instanceof</span> NullWritable;</span><br><span class="line">  <span class="keyword">if</span> (nullKey &amp;&amp; nullValue) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!nullKey) &#123;</span><br><span class="line">    writeObject(key);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!(nullKey || nullValue)) &#123;</span><br><span class="line">    out.write(keyValueSeparator);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!nullValue) &#123;</span><br><span class="line">    writeObject(value);</span><br><span class="line">  &#125;</span><br><span class="line">  out.write(newline);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>getRecordWriter</h1>
<p><code>out</code> 是一个<code>DataOutputStream</code>，它通过 <code>getRecordWriter</code>得到的。</p>
<p>默认是由 <code>FSDataOutputStream fileOut = fs.create(file, progress;</code> 生成，但它无法指定节点。</p>
<p>作出修改：</p>
<p><strong>关键是把它替换成 <code>DistributedFileSystem</code> 里的 <code>create</code> 方法，其中有个参数是 <code>favoredNodes </code>，顾名思义数据将优先存到它指定节点</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRecordWriter</span></span>(ignored: <span class="type">FileSystem</span>, job: <span class="type">JobConf</span>, name: <span class="type">String</span>, progress: <span class="type">Progressable</span>): <span class="type">RecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> keyValueSeparator = job.get(<span class="string">"mapreduce.output.textoutputformat.separator"</span>, <span class="string">"\t"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> file = <span class="type">FileOutputFormat</span>.getTaskOutputPath(job, name)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将 OutputStream 转成 DistributedFileSystem</span></span><br><span class="line">  <span class="keyword">val</span> fs = file.getFileSystem(job).asInstanceOf[<span class="type">DistributedFileSystem</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> favoredNodes = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">InetSocketAddress</span>](<span class="number">3</span>)</span><br><span class="line">	</span><br><span class="line">  <span class="comment">// 这里自己指定</span></span><br><span class="line">  favoredNodes(<span class="number">0</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.12"</span>, <span class="number">9866</span>) <span class="comment">// 这个port是该节点dataNode的port</span></span><br><span class="line">  favoredNodes(<span class="number">1</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.13"</span>, <span class="number">9866</span>)</span><br><span class="line">  favoredNodes(<span class="number">2</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.17"</span>, <span class="number">9866</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fileOut: <span class="type">FSDataOutputStream</span> = fs.create(file,</span><br><span class="line">    <span class="type">FsPermission</span>.getFileDefault.applyUMask(<span class="type">FsPermission</span>.getUMask(fs.getConf)),</span><br><span class="line">    <span class="literal">true</span>,</span><br><span class="line">    fs.getConf.getInt(<span class="type">IO_FILE_BUFFER_SIZE_KEY</span>, <span class="type">IO_FILE_BUFFER_SIZE_DEFAULT</span>),</span><br><span class="line">    fs.getDefaultReplication(file),</span><br><span class="line">    fs.getDefaultBlockSize(file),</span><br><span class="line">    <span class="literal">null</span>,</span><br><span class="line">    favoredNodes)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">TextOutputFormat</span>.<span class="type">LineRecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>](fileOut, keyValueSeparator)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>完整测试代码</h1>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.examples.zxyTest</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.<span class="type">InetSocketAddress</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.<span class="type">CommonConfigurationKeysPublic</span>.&#123;<span class="type">IO_FILE_BUFFER_SIZE_DEFAULT</span>, <span class="type">IO_FILE_BUFFER_SIZE_KEY</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.permission.<span class="type">FsPermission</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FSDataOutputStream</span>, <span class="type">FileSystem</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hdfs.<span class="type">DistributedFileSystem</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.&#123;<span class="type">IntWritable</span>, <span class="type">Text</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.&#123;<span class="type">FileOutputFormat</span>, <span class="type">JobConf</span>, <span class="type">RecordWriter</span>, <span class="type">TextOutputFormat</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.<span class="type">Progressable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 测试 指定 HDFS 存储节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FavoredNodesTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder</span><br><span class="line">      .appName(<span class="string">"FavoredNodesTest"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .config(<span class="string">"spark.driver.memory"</span>, <span class="string">"2g"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>((<span class="string">"A"</span>, <span class="number">1</span>), (<span class="string">"B"</span>, <span class="number">2</span>), (<span class="string">"C"</span>, <span class="number">3</span>), (<span class="string">"D"</span>, <span class="number">4</span>)), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    rdd.saveAsHadoopFile(<span class="string">"hdfs://dell-r720/zxyTest/output"</span>,</span><br><span class="line">      classOf[<span class="type">Text</span>],</span><br><span class="line">      classOf[<span class="type">IntWritable</span>],</span><br><span class="line">      classOf[<span class="type">MyTextOutputFormat</span>[<span class="type">Text</span>, <span class="type">IntWritable</span>]])</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"Result has been saved"</span>)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTextOutputFormat</span>[<span class="type">K</span>, <span class="type">V</span>] <span class="keyword">extends</span> <span class="title">TextOutputFormat</span>[<span class="type">K</span>, <span class="type">V</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRecordWriter</span></span>(ignored: <span class="type">FileSystem</span>, job: <span class="type">JobConf</span>, name: <span class="type">String</span>, progress: <span class="type">Progressable</span>): <span class="type">RecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> keyValueSeparator = job.get(<span class="string">"mapreduce.output.textoutputformat.separator"</span>, <span class="string">"\t"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> file = <span class="type">FileOutputFormat</span>.getTaskOutputPath(job, name)</span><br><span class="line">    <span class="keyword">val</span> fs = file.getFileSystem(job).asInstanceOf[<span class="type">DistributedFileSystem</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3 副本位置</span></span><br><span class="line">    <span class="keyword">val</span> favoredNodes = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">InetSocketAddress</span>](<span class="number">3</span>)</span><br><span class="line">    favoredNodes(<span class="number">0</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.12"</span>, <span class="number">9866</span>) <span class="comment">// 9866是该节点dataNode的port</span></span><br><span class="line">    favoredNodes(<span class="number">1</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.13"</span>, <span class="number">9866</span>)</span><br><span class="line">    favoredNodes(<span class="number">2</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.17"</span>, <span class="number">9866</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileOut: <span class="type">FSDataOutputStream</span> = fs.create(file,</span><br><span class="line">      <span class="type">FsPermission</span>.getFileDefault.applyUMask(<span class="type">FsPermission</span>.getUMask(fs.getConf)),</span><br><span class="line">      <span class="literal">true</span>,</span><br><span class="line">      fs.getConf.getInt(<span class="type">IO_FILE_BUFFER_SIZE_KEY</span>, <span class="type">IO_FILE_BUFFER_SIZE_DEFAULT</span>),</span><br><span class="line">      fs.getDefaultReplication(file),</span><br><span class="line">      fs.getDefaultBlockSize(file),</span><br><span class="line">      <span class="literal">null</span>,</span><br><span class="line">      favoredNodes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">TextOutputFormat</span>.<span class="type">LineRecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>](fileOut, keyValueSeparator)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>自动化运维工具-Ansible</title>
    <url>/2019/12/24/%E5%B7%A5%E5%85%B7/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7-Ansible/</url>
    <content><![CDATA[<p>Ansible 是一个开源的基于 OpenSSH 的自动化配置管理工具。可以用它来配置系统、部署软件和编排更高级的 IT 任务，比如持续部署或零停机更新。管理员可以通过 Ansible 在成百上千台计算机上同时执行指令(任务)，这是<a href="https://ansible-tran.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">官方文档</a>。</p>
<a id="more"></a>
<h1>配置组</h1>
<p>Ansible 可同时操作属于一个组的多台主机，在 <code>/etc/ansible/hosts</code> 中配置组</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim /etc/ansible/hosts</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用法：[组名] + hostnames</span></span><br><span class="line">[spark]</span><br><span class="line">dell-r730-[1:4]</span><br><span class="line"></span><br><span class="line">[hdfs]</span><br><span class="line">dell-r730-[1:4]</span><br><span class="line">dell-r730-7</span><br></pre></td></tr></table></figure>
<p>使用时可分为 <strong>ad-hoc</strong> 和 <strong>playbook</strong></p>
<h1>ad-hoc</h1>
<p>在命令行敲入的shell命令，去执行些简单的任务：</p>
<p><code>ansible 组名 -m 模块名 -a 具体操作 [-u 用户名] [--sudo] [-f 10]</code></p>
<ul>
<li><code>-m</code> 模块名</li>
<li><code>-a</code> 具体操作，<strong>一般是k v结构</strong>。</li>
<li><code>-u</code> 默认以当前用户的身份执行命令，也可手动指定</li>
<li><code>--sudo</code> 通过 sudo 去执行命令（ passwordless 模式 ）</li>
<li><code>-f</code> 并发量</li>
</ul>
<p>下面介绍些常用模块</p>
<h2 id="commond-和-shell"><a class="header-anchor" href="#commond-和-shell">¶</a>commond 和 shell</h2>
<p><code>commond </code>是默认的模块</p>
<p><code>commond</code> 和 <code>shell</code> 都是直接敲命令的。<code>command</code> 更安全，但<strong>不支持 shell 变量，也不支持管道等 shell 相关的东西</strong></p>
<p>shell使用变量时也存在限制，所以尽量<strong>不要敲太复杂的命令</strong>。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打印当前时间</span></span><br><span class="line">$ ansible zxy -a <span class="string">"date"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单管道命令</span></span><br><span class="line">$ ansible zxy -m shell -a <span class="string">"cat /etc/hosts &gt; test.txt"</span></span><br></pre></td></tr></table></figure>
<h2 id="file-和-copy"><a class="header-anchor" href="#file-和-copy">¶</a>file 和 copy</h2>
<p><code>file</code> 用于创建（删除）文件或文件夹，也可更改所有者权限<br>
<code>copy</code> 用与复制文件<br>
（我觉得有点麻烦，没有commnd来的快点）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建文件（如果存在，更新时间戳）</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testfile state=touch"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建目录（如果存在，不进行任何操作）</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testdir state=directory"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件或目录</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testdir state=absent"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可加 owner 定义所有者，mode 定义 权限，recurse 对目录递归。</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testdir state=directory owner=cluster mode=0777 recurse=true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制文件</span></span><br><span class="line"><span class="comment"># force=no 不强制覆盖，backup=no 不备份旧文件</span></span><br><span class="line">$ ansible zxy -m copy -a <span class="string">"src=/home/cluster/testfile dest=/home/cluster/testfile2 owner=cluster mode=0777"</span></span><br></pre></td></tr></table></figure>
<h2 id="synchronize"><a class="header-anchor" href="#synchronize">¶</a>synchronize</h2>
<p>文件或文件夹同步，有 push（默认）和 pull 模式。</p>
<p>默认启用了<code>archive</code>参数，该参数默认开启了recursive, links, perms, times, owner，group和-D参数。</p>
<p>可加 <code>--exclude=xxx</code> 忽略指定文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ansible zxy -m synchronize -a <span class="string">"src=/home/cluster/testfile dest=/home/cluster/testfile"</span></span><br></pre></td></tr></table></figure>
<h2 id="ping"><a class="header-anchor" href="#ping">¶</a>ping</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ansible zxy -m ping</span><br></pre></td></tr></table></figure>
<h1>playbook</h1>
<p>将一系列有序任务保存成yml文件，方便多次使用和有序的执行指定的任务。</p>
<ul>
<li>执行</li>
</ul>
<p><code>ansible-playbook playbook.yml</code></p>
<ul>
<li>格式</li>
</ul>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">- hosts:</span> <span class="string">组名</span></span><br><span class="line"><span class="attr">  vars:</span></span><br><span class="line">    <span class="string">变量名:</span> <span class="string">值</span></span><br><span class="line"><span class="attr">  remote_user:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">  tasks:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">描述任务（自定义）</span></span><br><span class="line">    <span class="string">模块名:</span> <span class="string">具体操作</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">xxxx</span></span><br><span class="line">    <span class="string">模块名:</span> <span class="string">xxxxx</span></span><br><span class="line"><span class="attr">    notify:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">钩子名(</span> <span class="string">task</span> <span class="string">结束且该</span> <span class="string">task</span> <span class="string">有意义（改变了东西）时被触发，只执行一次)</span></span><br><span class="line"><span class="attr">  handlers:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">钩子名</span></span><br><span class="line">      <span class="string">模块名:</span> <span class="string">具体操作</span></span><br></pre></td></tr></table></figure>
<ul>
<li>例子</li>
</ul>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">- hosts:</span> <span class="string">webservers</span></span><br><span class="line"><span class="attr">  vars:</span></span><br><span class="line"><span class="attr">    http_port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    max_clients:</span> <span class="number">200</span></span><br><span class="line"><span class="attr">  remote_user:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">  tasks:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ensure</span> <span class="string">apache</span> <span class="string">is</span> <span class="string">at</span> <span class="string">the</span> <span class="string">latest</span> <span class="string">version</span></span><br><span class="line"><span class="attr">    yum:</span> <span class="string">pkg=httpd</span> <span class="string">state=latest</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">write</span> <span class="string">the</span> <span class="string">apache</span> <span class="string">config</span> <span class="string">file</span></span><br><span class="line"><span class="attr">    template:</span> <span class="string">src=/srv/httpd.j2</span> <span class="string">dest=/etc/httpd.conf</span></span><br><span class="line"><span class="attr">    notify:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">restart</span> <span class="string">apache</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ensure</span> <span class="string">apache</span> <span class="string">is</span> <span class="string">running</span></span><br><span class="line"><span class="attr">    service:</span> <span class="string">name=httpd</span> <span class="string">state=started</span></span><br><span class="line"><span class="attr">  handlers:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">restart</span> <span class="string">apache</span></span><br><span class="line"><span class="attr">      service:</span> <span class="string">name=httpd</span> <span class="string">state=restarted</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据bug记录</title>
    <url>/2019/12/20/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AEbug%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>记录遇到的简单bug</p>
<a id="more"></a>
<h1>201911-201912</h1>
<h2 id="集群时间不同步"><a class="header-anchor" href="#集群时间不同步">¶</a>集群时间不同步</h2>
<p><strong>错误详情</strong></p>
<p>使用 spark 时，yarn 启动节点时报错</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Application application_1576029485466_0001 failed 2 times due to Error launching </span><br><span class="line">appattempt_1576029485466_0001_000002. Got exception: org.apache.hadoop.yarn.exceptions.YarnException: </span><br><span class="line">Unauthorized request to start container.</span><br><span class="line">This token is expired. current time is 1576059076270 found 1576030868681</span><br></pre></td></tr></table></figure>
<p><strong>解决办法</strong></p>
<p>让时间同步</p>
<h2 id="unable-to-create-new-native-thread"><a class="header-anchor" href="#unable-to-create-new-native-thread">¶</a>unable to create new native thread</h2>
<p><strong>错误详情</strong></p>
<p>使用 spark 时，程序报错</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-12-11 11:34:58,366 INFO scheduler.DAGScheduler: ResultStage 0 (collect at SparkSharder.java:430) failed in 80.679 s due to Job aborted due to stage failure: Task 5428 in stage 0.0 failed 4 times, most recent failure: Lost task 5428.3 in stage 0.0 (TID 5440, dell-r730-4, executor 3): java.io.IOException: DestHost:destPort dell-r720:8020 , LocalHost:localPort dell-r730-4/10.0.0.14:0. Failed on local exception: java.io.IOException: Couldn&apos;t set up IO streams: java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)</span><br><span class="line">	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:808)</span><br><span class="line">	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)</span><br><span class="line">	at org.apache.hadoop.ipc.Client.call(Client.java:1491)</span><br><span class="line">	at org.apache.hadoop.ipc.Client.call(Client.java:1388)</span><br><span class="line">	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)</span><br><span class="line">	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)</span><br><span class="line">	at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)</span><br><span class="line">	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:907)</span><br><span class="line">	at sun.reflect.GeneratedMethodAccessor307.invoke(Unknown Source)</span><br></pre></td></tr></table></figure>
<p><strong>解决办法</strong></p>
<p>原因是超过了<code>unlimt -u</code>设定的最大线程数，把它增大即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改 limits.conf</span></span><br><span class="line">$ vim /etc/security/limits.conf</span><br><span class="line">...</span><br><span class="line">*       soft    nofile  65536      <span class="comment"># 文件打开数（以前改的）</span></span><br><span class="line">*       hard    nofile  65536</span><br><span class="line"></span><br><span class="line">*       hard    nproc     65536    <span class="comment"># 线程数（加上这两行）</span></span><br><span class="line">*       soft    nproc     65536</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果没有效果，则继续修改 20-nproc.conf（它限制了线程最大值）</span></span><br><span class="line">$ vim /etc/security/limits.d/20-nproc.conf</span><br><span class="line"><span class="comment"># *       soft    nproc     65536  # 修改它</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>bug</tag>
      </tags>
  </entry>
  <entry>
    <title>spark调试-常用代码</title>
    <url>/2019/12/20/spark/spark%E8%B0%83%E8%AF%95-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<p>一些我用到的调试代码</p>
<a id="more"></a>
<h1>统计每个分区的record个数</h1>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; numOfRecordPerPartition = javaRDD.mapPartitionsWithIndex(</span><br><span class="line">        ((Function2&lt;Integer, Iterator&lt;T&gt;, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt;) (Index, iterator) -&gt; &#123;</span><br><span class="line">            List&lt;Tuple2&lt;Integer, Integer&gt;&gt; numOfRecord = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">int</span> totalElement = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">                iterator.next();</span><br><span class="line">                totalElement++;</span><br><span class="line">            &#125;</span><br><span class="line">            numOfRecord.add(<span class="keyword">new</span> Tuple2&lt;&gt;(Index, totalElement));</span><br><span class="line">            <span class="keyword">return</span> numOfRecord.iterator();</span><br><span class="line">        &#125;), <span class="keyword">true</span>).collect();</span><br></pre></td></tr></table></figure>
<h1>收集每个分区的第一个record</h1>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Tuple2&lt;Integer, T&gt;&gt; firstRecordPerPartition = javaRDD.mapPartitionsWithIndex(</span><br><span class="line">        ((Function2&lt;Integer, Iterator&lt;T&gt;, Iterator&lt;Tuple2&lt;Integer, T&gt;&gt;&gt;) (Index, iterator) -&gt; &#123;</span><br><span class="line">            List&lt;Tuple2&lt;Integer, T&gt;&gt; record = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">if</span> (iterator.hasNext())</span><br><span class="line">                record.add(<span class="keyword">new</span> Tuple2&lt;&gt;(Index, iterator.next()));</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                record.add(<span class="keyword">null</span>);</span><br><span class="line">            <span class="keyword">return</span> record.iterator();</span><br><span class="line">        &#125;), <span class="keyword">true</span>).collect();</span><br></pre></td></tr></table></figure>
<h1>查看当前RDD的PreferredLocations</h1>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Partition[] partitions = javaRDD.rdd().getPartitions();</span><br><span class="line">List&lt;Seq&lt;String&gt;&gt; preferredLocationsList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (Partition partition : partitions) &#123;</span><br><span class="line">    preferredLocationsList.add(javaRDD.rdd().getPreferredLocations(partition));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-常用命令</title>
    <url>/2019/12/20/linux/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>一些用过的linux命令</p>
<a id="more"></a>
<h1>文件</h1>
<ol>
<li><strong>查看目录内子目录所使用的空间</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ du -h  --max-depth=1</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><strong>改变文件（夹）拥有者</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ chown (-R) zxy filename</span><br></pre></td></tr></table></figure>
<ol start="3">
<li><strong>改变文件权限</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ chmod 755 filename</span><br></pre></td></tr></table></figure>
<ol start="4">
<li><strong>压缩与解压</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar 压缩，-p 保留权限，-z 使用 gzip</span></span><br><span class="line">$ tar cvpfz /mnt/data/homeback.tgz /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># tar 解压</span></span><br><span class="line">$ tar xvpfz /mnt/data/homeback.tgz -C /</span><br></pre></td></tr></table></figure>
<ol start="5">
<li><strong>查看磁盘使用情况</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ df -h</span><br></pre></td></tr></table></figure>
<ol start="6">
<li><strong>创建软连接</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ln -s ./hadoop-2.7.7 ./hadoop</span><br></pre></td></tr></table></figure>
<h1>系统</h1>
<ol>
<li>端口使用</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示 tcp，udp 的端口和进程等相关情况</span></span><br><span class="line">$ netstat -tunlp</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>批量删除进程</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">kill</span> -9 `ps -ef |grep xxx|awk <span class="string">'&#123;print $2&#125;'</span>`</span><br></pre></td></tr></table></figure>
<h1>vim</h1>
<table>
<thead>
<tr>
<th>命令</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>gg</td>
<td>跳转到文件头</td>
</tr>
<tr>
<td>Shift+g</td>
<td>跳转到文件末尾</td>
</tr>
<tr>
<td>/字符串</td>
<td>从开头查找，n下一个，N上一个</td>
</tr>
<tr>
<td>?字符串</td>
<td>从底部查找</td>
</tr>
</tbody>
</table>
<h1>服务器</h1>
<ol>
<li><strong>添加用户</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ adduser username</span><br><span class="line">$ passwd username</span><br><span class="line"><span class="comment"># 赋予root权限</span></span><br><span class="line">$ vim /etc/sudoers</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><strong>免密登陆</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub zxy@10.0.0.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line">$ ssh zxy@10.0.0.1 <span class="string">"cat &gt;&gt; ~/.ssh/authorized_keys"</span> &lt; ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure>
<ol start="3">
<li><strong>传输文件</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 源文件 -&gt; 目的地</span></span><br><span class="line">scp -r zxy@10.0.0.1:/home/zxy/filename  /Users/zxy/Desktop/</span><br></pre></td></tr></table></figure>
<ol start="4">
<li><strong>配置网卡ip地址</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># (1) 修改对应网卡配置文件（centos）</span></span><br><span class="line">$ sudo vim /etc/sysconfig/network-scripts/ifcfg-em1</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2) 重启生效</span></span><br><span class="line">$ sudo service network restart</span><br><span class="line"></span><br><span class="line"><span class="comment"># (3) 查看效果</span></span><br><span class="line">$ ifconfig</span><br></pre></td></tr></table></figure>
<ol start="5">
<li><strong>使用 nfs 服务器</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解释：</span></span><br><span class="line"><span class="comment"># /mnt/storage2/data                     用于共享的目录</span></span><br><span class="line"><span class="comment"># *                                      客户端：所有主机</span></span><br><span class="line"><span class="comment"># rw                                     可读可写</span></span><br><span class="line"><span class="comment"># sync                                   数据同步，效率低，但可以保证数据的一致性</span></span><br><span class="line"><span class="comment"># no_root_squash                         让root保持权限，也就是让客户端的root相当于服务端的root</span></span><br><span class="line"><span class="comment"># all_squash,anonuid=1001,anongid=1001   客户端写数据时，普通用户名强转成指定名字（1001）</span></span><br><span class="line"><span class="comment"># no_all_squash 默认                      客户端写数据时，保持用户名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (1) 修改 exports </span></span><br><span class="line">$ sudo vim /etc/exports </span><br><span class="line"></span><br><span class="line"><span class="comment"># /mnt/storage2/data  该目录统一用户 1001（cluster集群使用）</span></span><br><span class="line">/mnt/storage2/data *(rw,sync,no_root_squash,all_squash,anonuid=1001,anongid=1001)</span><br><span class="line"><span class="comment"># /mnt/storage2/users 该目录用于共享，保持个人用户名</span></span><br><span class="line">/mnt/storage2/users *(rw,sync,no_root_squash)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2) 服务端 挂载（更新）</span></span><br><span class="line">$ exportfs -arv</span><br><span class="line"></span><br><span class="line"><span class="comment"># (3) 客户端 挂载</span></span><br><span class="line">$ mount -t nfs storage-2:/mnt/storage2/data /home/cluster/Storage2</span><br><span class="line">$ mount -t nfs storage-2:/mnt/storage2/users /mnt/users</span><br><span class="line"></span><br><span class="line"><span class="comment"># (4) 客户端 卸载</span></span><br><span class="line">$ umount /mnt/users</span><br><span class="line"></span><br><span class="line"><span class="comment"># (5) 服务端 卸载</span></span><br><span class="line">$ exportfs -auv</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>spark源码-shuffle之SortShuffleWriter</title>
    <url>/2019/12/04/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BSortShuffleWriter/</url>
    <content><![CDATA[<p>spark 三大ShuffleWriter 之 SortShuffleWriter</p>
<a id="more"></a>
<h1>特点</h1>
<ul>
<li>
<p>最大特点就是<strong>支持 map-side aggregation</strong></p>
</li>
<li>
<p>最基础的 <code>ShuffleWriter</code>。当另外两种用不了时才选它～</p>
</li>
</ul>
<h1>大致流程</h1>
<p>首先记住有3种情况：<strong>含 aggregator 和 ordering</strong>；<strong>含 aggregator 但不含 ordering</strong>； <strong>前两种都不含</strong>。</p>
<p>为啥没有只含ordering的情况呢？因为不含aggregator就不做排序，永远记住<strong>ShuffleWriter阶段的排序只是为了使聚合更舒服</strong>！</p>
<ol>
<li>
<p><strong>选择 <code>sorter</code></strong>：情况1和2 选同一种<code>sorter</code>，情况3选另一种。我把它们称为 分支1 和 分支2</p>
</li>
<li>
<p><strong>读取数据</strong>：分支1把数据读进<code>PartitionedAppendOnlyMap</code>（把同一分区key相同的聚合），分支2把数据读进<code>PartitionedPairBuffer</code>（简单放入）</p>
</li>
<li>
<p><strong>数据数量达到阈值发生spill</strong>：这个spill文件整体是按<strong>分区顺序</strong>堆叠的。不同点是分区内部数据情况：情况1按 ordering 排序；情况2按 key 的 hash值 排序（这个排序只是为了方便聚合）；情况3 不排序</p>
</li>
<li>
<p><strong>合并spill文件和内存中未spill的文件，并返回分区长度数组</strong>：情况1先归并排序再聚合；情况2只聚合；情况3啥都不干</p>
</li>
<li>
<p><strong>根据分区长度数组生成索引文件</strong></p>
</li>
<li>
<p><strong>封装信息到<code>MapStatus</code>返回</strong></p>
</li>
</ol>
<p>总的来说就是一个 <strong>task</strong> 生成一个<strong>由 spill 文件合并形成的且聚合了的大文件</strong>和一个<strong>索引文件</strong>。</p>
<p>由于情况2更复杂点，以情况2为示例：<br>
<img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/shuffle/SortShuffleWriter.jpg" alt></p>
<h1>源码</h1>
<h2 id="write-…"><a class="header-anchor" href="#write-…">¶</a>write(…)</h2>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 流程1：根据是否需要 mapSideCombine 选择不同的 sorter</span></span><br><span class="line">  sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// In this case we pass neither an aggregator nor an ordering to the sorter, because we don't</span></span><br><span class="line">    <span class="comment">// care whether the keys get sorted in each partition; that will be done on the reduce side</span></span><br><span class="line">    <span class="comment">// if the operation being run is sortByKey.</span></span><br><span class="line">    <span class="comment">// 注意 ordering = None，官方解释的很清楚了，对吧</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](</span><br><span class="line">      context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 流程2-3：读取数据 与 spill</span></span><br><span class="line">  sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">  <span class="comment">// 文件名 "shuffle_" + shuffleId + "_" + mapId + "_" + reduceId + ".data"</span></span><br><span class="line">  <span class="keyword">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line">  <span class="keyword">val</span> tmp = <span class="type">Utils</span>.tempFileWith(output)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 流程4: 合并 spill文件 和 内存中未spill的文件，并返回分区长度数组</span></span><br><span class="line">    <span class="keyword">val</span> blockId = <span class="type">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class="type">IndexShuffleBlockResolver</span>.<span class="type">NOOP_REDUCE_ID</span>)</span><br><span class="line">    <span class="keyword">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class="line">    <span class="comment">// 流程5: 根据分区长度数组生成索引文件。这个步骤都是一样的，参考 BypassMergeSortShuffleWriter 篇</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class="line">    <span class="comment">// 流程6: 封装信息到 MapStatus 返回</span></span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logError(<span class="string">s"Error while deleting temp file <span class="subst">$&#123;tmp.getAbsolutePath&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="insertAll-…"><a class="header-anchor" href="#insertAll-…">¶</a>insertAll(…)</h2>
<p>流程2-3：读取数据 与 spill</p>
<p>主要关注 <code>PartitionedAppendOnlyMap </code> 和 <code>PartitionedPairBuffer </code></p>
<ul>
<li>
<p>相同点：都实现<code>WritablePartitionedPairCollection</code> trait。它们内部都是用 <code>Array</code> （key0, value0, key1, value1, key2, value2…）实现 Map 逻辑。<strong>key 是 （分区ID，原key）</strong></p>
</li>
<li>
<p>不同点：<code>PartitionedAppendOnlyMap </code><strong>支持添加于更新</strong> value：它使用<code>map.changeValue((getPartition(kv._1), kv._1), update)</code> 完成数据添加或者更新（聚合）。而<code>PartitionedPairBuffer </code><strong>仅支持添加</strong>。</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertAll</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> stop combining if we find that the reduction factor isn't high</span></span><br><span class="line">  <span class="keyword">val</span> shouldCombine = aggregator.isDefined</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 选择是否 combine</span></span><br><span class="line">  <span class="keyword">if</span> (shouldCombine) &#123;</span><br><span class="line">    <span class="comment">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class="line">    <span class="keyword">val</span> mergeValue = aggregator.get.mergeValue</span><br><span class="line">    <span class="keyword">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class="line">    <span class="keyword">var</span> kv: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="comment">// 从 aggregator 中取出 createCombiner 和 mergeValue，制作成update函数</span></span><br><span class="line">    <span class="keyword">val</span> update = (hadValue: <span class="type">Boolean</span>, oldValue: <span class="type">C</span>) =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class="keyword">else</span> createCombiner(kv._2)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;</span><br><span class="line">      <span class="comment">// 计数 + 1</span></span><br><span class="line">      addElementsRead()</span><br><span class="line">      kv = records.next()</span><br><span class="line">      <span class="comment">// combine 模式使用 PartitionedAppendOnlyMap</span></span><br><span class="line">      map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class="line">      <span class="comment">// 流程3: spill</span></span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Stick values into our buffer</span></span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;</span><br><span class="line">      addElementsRead()</span><br><span class="line">      <span class="keyword">val</span> kv = records.next()</span><br><span class="line">      <span class="comment">// 非 combine 模式使用 PartitionedPairBuffer</span></span><br><span class="line">      buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class="type">C</span>])</span><br><span class="line">      <span class="comment">// 流程3: spill</span></span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="maybeSpill-…"><a class="header-anchor" href="#maybeSpill-…">¶</a>maybeSpill(…)</h2>
<p>spill 的条件：内存申请没成功 或者 达到设定的阈值<code>numElementsForceSpillThreshold</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpill</span></span>(collection: <span class="type">C</span>, currentMemory: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> shouldSpill = <span class="literal">false</span></span><br><span class="line">  <span class="comment">// 元素个数是32的整数倍 且 大于 myMemoryThreshold</span></span><br><span class="line">  <span class="keyword">if</span> (elementsRead % <span class="number">32</span> == <span class="number">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class="line">    <span class="comment">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class="line">    <span class="keyword">val</span> amountToRequest = <span class="number">2</span> * currentMemory - myMemoryThreshold</span><br><span class="line">    <span class="comment">// 申请内存</span></span><br><span class="line">    <span class="keyword">val</span> granted = acquireMemory(amountToRequest)</span><br><span class="line">    myMemoryThreshold += granted</span><br><span class="line">    <span class="comment">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class="line">    <span class="comment">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class="line">    shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// spill条件：上面的申请没成功 或者  达到阈值</span></span><br><span class="line">  shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class="line">  <span class="comment">// Actually spill</span></span><br><span class="line">  <span class="keyword">if</span> (shouldSpill) &#123;</span><br><span class="line">    _spillCount += <span class="number">1</span></span><br><span class="line">    logSpillage(currentMemory)</span><br><span class="line">    <span class="comment">// spill 在这里发生</span></span><br><span class="line">    spill(collection)</span><br><span class="line">    _elementsRead = <span class="number">0</span></span><br><span class="line">    _memoryBytesSpilled += currentMemory</span><br><span class="line">    releaseMemory()</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="spill-…"><a class="header-anchor" href="#spill-…">¶</a>spill(…)</h2>
<p>先排序，后spill。排序方面，由于分支不同，有<strong>两个排序逻辑</strong>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">protected</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">spill</span></span>(collection: <span class="type">WritablePartitionedPairCollection</span>[<span class="type">K</span>, <span class="type">C</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 排序</span></span><br><span class="line">  <span class="keyword">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class="line">  <span class="comment">// spill</span></span><br><span class="line">  <span class="keyword">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</span><br><span class="line">  spills += spillFile</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>PartitionedAppendOnlyMap </code>的排序逻辑：<strong>2重排序</strong>，先按分区ID排，再对分区内的数据排序（优先按 ordering 排序，否则hash）</p>
<p>注意这个hash排序，学过java中 == 和 equals 的区别的兄弟应该知道，hashcode 相等 是 两个对象 equals 的<strong>必要条件</strong>。这里只能保证 hashcode 相同的数据在一起，后续聚合时，<strong>还需经过比较后才能聚合</strong>（先打个预防针，后面源码会读到它）。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionKeyComparator</span></span>[<span class="type">K</span>](keyComparator: <span class="type">Comparator</span>[<span class="type">K</span>]): <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(a: (<span class="type">Int</span>, <span class="type">K</span>), b: (<span class="type">Int</span>, <span class="type">K</span>)): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> partitionDiff = a._1 - b._1</span><br><span class="line">      <span class="keyword">if</span> (partitionDiff != <span class="number">0</span>) &#123;</span><br><span class="line">        partitionDiff</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        keyComparator.compare(a._2, b._2)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// keyComparator：有 ordering 用 ordering，否则按 hash 排序。</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">val</span> keyComparator: <span class="type">Comparator</span>[<span class="type">K</span>] = ordering.getOrElse(<span class="keyword">new</span> <span class="type">Comparator</span>[<span class="type">K</span>] &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(a: <span class="type">K</span>, b: <span class="type">K</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> h1 = <span class="keyword">if</span> (a == <span class="literal">null</span>) <span class="number">0</span> <span class="keyword">else</span> a.hashCode()</span><br><span class="line">    <span class="keyword">val</span> h2 = <span class="keyword">if</span> (b == <span class="literal">null</span>) <span class="number">0</span> <span class="keyword">else</span> b.hashCode()</span><br><span class="line">    <span class="keyword">if</span> (h1 &lt; h2) <span class="number">-1</span> <span class="keyword">else</span> <span class="keyword">if</span> (h1 == h2) <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><code>PartitionedPairBuffer</code>的排序逻辑：仅<strong>比较分区ID</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">partitionedDestructiveSortedIterator</span></span>(keyComparator: <span class="type">Option</span>[<span class="type">Comparator</span>[<span class="type">K</span>]])</span><br><span class="line">  : <span class="type">Iterator</span>[((<span class="type">Int</span>, <span class="type">K</span>), <span class="type">V</span>)] = &#123;</span><br><span class="line">  <span class="keyword">val</span> comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Sorter</span>(<span class="keyword">new</span> <span class="type">KVArraySortDataFormat</span>[(<span class="type">Int</span>, <span class="type">K</span>), <span class="type">AnyRef</span>]).sort(data, <span class="number">0</span>, curSize, comparator)</span><br><span class="line">  iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A comparator for (Int, K) pairs that orders them by only their partition ID.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionComparator</span></span>[<span class="type">K</span>]: <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] = <span class="keyword">new</span> <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(a: (<span class="type">Int</span>, <span class="type">K</span>), b: (<span class="type">Int</span>, <span class="type">K</span>)): <span class="type">Int</span> = &#123;</span><br><span class="line">    a._1 - b._1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="writePartitionedFile-…"><a class="header-anchor" href="#writePartitionedFile-…">¶</a>writePartitionedFile(…)</h2>
<p>流程4: 合并spill文件和内存中未spill的文件，并返回分区长度数组</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writePartitionedFile</span></span>(</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    outputFile: <span class="type">File</span>): <span class="type">Array</span>[<span class="type">Long</span>] = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Track location of each range in the output file</span></span><br><span class="line">  <span class="keyword">val</span> lengths = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Long</span>](numPartitions)</span><br><span class="line">  <span class="keyword">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class="line">    context.taskMetrics().shuffleWriteMetrics)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 首先知道：collection.destructiveSortedWritablePartitionedIterator(comparator) 用这玩意获取内存中的数据（未被spill），后面多次用到它</span></span><br><span class="line">  <span class="keyword">if</span> (spills.isEmpty) &#123;</span><br><span class="line">    <span class="comment">// Case where we only have in-memory data</span></span><br><span class="line">    <span class="comment">// 只有内存文件，刷进内存即可（当然排序什么的还是要的，和上一步一样的规则）</span></span><br><span class="line">    <span class="keyword">val</span> collection = <span class="keyword">if</span> (aggregator.isDefined) map <span class="keyword">else</span> buffer</span><br><span class="line">    <span class="keyword">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class="line">    <span class="keyword">while</span> (it.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> partitionId = it.nextPartition()</span><br><span class="line">      <span class="keyword">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class="line">        it.writeNext(writer)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">      <span class="comment">// 记录分区长度</span></span><br><span class="line">      lengths(partitionId) = segment.length</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 合并操作在这里：this.partitionedIterator，内部调用 merge()</span></span><br><span class="line">    <span class="keyword">for</span> ((id, elements) &lt;- <span class="keyword">this</span>.partitionedIterator) &#123;</span><br><span class="line">      <span class="keyword">if</span> (elements.hasNext) &#123;</span><br><span class="line">        <span class="keyword">for</span> (elem &lt;- elements) &#123;</span><br><span class="line">          writer.write(elem._1, elem._2)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">        lengths(id) = segment.length</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  writer.close()</span><br><span class="line">  context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class="line">  context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class="line">  context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class="line"></span><br><span class="line">  lengths</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="merge-…"><a class="header-anchor" href="#merge-…">¶</a>merge(…)</h2>
<p>把 spill文件 和 内存文件 同分区的数据放到一起计算（3种计算情况）</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(spills: <span class="type">Seq</span>[<span class="type">SpilledFile</span>], inMemory: <span class="type">Iterator</span>[((<span class="type">Int</span>, <span class="type">K</span>), <span class="type">C</span>)])</span><br><span class="line">    : <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]])] = &#123;</span><br><span class="line">  <span class="keyword">val</span> readers = spills.map(<span class="keyword">new</span> <span class="type">SpillReader</span>(_))</span><br><span class="line">  <span class="keyword">val</span> inMemBuffered = inMemory.buffered</span><br><span class="line">  (<span class="number">0</span> until numPartitions).iterator.map &#123; p =&gt;</span><br><span class="line">    <span class="comment">// 很明显这兄弟函数式编程写的很6</span></span><br><span class="line">    <span class="comment">// 主要就是把 spill文件 和 内存文件 同分区的数据放到一起计算（3种计算情况）。其实和以前的2重循环是一个意思</span></span><br><span class="line">    <span class="keyword">val</span> inMemIterator = <span class="keyword">new</span> <span class="type">IteratorForPartition</span>(p, inMemBuffered)</span><br><span class="line">    <span class="keyword">val</span> iterators = readers.map(_.readNextPartition()) ++ <span class="type">Seq</span>(inMemIterator)</span><br><span class="line">    <span class="keyword">if</span> (aggregator.isDefined) &#123;</span><br><span class="line">      <span class="comment">// Perform partial aggregation across partitions</span></span><br><span class="line">      <span class="comment">// 聚合：（分有无 ordering 两种情况）</span></span><br><span class="line">      (p, mergeWithAggregation(</span><br><span class="line">        iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ordering.isDefined) &#123;</span><br><span class="line">      <span class="comment">// No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);</span></span><br><span class="line">      <span class="comment">// sort the elements without trying to merge them</span></span><br><span class="line">      <span class="comment">// 只排序：对它们进行归并排序。</span></span><br><span class="line">      <span class="comment">// 说实话，我不觉得它会进这一分支。因为我多次强调过没有 aggregator 就必定没有 ordering</span></span><br><span class="line">      (p, mergeSort(iterators, ordering.get))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 啥都不要，直接把同分区文件 flatten</span></span><br><span class="line">      (p, iterators.iterator.flatten)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="mergeWithAggregation-…"><a class="header-anchor" href="#mergeWithAggregation-…">¶</a>mergeWithAggregation(…)</h2>
<p>聚合：（分有无 ordering 两种情况）</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">mergeWithAggregation</span></span>(</span><br><span class="line">    iterators: <span class="type">Seq</span>[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]],</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    comparator: <span class="type">Comparator</span>[<span class="type">K</span>],</span><br><span class="line">    totalOrder: <span class="type">Boolean</span>)</span><br><span class="line">    : <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] =</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span> (!totalOrder) &#123;</span><br><span class="line">    <span class="comment">// We only have a partial ordering, e.g. comparing the keys by hash code, which means that</span></span><br><span class="line">    <span class="comment">// multiple distinct keys might be treated as equal by the ordering. To deal with this, we</span></span><br><span class="line">    <span class="comment">// need to read all keys considered equal by the ordering at once and compare them.</span></span><br><span class="line">    <span class="comment">// 无 ordering ，comparator 是 hash比较器。hash值相同的是key相同的必要条件</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]] &#123;</span><br><span class="line">      <span class="keyword">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Buffers reused across elements to decrease memory allocation</span></span><br><span class="line">      <span class="keyword">val</span> keys = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">K</span>]</span><br><span class="line">      <span class="keyword">val</span> combiners = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">C</span>]</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = sorted.hasNext</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = &#123;</span><br><span class="line">        <span class="keyword">if</span> (!hasNext) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoSuchElementException</span></span><br><span class="line">        &#125;</span><br><span class="line">        keys.clear()</span><br><span class="line">        combiners.clear()</span><br><span class="line">        <span class="keyword">val</span> firstPair = sorted.next()</span><br><span class="line">        keys += firstPair._1</span><br><span class="line">        combiners += firstPair._2</span><br><span class="line">        <span class="keyword">val</span> key = firstPair._1</span><br><span class="line">        <span class="comment">// hash值相等</span></span><br><span class="line">        <span class="keyword">while</span> (sorted.hasNext &amp;&amp; comparator.compare(sorted.head._1, key) == <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">val</span> pair = sorted.next()</span><br><span class="line">          <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">          <span class="keyword">var</span> foundKey = <span class="literal">false</span></span><br><span class="line">          <span class="keyword">while</span> (i &lt; keys.size &amp;&amp; !foundKey) &#123;</span><br><span class="line">            <span class="comment">// 注意 == 。 scala 就是用 == 比较对象相等的 </span></span><br><span class="line">            <span class="keyword">if</span> (keys(i) == pair._1) &#123;</span><br><span class="line">              <span class="comment">// key 相等 就合并</span></span><br><span class="line">              combiners(i) = mergeCombiners(combiners(i), pair._2)</span><br><span class="line">              foundKey = <span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (!foundKey) &#123;</span><br><span class="line">            keys += pair._1</span><br><span class="line">            combiners += pair._2</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Note that we return an iterator of elements since we could've had many keys marked</span></span><br><span class="line">        <span class="comment">// equal by the partial order; we flatten this below to get a flat iterator of (K, C).</span></span><br><span class="line">        keys.iterator.zip(combiners.iterator)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.flatMap(i =&gt; i)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// We have a total ordering, so the objects with the same key are sequential.</span></span><br><span class="line">    <span class="comment">// 有 ordering：先归并排序，再把有相同的key的元素聚合就行了</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] &#123;</span><br><span class="line">      <span class="comment">// 归并排序</span></span><br><span class="line">      <span class="keyword">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = sorted.hasNext</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">        <span class="keyword">if</span> (!hasNext) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoSuchElementException</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> elem = sorted.next()</span><br><span class="line">        <span class="keyword">val</span> k = elem._1</span><br><span class="line">        <span class="keyword">var</span> c = elem._2</span><br><span class="line">        <span class="comment">// key 相等 就合并</span></span><br><span class="line">        <span class="keyword">while</span> (sorted.hasNext &amp;&amp; sorted.head._1 == k) &#123;</span><br><span class="line">          <span class="keyword">val</span> pair = sorted.next()</span><br><span class="line">          c = mergeCombiners(c, pair._2)</span><br><span class="line">        &#125;</span><br><span class="line">        (k, c)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark源码-shuffle之UnsafeShuffleWriter</title>
    <url>/2019/12/02/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BUnsafeShuffleWriter/</url>
    <content><![CDATA[<p>spark 三大ShuffleWriter 之 UnsafeShuffleWriter</p>
<a id="more"></a>
<h1>特点</h1>
<ul>
<li>
<p>它<strong>只适用不需要 map-side aggregation</strong> 的Shuffle操作</p>
</li>
<li>
<p>它使用UnSafe API操作序列化数据，而不是Java对象，减少了内存占用及因此导致的GC耗时(参考Spark 内存管理之Tungsten)，因此使用它时需要<strong>Serializer支持relocation</strong>。</p>
</li>
<li>
<p><strong>reduce端的分区数目小于等于 2^24</strong> (因为排序过程中需要使用是数据指针，它记录了数据的地址和分区ID，其中分区ID占24位)</p>
</li>
<li>
<p>溢写 &amp; 合并时使用UnSafe API直接操作序列化数据，<strong>合并时不需要反序列化数据</strong>。</p>
</li>
<li>
<p>溢写 &amp; 合并时<strong>可以使用fastMerge提升效率</strong>(调用NIO的transferTo方法)</p>
</li>
</ul>
<h1>大致流程</h1>
<ol>
<li>
<p>遍历数据，插入到<code>ShuffleExternalSorter</code>中。该过程完成许多事：<strong>将数据读入内存</strong> 同时 <strong>将数据指针不断写到指针数组</strong>中，当达到spill阈值，使用<code>writeSortedFile()</code>排序（排序排的是指针数组）并spill到磁盘。</p>
</li>
<li>
<p><code>sorter.closeAndGetSpills()</code>：spill 内存中剩余的文件，返回所有 spill 文件的元信息（重点是每个分区的长度）</p>
</li>
<li>
<p>合并所有 spill 文件成一个大文件（有3种合并工具选择），并返回分区长度数组</p>
</li>
<li>
<p>根据分区长度数组生成索引文件</p>
</li>
<li>
<p>封装信息到<code>MapStatus</code>返回</p>
</li>
</ol>
<p>总的来说就是一个 <strong>task</strong> 生成一个<strong>由 spill 文件合并形成的大文件（这玩意只是按分区排好，内部是无序的）<strong>和一个</strong>索引文件</strong>（上面流程是主要流程，重命名什么的就不写了）。<br>
<img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/shuffle/UnsafeShuffleWriter.jpg" alt></p>
<h1>源码</h1>
<h2 id="write-…"><a class="header-anchor" href="#write-…">¶</a>write(…)</h2>
<p>这个<code>write(...)</code>方法就比较清爽了，因为步骤都封在方法里了</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public void write(scala.collection.<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  boolean success = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">      <span class="comment">// 对应流程1</span></span><br><span class="line">      insertRecordIntoSorter(records.next());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 剩余流程</span></span><br><span class="line">    closeAndWriteOutput();</span><br><span class="line">    success = <span class="literal">true</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (sorter != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        sorter.cleanupResources();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (<span class="type">Exception</span> e) &#123;</span><br><span class="line">        <span class="comment">// Only throw this error if we won't be masking another</span></span><br><span class="line">        <span class="comment">// error.</span></span><br><span class="line">        <span class="keyword">if</span> (success) &#123;</span><br><span class="line">          <span class="keyword">throw</span> e;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          logger.error(<span class="string">"In addition to a failure during writing, we failed during "</span> +</span><br><span class="line">                       <span class="string">"cleanup."</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="insertRecordIntoSorter-…"><a class="header-anchor" href="#insertRecordIntoSorter-…">¶</a>insertRecordIntoSorter(…)</h2>
<p>其核心方法是<code>sorter.insertRecord()</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">void insertRecordIntoSorter(<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert(sorter != <span class="literal">null</span>);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">K</span> key = record._1();</span><br><span class="line">  <span class="keyword">final</span> int partitionId = partitioner.getPartition(key);</span><br><span class="line">  serBuffer.reset();</span><br><span class="line">  <span class="comment">// serOutputStream: 用于序列化对象的写入的流</span></span><br><span class="line">  serOutputStream.writeKey(key, <span class="type">OBJECT_CLASS_TAG</span>);</span><br><span class="line">  serOutputStream.writeValue(record._2(), <span class="type">OBJECT_CLASS_TAG</span>);</span><br><span class="line">  serOutputStream.flush();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> int serializedRecordSize = serBuffer.size();</span><br><span class="line">  assert (serializedRecordSize &gt; <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 都是它干的</span></span><br><span class="line">  sorter.insertRecord(</span><br><span class="line">    serBuffer.getBuf(), <span class="type">Platform</span>.<span class="type">BYTE_ARRAY_OFFSET</span>, serializedRecordSize, partitionId);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="sorter-insertRecord-…"><a class="header-anchor" href="#sorter-insertRecord-…">¶</a>sorter.insertRecord(…)</h2>
<p><strong>将数据读入内存</strong> 同时 <strong>将数据指针不断写到指针数组</strong>中，<strong>当达到spill阈值，发生 spill</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public void insertRecord(<span class="type">Object</span> recordBase, long recordOffset, int length, int partitionId)</span><br><span class="line">  <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// for tests</span></span><br><span class="line">  assert(inMemSorter != <span class="literal">null</span>);</span><br><span class="line">  <span class="comment">// 如果 Sorter 内的数据超过阈值，就发生 spill</span></span><br><span class="line">  <span class="keyword">if</span> (inMemSorter.numRecords() &gt;= numElementsForSpillThreshold) &#123;</span><br><span class="line">    logger.info(<span class="string">"Spilling data because number of spilledRecords crossed the threshold "</span> +</span><br><span class="line">      numElementsForSpillThreshold);</span><br><span class="line">    spill();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 检查 inMemSorter中 的数组是否满了，如果满了就扩容</span></span><br><span class="line">  growPointerArrayIfNecessary();</span><br><span class="line">  <span class="keyword">final</span> int uaoSize = <span class="type">UnsafeAlignedOffset</span>.getUaoSize();</span><br><span class="line">  <span class="comment">// Need 4 or 8 bytes to store the record length.</span></span><br><span class="line">  <span class="keyword">final</span> int required = length + uaoSize;</span><br><span class="line">  acquireNewPageIfNecessary(required);</span><br><span class="line"></span><br><span class="line">  assert(currentPage != <span class="literal">null</span>);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">Object</span> base = currentPage.getBaseObject();</span><br><span class="line">  <span class="keyword">final</span> long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);</span><br><span class="line">  <span class="type">UnsafeAlignedOffset</span>.putSize(base, pageCursor, length);</span><br><span class="line">  pageCursor += uaoSize;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 将数据写入 MemoryBlock 中</span></span><br><span class="line">  <span class="type">Platform</span>.copyMemory(recordBase, recordOffset, base, pageCursor, length);</span><br><span class="line">  pageCursor += length;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 将数据指针信息写入inMemSorter的数组中</span></span><br><span class="line">  inMemSorter.insertRecord(recordAddress, partitionId);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="spill"><a class="header-anchor" href="#spill">¶</a>spill()</h2>
<p><code>spill()</code> 的核心是<code> writeSortedFile(false)</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public long spill(long size, <span class="type">MemoryConsumer</span> trigger) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (trigger != <span class="keyword">this</span> || inMemSorter == <span class="literal">null</span> || inMemSorter.numRecords() == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>L;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  logger.info(<span class="string">"Thread &#123;&#125; spilling sort data of &#123;&#125; to disk (&#123;&#125; &#123;&#125; so far)"</span>,</span><br><span class="line">    <span class="type">Thread</span>.currentThread().getId(),</span><br><span class="line">    <span class="type">Utils</span>.bytesToString(getMemoryUsage()),</span><br><span class="line">    spills.size(),</span><br><span class="line">    spills.size() &gt; <span class="number">1</span> ? <span class="string">" times"</span> : <span class="string">" time"</span>);</span><br><span class="line"></span><br><span class="line">  writeSortedFile(<span class="literal">false</span>);</span><br><span class="line">  <span class="keyword">final</span> long spillSize = freeMemory();</span><br><span class="line">  <span class="comment">// spill 完整。重置 inMemSorter</span></span><br><span class="line">  inMemSorter.reset();</span><br><span class="line">  <span class="comment">// Reset the in-memory sorter's pointer array only after freeing up the memory pages holding the</span></span><br><span class="line">  <span class="comment">// records. Otherwise, if the task is over allocated memory, then without freeing the memory</span></span><br><span class="line">  <span class="comment">// pages, we might not be able to get memory for the pointer array.</span></span><br><span class="line">  taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);</span><br><span class="line">  <span class="keyword">return</span> spillSize;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="writeSortedFile"><a class="header-anchor" href="#writeSortedFile">¶</a>writeSortedFile()</h2>
<p>对内存中的数据进行排序（排序排的是指针数组）并 写到磁盘</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> void writeSortedFile(boolean isLastFile) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">ShuffleWriteMetrics</span> writeMetricsToUse;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (isLastFile) &#123;</span><br><span class="line">    writeMetricsToUse = writeMetrics;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    writeMetricsToUse = <span class="keyword">new</span> <span class="type">ShuffleWriteMetrics</span>();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// This call performs the actual sort.</span></span><br><span class="line">  <span class="comment">// 迭代器：包含分区有序的数据指针（排序在这里进行，有两种排序手段）</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">ShuffleInMemorySorter</span>.<span class="type">ShuffleSorterIterator</span> sortedRecords =</span><br><span class="line">    inMemSorter.getSortedIterator();</span><br><span class="line">  <span class="keyword">final</span> byte[] writeBuffer = <span class="keyword">new</span> byte[diskWriteBufferSize];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 文件名：temp_shuffle_ + UUID</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">Tuple2</span>&lt;<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>&gt; spilledFileInfo =</span><br><span class="line">    blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> file = spilledFileInfo._2();</span><br><span class="line">  <span class="keyword">final</span> <span class="type">TempShuffleBlockId</span> blockId = spilledFileInfo._1();</span><br><span class="line">  <span class="keyword">final</span> <span class="type">SpillInfo</span> spillInfo = <span class="keyword">new</span> <span class="type">SpillInfo</span>(numPartitions, file, blockId);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">SerializerInstance</span> ser = <span class="type">DummySerializerInstance</span>.<span class="type">INSTANCE</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">DiskBlockObjectWriter</span> writer =</span><br><span class="line">    blockManager.getDiskWriter(blockId, file, ser, fileBufferSizeBytes, writeMetricsToUse);</span><br><span class="line"></span><br><span class="line">  int currentPartition = <span class="number">-1</span>;</span><br><span class="line">  <span class="keyword">final</span> int uaoSize = <span class="type">UnsafeAlignedOffset</span>.getUaoSize();</span><br><span class="line">  <span class="keyword">while</span> (sortedRecords.hasNext()) &#123;</span><br><span class="line">    sortedRecords.loadNext();</span><br><span class="line">    <span class="keyword">final</span> int partition = sortedRecords.packedRecordPointer.getPartitionId();</span><br><span class="line">    assert (partition &gt;= currentPartition);</span><br><span class="line">    <span class="keyword">if</span> (partition != currentPartition) &#123;</span><br><span class="line">      <span class="comment">// Switch to the new partition</span></span><br><span class="line">      <span class="keyword">if</span> (currentPartition != <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileSegment</span> fileSegment = writer.commitAndGet();</span><br><span class="line">        <span class="comment">// 记录每个分区的长度</span></span><br><span class="line">        spillInfo.partitionLengths[currentPartition] = fileSegment.length();</span><br><span class="line">      &#125;</span><br><span class="line">      currentPartition = partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Object</span> recordPage = taskMemoryManager.getPage(recordPointer);</span><br><span class="line">    <span class="keyword">final</span> long recordOffsetInPage = taskMemoryManager.getOffsetInPage(recordPointer);</span><br><span class="line">    int dataRemaining = <span class="type">UnsafeAlignedOffset</span>.getSize(recordPage, recordOffsetInPage);</span><br><span class="line">    long recordReadPosition = recordOffsetInPage + uaoSize; <span class="comment">// skip over record length</span></span><br><span class="line">    <span class="keyword">while</span> (dataRemaining &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">final</span> int toTransfer = <span class="type">Math</span>.min(diskWriteBufferSize, dataRemaining);</span><br><span class="line">      <span class="type">Platform</span>.copyMemory(</span><br><span class="line">        recordPage, recordReadPosition, writeBuffer, <span class="type">Platform</span>.<span class="type">BYTE_ARRAY_OFFSET</span>, toTransfer);</span><br><span class="line">      writer.write(writeBuffer, <span class="number">0</span>, toTransfer);</span><br><span class="line">      recordReadPosition += toTransfer;</span><br><span class="line">      dataRemaining -= toTransfer;</span><br><span class="line">    &#125;</span><br><span class="line">    writer.recordWritten();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">FileSegment</span> committedSegment = writer.commitAndGet();</span><br><span class="line">  writer.close();</span><br><span class="line">  <span class="keyword">if</span> (currentPartition != <span class="number">-1</span>) &#123;</span><br><span class="line">    spillInfo.partitionLengths[currentPartition] = committedSegment.length();</span><br><span class="line">    spills.add(spillInfo);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!isLastFile) &#123;  <span class="comment">// i.e. this is a spill file</span></span><br><span class="line">    writeMetrics.incRecordsWritten(writeMetricsToUse.recordsWritten());</span><br><span class="line">    taskContext.taskMetrics().incDiskBytesSpilled(writeMetricsToUse.bytesWritten());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="closeAndWriteOutput"><a class="header-anchor" href="#closeAndWriteOutput">¶</a>closeAndWriteOutput();</h2>
<p>终于完成了流程1: <code>insertRecordIntoSorter</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">void closeAndWriteOutput() <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert(sorter != <span class="literal">null</span>);</span><br><span class="line">  updatePeakMemoryUsed();</span><br><span class="line">  serBuffer = <span class="literal">null</span>;</span><br><span class="line">  serOutputStream = <span class="literal">null</span>;</span><br><span class="line">  <span class="comment">// 流程2: spill 内存中剩余的文件，返回所有 spill 文件的元信息（重点是每个分区的长度）</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">SpillInfo</span>[] spills = sorter.closeAndGetSpills();</span><br><span class="line">  sorter = <span class="literal">null</span>;</span><br><span class="line">  <span class="keyword">final</span> long[] partitionLengths;</span><br><span class="line">  <span class="comment">// 文件名："shuffle_" + shuffleId + "_" + mapId + "_" + reduceId + ".data"</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> tmp = <span class="type">Utils</span>.tempFileWith(output);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 流程3: 合并 spill 文件，并返回 spill 文件的大小，用于计算索引文件</span></span><br><span class="line">      partitionLengths = mergeSpills(spills, tmp);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">SpillInfo</span> spill : spills) &#123;</span><br><span class="line">        <span class="keyword">if</span> (spill.file.exists() &amp;&amp; ! spill.file.delete()) &#123;</span><br><span class="line">          logger.error(<span class="string">"Error while deleting spill file &#123;&#125;"</span>, spill.file.getPath());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 流程4: 生成索引文件。这个步骤都是一样的，参考 BypassMergeSortShuffleWriter</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logger.error(<span class="string">"Error while deleting temp file &#123;&#125;"</span>, tmp.getAbsolutePath());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  流程<span class="number">5</span>: 封装信息到`<span class="type">MapStatus</span>`返回</span><br><span class="line">  mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="mergeSpills-…"><a class="header-anchor" href="#mergeSpills-…">¶</a>mergeSpills(…)</h3>
<p>合并 spill 文件，有3种合并手段：</p>
<ul>
<li>
<p>快合并：<strong>不使用压缩，或者特定的支持拼接的压缩格式</strong>：Snappy、LZF、LZ4、ZStd</p>
<ol>
<li>
<p>当使用nio的<code>transferTo</code>传输 且 不需要加密时，使用 <code>mergeSpillsWithTransferTo(spills, outputFile)</code></p>
</li>
<li>
<p>否则使用<code>mergeSpillsWithFileStream(spills, outputFile, null)</code></p>
</li>
</ol>
</li>
<li>
<p>慢合并：</p>
<ol>
<li>使用<code>mergeSpillsWithFileStream(spills, outputFile, compressionCodec)</code></li>
</ol>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> long[] mergeSpills(<span class="type">SpillInfo</span>[] spills, <span class="type">File</span> outputFile) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  <span class="comment">// 压缩，以及压缩格式</span></span><br><span class="line">  <span class="keyword">final</span> boolean compressionEnabled = sparkConf.getBoolean(<span class="string">"spark.shuffle.compress"</span>, <span class="literal">true</span>);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">CompressionCodec</span> compressionCodec = <span class="type">CompressionCodec</span>$.<span class="type">MODULE</span>$.createCodec(sparkConf);</span><br><span class="line">  <span class="keyword">final</span> boolean fastMergeEnabled =</span><br><span class="line">    sparkConf.getBoolean(<span class="string">"spark.shuffle.unsafe.fastMergeEnabled"</span>, <span class="literal">true</span>);</span><br><span class="line">  <span class="comment">// 支持快速合并的情况：不使用压缩，或者特定的支持拼接的压缩格式：Snappy、LZF、LZ4、ZStd</span></span><br><span class="line">  <span class="keyword">final</span> boolean fastMergeIsSupported = !compressionEnabled ||</span><br><span class="line">    <span class="type">CompressionCodec</span>$.<span class="type">MODULE</span>$.supportsConcatenationOfSerializedStreams(compressionCodec);</span><br><span class="line">  <span class="comment">// 是否加密</span></span><br><span class="line">  <span class="keyword">final</span> boolean encryptionEnabled = blockManager.serializerManager().encryptionEnabled();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (spills.length == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">FileOutputStream</span>(outputFile).close(); <span class="comment">// Create an empty file</span></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> long[partitioner.numPartitions()];</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (spills.length == <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="type">Files</span>.move(spills[<span class="number">0</span>].file, outputFile);</span><br><span class="line">      <span class="keyword">return</span> spills[<span class="number">0</span>].partitionLengths;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">final</span> long[] partitionLengths;</span><br><span class="line">      <span class="keyword">if</span> (fastMergeEnabled &amp;&amp; fastMergeIsSupported) &#123;</span><br><span class="line">        <span class="comment">// 当使用nio的transferTo传输 且 不需要加密时，使用 transferTo-based fast merge</span></span><br><span class="line">        <span class="keyword">if</span> (transferToEnabled &amp;&amp; !encryptionEnabled) &#123;</span><br><span class="line">          logger.debug(<span class="string">"Using transferTo-based fast merge"</span>);</span><br><span class="line">          partitionLengths = mergeSpillsWithTransferTo(spills, outputFile);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 否则使用 fileStream-based fast merge</span></span><br><span class="line">          logger.debug(<span class="string">"Using fileStream-based fast merge"</span>);</span><br><span class="line">          partitionLengths = mergeSpillsWithFileStream(spills, outputFile, <span class="literal">null</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logger.debug(<span class="string">"Using slow merge"</span>);</span><br><span class="line">        <span class="comment">// 使用 slow merge</span></span><br><span class="line">        partitionLengths = mergeSpillsWithFileStream(spills, outputFile, compressionCodec);</span><br><span class="line">      &#125;</span><br><span class="line">      writeMetrics.decBytesWritten(spills[spills.length - <span class="number">1</span>].file.length());</span><br><span class="line">      writeMetrics.incBytesWritten(outputFile.length());</span><br><span class="line">      <span class="keyword">return</span> partitionLengths;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (<span class="type">IOException</span> e) &#123;</span><br><span class="line">    <span class="keyword">if</span> (outputFile.exists() &amp;&amp; !outputFile.delete()) &#123;</span><br><span class="line">      logger.error(<span class="string">"Unable to delete output file &#123;&#125;"</span>, outputFile.getPath());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="mergeSpillsWithTransferTo-…"><a class="header-anchor" href="#mergeSpillsWithTransferTo-…">¶</a>mergeSpillsWithTransferTo(…)</h3>
<p>由于内部流程都差不多，就举一个为例，核心是个<strong>2重循环</strong>，将所有spill文件中同一分区的数据合并，并按分区号排列</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> long[] mergeSpillsWithTransferTo(<span class="type">SpillInfo</span>[] spills, <span class="type">File</span> outputFile) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert (spills.length &gt;= <span class="number">2</span>);</span><br><span class="line">  <span class="keyword">final</span> int numPartitions = partitioner.numPartitions();</span><br><span class="line">  <span class="keyword">final</span> long[] partitionLengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">  <span class="keyword">final</span> <span class="type">FileChannel</span>[] spillInputChannels = <span class="keyword">new</span> <span class="type">FileChannel</span>[spills.length];</span><br><span class="line">  <span class="keyword">final</span> long[] spillInputChannelPositions = <span class="keyword">new</span> long[spills.length];</span><br><span class="line">  <span class="type">FileChannel</span> mergedFileOutputChannel = <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">  boolean threwException = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 对每个 spill 文件产出输入流</span></span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; spills.length; i++) &#123;</span><br><span class="line">      spillInputChannels[i] = <span class="keyword">new</span> <span class="type">FileInputStream</span>(spills[i].file).getChannel();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 合并文件 的输出流</span></span><br><span class="line">    mergedFileOutputChannel = <span class="keyword">new</span> <span class="type">FileOutputStream</span>(outputFile, <span class="literal">true</span>).getChannel();</span><br><span class="line"></span><br><span class="line">    long bytesWrittenToMergedFile = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 2重循环，结果是将所有spill文件中同一分区的数据合并，并按分区号排列</span></span><br><span class="line">    <span class="keyword">for</span> (int partition = <span class="number">0</span>; partition &lt; numPartitions; partition++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; spills.length; i++) &#123;</span><br><span class="line">        <span class="keyword">final</span> long partitionLengthInSpill = spills[i].partitionLengths[partition];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileChannel</span> spillInputChannel = spillInputChannels[i];</span><br><span class="line">        <span class="keyword">final</span> long writeStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">        <span class="comment">// 将 spill 里的 指定分区数据 写入合并文件中</span></span><br><span class="line">        <span class="type">Utils</span>.copyFileStreamNIO(</span><br><span class="line">          spillInputChannel,             <span class="comment">// spill 文件输入流</span></span><br><span class="line">          mergedFileOutputChannel,       <span class="comment">// 合并文件输出流</span></span><br><span class="line">          spillInputChannelPositions[i], <span class="comment">// spill 中 该分区起始位置</span></span><br><span class="line">          partitionLengthInSpill);       <span class="comment">// spill 中 该分区长度</span></span><br><span class="line">        spillInputChannelPositions[i] += partitionLengthInSpill;</span><br><span class="line">        writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - writeStartTime);</span><br><span class="line">        bytesWrittenToMergedFile += partitionLengthInSpill;</span><br><span class="line">        partitionLengths[partition] += partitionLengthInSpill; <span class="comment">// 所有 spill 中该分区的总长度</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (mergedFileOutputChannel.position() != bytesWrittenToMergedFile) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(</span><br><span class="line">        <span class="string">"Current position "</span> + mergedFileOutputChannel.position() + <span class="string">" does not equal expected "</span> +</span><br><span class="line">          <span class="string">"position "</span> + bytesWrittenToMergedFile + <span class="string">" after transferTo. Please check your kernel"</span> +</span><br><span class="line">          <span class="string">" version to see if it is 2.6.32, as there is a kernel bug which will lead to "</span> +</span><br><span class="line">          <span class="string">"unexpected behavior when using transferTo. You can set spark.file.transferTo=false "</span> +</span><br><span class="line">          <span class="string">"to disable this NIO feature."</span></span><br><span class="line">      );</span><br><span class="line">    &#125;</span><br><span class="line">    threwException = <span class="literal">false</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; spills.length; i++) &#123;</span><br><span class="line">      assert(spillInputChannelPositions[i] == spills[i].file.length());</span><br><span class="line">      <span class="type">Closeables</span>.close(spillInputChannels[i], threwException);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Closeables</span>.close(mergedFileOutputChannel, threwException);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> partitionLengths;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark源码-shuffle之BypassMergeSortShuffleWriter</title>
    <url>/2019/11/30/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BBypassMergeSortShuffleWriter/</url>
    <content><![CDATA[<p>spark 三大ShuffleWriter 之 BypassMergeSortShuffleWriter</p>
<a id="more"></a>
<h1>特点</h1>
<ul>
<li>
<p>它<strong>只适用不需要 map-side aggregation</strong>的Shuffle操作，且<strong>Reducer任务数量比较少</strong>（默认200）</p>
</li>
<li>
<p>数据是直接写入文件，数据量较大的时候，网络I/O和内存负担较重</p>
</li>
<li>
<p>写分区文件时开了多个<code>DiskBlockObjectWriter</code>，对<strong>内存消耗比较大</strong></p>
</li>
</ul>
<h1>大致流程</h1>
<ol>
<li>
<p>为每个分区都创建一个<code>DiskBlockObjectWriter</code></p>
</li>
<li>
<p>遍历数据，使用<code>DiskBlockObjectWriter</code>的<code>write</code>方法将数据写入到不同分区文件中</p>
</li>
<li>
<p>刷写分区文件到磁盘</p>
</li>
<li>
<p>合并分区文件成一个大文件，并返回记录每个分区文件的长度的数组</p>
</li>
<li>
<p>根据分区长度数组生成索引文件</p>
</li>
<li>
<p>封装信息到<code>MapStatus</code>返回</p>
</li>
</ol>
<p>总的来说就是生成了一个<strong>由分区文件合并形成的大文件</strong>和一个<strong>索引文件</strong>（上面流程是主要流程，重命名什么的就不写了）。</p>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/shuffle/BypassMergeSortShuffleWriter.jpg" alt></p>
<h1>源码</h1>
<h2 id="write-…"><a class="header-anchor" href="#write-…">¶</a>write(…)</h2>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public void write(<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert (partitionWriters == <span class="literal">null</span>);</span><br><span class="line">  <span class="keyword">if</span> (!records.hasNext()) &#123;</span><br><span class="line">    partitionLengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, <span class="literal">null</span>);</span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">final</span> <span class="type">SerializerInstance</span> serInstance = serializer.newInstance();</span><br><span class="line">  <span class="keyword">final</span> long openStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">  <span class="comment">// partitionWriter 数组：分区号即是数组偏移量。它们将数据按分区号分别写入不同的个文件，有多少个分区就形成多少个文件</span></span><br><span class="line">  <span class="comment">// 这里的 numPartitions 是分区后的数量</span></span><br><span class="line">  partitionWriters = <span class="keyword">new</span> <span class="type">DiskBlockObjectWriter</span>[numPartitions];</span><br><span class="line">  partitionWriterSegments = <span class="keyword">new</span> <span class="type">FileSegment</span>[numPartitions];</span><br><span class="line">  <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Tuple2</span>&lt;<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>&gt; tempShuffleBlockIdPlusFile =</span><br><span class="line">      blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">File</span> file = tempShuffleBlockIdPlusFile._2();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">BlockId</span> blockId = tempShuffleBlockIdPlusFile._1();</span><br><span class="line">    partitionWriters[i] =</span><br><span class="line">      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Creating the file to write to and creating a disk writer both involve interacting with</span></span><br><span class="line">  <span class="comment">// the disk, and can take a long time in aggregate when we open many files, so should be</span></span><br><span class="line">  <span class="comment">// included in the shuffle write time.</span></span><br><span class="line">  writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - openStartTime);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record = records.next();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">K</span> key = record._1();</span><br><span class="line">    <span class="comment">// 按分区器的规则写入数据</span></span><br><span class="line">    partitionWriters[partitioner.getPartition(key)].write(key, record._2());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">DiskBlockObjectWriter</span> writer = partitionWriters[i];</span><br><span class="line">    <span class="comment">// 刷写数据到磁盘</span></span><br><span class="line">    partitionWriterSegments[i] = writer.commitAndGet();</span><br><span class="line">    writer.close();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 文件名："shuffle_" + shuffleId + "_" + mapId + "_" + reduceId + ".data"</span></span><br><span class="line">  <span class="type">File</span> output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">  <span class="type">File</span> tmp = <span class="type">Utils</span>.tempFileWith(output);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 合并生成的分区文件，并返回每个分区文件的大小，用于计算偏移量</span></span><br><span class="line">    partitionLengths = writePartitionedFile(tmp);</span><br><span class="line">    <span class="comment">// 生成索引文件</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="comment">// 这里可以删除是因为 writeIndexFileAndCommit 中重命名了它</span></span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logger.error(<span class="string">"Error while deleting temp file &#123;&#125;"</span>, tmp.getAbsolutePath());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="writePartitionedFile-…"><a class="header-anchor" href="#writePartitionedFile-…">¶</a>writePartitionedFile(…)</h2>
<p>合并生成的分区文件，并返回每个分区文件的大小，用于计算偏移量</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> long[] writePartitionedFile(<span class="type">File</span> outputFile) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  <span class="comment">// Track location of the partition starts in the output file</span></span><br><span class="line">  <span class="comment">// lengths数组：记录每个分区文件的大小</span></span><br><span class="line">  <span class="keyword">final</span> long[] lengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">  <span class="keyword">if</span> (partitionWriters == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="comment">// We were passed an empty iterator</span></span><br><span class="line">    <span class="keyword">return</span> lengths;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并文件</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">FileOutputStream</span> out = <span class="keyword">new</span> <span class="type">FileOutputStream</span>(outputFile, <span class="literal">true</span>);</span><br><span class="line">  <span class="keyword">final</span> long writeStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">  boolean threwException = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">File</span> file = partitionWriterSegments[i].file();</span><br><span class="line">      <span class="keyword">if</span> (file.exists()) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileInputStream</span> in = <span class="keyword">new</span> <span class="type">FileInputStream</span>(file);</span><br><span class="line">        boolean copyThrewException = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 把分区文件 拷贝到 合并文件 中，存入文件大小到lengths数组中</span></span><br><span class="line">          lengths[i] = <span class="type">Utils</span>.copyStream(in, out, <span class="literal">false</span>, transferToEnabled);</span><br><span class="line">          copyThrewException = <span class="literal">false</span>;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          <span class="type">Closeables</span>.close(in, copyThrewException);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 删除分区文件</span></span><br><span class="line">        <span class="keyword">if</span> (!file.delete()) &#123;</span><br><span class="line">          logger.error(<span class="string">"Unable to delete file for partition &#123;&#125;"</span>, i);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    threwException = <span class="literal">false</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="type">Closeables</span>.close(out, threwException);</span><br><span class="line">    writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - writeStartTime);</span><br><span class="line">  &#125;</span><br><span class="line">  partitionWriters = <span class="literal">null</span>;</span><br><span class="line">  <span class="keyword">return</span> lengths;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="writeIndexFileAndCommit-…"><a class="header-anchor" href="#writeIndexFileAndCommit-…">¶</a>writeIndexFileAndCommit(…)</h2>
<p>生成索引文件</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeIndexFileAndCommit</span></span>(</span><br><span class="line">    shuffleId: <span class="type">Int</span>,</span><br><span class="line">    mapId: <span class="type">Int</span>,</span><br><span class="line">    lengths: <span class="type">Array</span>[<span class="type">Long</span>],</span><br><span class="line">    dataTmp: <span class="type">File</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> indexFile = getIndexFile(shuffleId, mapId)</span><br><span class="line">  <span class="keyword">val</span> indexTmp = <span class="type">Utils</span>.tempFileWith(indexFile)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> dataFile = getDataFile(shuffleId, mapId)</span><br><span class="line">    <span class="comment">// There is only one IndexShuffleBlockResolver per executor, this synchronization make sure</span></span><br><span class="line">    <span class="comment">// the following check and rename are atomic.</span></span><br><span class="line">    synchronized &#123;</span><br><span class="line">      <span class="keyword">val</span> existingLengths = checkIndexAndDataFile(indexFile, dataFile, lengths.length)</span><br><span class="line">      <span class="keyword">if</span> (existingLengths != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Another attempt for the same task has already written our map outputs successfully,</span></span><br><span class="line">        <span class="comment">// so just use the existing partition lengths and delete our temporary map outputs.</span></span><br><span class="line">        <span class="type">System</span>.arraycopy(existingLengths, <span class="number">0</span>, lengths, <span class="number">0</span>, lengths.length)</span><br><span class="line">        <span class="keyword">if</span> (dataTmp != <span class="literal">null</span> &amp;&amp; dataTmp.exists()) &#123;</span><br><span class="line">          dataTmp.delete()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// This is the first successful attempt in writing the map outputs for this task,</span></span><br><span class="line">        <span class="comment">// so override any existing index and data files with the ones we wrote.</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">DataOutputStream</span>(<span class="keyword">new</span> <span class="type">BufferedOutputStream</span>(<span class="keyword">new</span> <span class="type">FileOutputStream</span>(indexTmp)))</span><br><span class="line">        <span class="type">Utils</span>.tryWithSafeFinally &#123;</span><br><span class="line">          <span class="comment">// We take in lengths of each block, need to convert it to offsets.</span></span><br><span class="line">          <span class="comment">// 索引其实就是按 分区文件大小 叠上去而已</span></span><br><span class="line">          <span class="keyword">var</span> offset = <span class="number">0</span>L</span><br><span class="line">          out.writeLong(offset)</span><br><span class="line">          <span class="keyword">for</span> (length &lt;- lengths) &#123;</span><br><span class="line">            offset += length</span><br><span class="line">            out.writeLong(offset)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; &#123;</span><br><span class="line">          out.close()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (indexFile.exists()) &#123;</span><br><span class="line">          indexFile.delete()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (dataFile.exists()) &#123;</span><br><span class="line">          dataFile.delete()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 重命名 indexTmp</span></span><br><span class="line">        <span class="keyword">if</span> (!indexTmp.renameTo(indexFile)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"fail to rename file "</span> + indexTmp + <span class="string">" to "</span> + indexFile)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 重命名 dataTmp</span></span><br><span class="line">        <span class="keyword">if</span> (dataTmp != <span class="literal">null</span> &amp;&amp; dataTmp.exists() &amp;&amp; !dataTmp.renameTo(dataFile)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"fail to rename file "</span> + dataTmp + <span class="string">" to "</span> + dataFile)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (indexTmp.exists() &amp;&amp; !indexTmp.delete()) &#123;</span><br><span class="line">      logError(<span class="string">s"Failed to delete temporary index file at <span class="subst">$&#123;indexTmp.getAbsolutePath&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark源码-shuffle之ShuffleManager</title>
    <url>/2019/11/29/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BShuffleManager/</url>
    <content><![CDATA[<p>spark shuffle 学习起始篇，涉及源码中shuffle的相关概念 加 SortShuffleManager</p>
<a id="more"></a>
<h1>Spark 里的 Shuffle</h1>
<p>shuffle 主要分为:</p>
<ul>
<li><strong>ShuffleWrite</strong> 阶段：上一个stage 的尾任务<code>ShuffleMapTask</code> 把数据写入磁盘，也叫<code>ShuffleMap</code></li>
<li><strong>ShuffleRead</strong> 阶段： 下一个stage 拉取数据，也叫<code>ShuffleReduce</code></li>
</ul>
<p>源码中的一些概念：</p>
<ul>
<li>
<p>如果把 spark 整个流程看成一辆火车，那么除了最后一节是<code>ResultStage </code>，其它每一节车厢就是一个<code>ShuffleMapStage </code>，连接车厢的部分就是<code>shuffle</code>。</p>
</li>
<li>
<p>车厢头进行 <strong>read</strong>，车厢尾进行 <strong>write</strong>。很容易理解<code>ShuffleMapStage</code>需要读前一个stage内容，也需要把输出写入下一个stage；而<code>ResultStage</code>只需要读。这些可以在源码中发现。</p>
</li>
<li>
<p><code>ShuffleMapStage</code>对应 <code>ShuffleMapTask</code>，<code>ResultStage</code> 对应 <code>ResultTask</code>。</p>
</li>
<li>
<p><strong>read</strong> 实质是<code>ShuffleReader</code>里的 <code>read()</code>方法；<strong>write</strong> 是实质是<code>ShuffleWriter</code>里的 <code>write()</code>方法。</p>
</li>
<li>
<p><code>ShuffleReader</code> 和 <code>ShuffleWriter</code> 这两大组件都由<code>ShuffleManager</code>进行选择。</p>
</li>
</ul>
<p>好了，脑子里有了这些概念，就可以对这3个模块进行仔细研究了。</p>
<h1>SortShuffleManager</h1>
<p>由于 Spark 2.0以后，<code>ShuffleManager</code>只提供一种实现：<code>SortShuffleManager</code>，因此只深入研究它。</p>
<p>以下是<code>ShuffleMapTask</code>中的写操作。可以发现 <code>shuffleManager</code> 是 <code>SparkEnv</code> 中的属性。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 从 SparkEnv 中 得到 shuffleManager</span></span><br><span class="line"><span class="keyword">val</span> manager = <span class="type">SparkEnv</span>.get.shuffleManager</span><br><span class="line"><span class="comment">// 从 shuffleManager 中得到 ShuffleWriter</span></span><br><span class="line">writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class="line"><span class="comment">// 执行 ShuffleWriter 里的 write 方法</span></span><br><span class="line">writer.write(rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])</span><br><span class="line"><span class="comment">// 成功写入，收尾工作</span></span><br><span class="line">writer.stop(success = <span class="literal">true</span>).get</span><br></pre></td></tr></table></figure>
<p>这里事先预告下，有3种<code>ShuffleWriter</code>，1种<code>ShuffleReader</code>。</p>
<p>那么如何选择呢？注意上面，它取决于<code>dep.shuffleHandle</code>，而它来自<code>shuffleManager</code>的<code>registerShuffle()</code>方法：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> shuffleHandle: <span class="type">ShuffleHandle</span> = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">  shuffleId, _rdd.partitions.length, <span class="keyword">this</span>)</span><br></pre></td></tr></table></figure>
<p>好的，让我们进入<code>SortShuffleManager</code>中一探究竟。</p>
<h2 id="registerShuffle-…"><a class="header-anchor" href="#registerShuffle-…">¶</a>registerShuffle(…)</h2>
<p>其实就是选择 <code>handle</code> 的过程</p>
<ol>
<li>
<p>如果 <strong>不需要map端的聚合操作</strong> 且 <strong>shuffle 后的分区数量小于等于200</strong>（<code>spark.shuffle.sort.bypassMergeThreshold</code>），就选择 <code>BypassMergeSortShuffleHandle</code>。否则进入第二步</p>
</li>
<li>
<p>如果 <strong>序列化器支持重定位</strong> 且 <strong>不需要map端聚合</strong> 且 <strong>shuffle 后的分区数目小于等于2^24)</strong>，就选择 <code>SerializedShuffleHandle</code>。否则进入第三步</p>
</li>
<li>
<p>选择 <code>BaseShuffleHandle</code></p>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    shuffleId: <span class="type">Int</span>,</span><br><span class="line">    numMaps: <span class="type">Int</span>,</span><br><span class="line">    dependency: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]): <span class="type">ShuffleHandle</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, numMaps, dependency)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="getWriter-…"><a class="header-anchor" href="#getWriter-…">¶</a>getWriter(…)</h2>
<p>根据上面得到的<code>handle</code>，进行模式匹配选择<code>ShuffleWriter</code>，有3种：</p>
<p><code>BypassMergeSortHandle</code>  --&gt; <code>BypassMergeSortShuffleWriter</code></p>
<p><code>SerializedShuffleHandle</code> --&gt; <code>UnsafeShuffleWriter</code></p>
<p><code>other(BaseShuffleHandle)</code> --&gt; <code>SortShuffleWriter</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getWriter</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">    handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">    mapId: <span class="type">Int</span>,</span><br><span class="line">    context: <span class="type">TaskContext</span>): <span class="type">ShuffleWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">  numMapsForShuffle.putIfAbsent(</span><br><span class="line">    handle.shuffleId, handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[_, _, _]].numMaps)</span><br><span class="line">  <span class="keyword">val</span> env = <span class="type">SparkEnv</span>.get</span><br><span class="line">  handle <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> unsafeShuffleHandle: <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">UnsafeShuffleWriter</span>(</span><br><span class="line">        env.blockManager,</span><br><span class="line">        shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">        context.taskMemoryManager(),</span><br><span class="line">        unsafeShuffleHandle,</span><br><span class="line">        mapId,</span><br><span class="line">        context,</span><br><span class="line">        env.conf)</span><br><span class="line">    <span class="keyword">case</span> bypassMergeSortHandle: <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleWriter</span>(</span><br><span class="line">        env.blockManager,</span><br><span class="line">        shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">        bypassMergeSortHandle,</span><br><span class="line">        mapId,</span><br><span class="line">        context,</span><br><span class="line">        env.conf)</span><br><span class="line">    <span class="keyword">case</span> other: <span class="type">BaseShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>, _] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">SortShuffleWriter</span>(shuffleBlockResolver, other, mapId, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="getReader-…"><a class="header-anchor" href="#getReader-…">¶</a>getReader(…)</h2>
<p>只有一种<code>ShuffleReader</code>：<code>BlockStoreShuffleReader</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getReader</span></span>[<span class="type">K</span>, <span class="type">C</span>](</span><br><span class="line">    handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">    startPartition: <span class="type">Int</span>,</span><br><span class="line">    endPartition: <span class="type">Int</span>,</span><br><span class="line">    context: <span class="type">TaskContext</span>): <span class="type">ShuffleReader</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">BlockStoreShuffleReader</span>(</span><br><span class="line">    handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[<span class="type">K</span>, _, <span class="type">C</span>]], startPartition, endPartition, context)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark源码-Partitioner</title>
    <url>/2019/11/28/spark/spark%E6%BA%90%E7%A0%81-Partitioner/</url>
    <content><![CDATA[<p>Partitioner（分区器）学习</p>
<a id="more"></a>
<h1>Partitioner</h1>
<p>分区器，RDD五大特性之五（只针对（k,v）类型的RDD）。它的核心作用是使用 <code>getPartition(key: Any)</code>对每条数据进行分区</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它有两大实现，<code>HashPartitioner</code> 和 <code>RangePartitioner</code>。分区是为了并行处理，所以让每个分区的大小差不多是首要目标。</p>
<h1>HashPartitioner</h1>
<p>这个是最简单的，直接通过 key 的 hashCode 取模分区，能让数据大致均匀地分布在各个分区。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class HashPartitioner(partitions: Int) extends Partitioner &#123;</span><br><span class="line">  require(partitions &gt;= 0, s&quot;Number of partitions ($partitions) cannot be negative.&quot;)</span><br><span class="line"></span><br><span class="line">  def numPartitions: Int = partitions</span><br><span class="line"></span><br><span class="line">  // 看这里</span><br><span class="line">  def getPartition(key: Any): Int = key match &#123;</span><br><span class="line">    case null =&gt; 0</span><br><span class="line">    case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def equals(other: Any): Boolean = other match &#123;</span><br><span class="line">    case h: HashPartitioner =&gt;</span><br><span class="line">      h.numPartitions == numPartitions</span><br><span class="line">    case _ =&gt;</span><br><span class="line">      false</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def hashCode: Int = numPartitions</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>RangePartitioner</h1>
<p>既然有了hash分区，为什么还要range分区呢。事想需要全局排序时，如果使用hash分区，排序后只是分区有序，然后再对分区进行归并排序，这样工作量是不是特别大。所以排序时一般用 <code>RangePartitioner</code> ，比如 <code>sortByKey</code>。它的效果让是一个分区中的元素肯定都是比另一个分区内的元素小或者大。这样分区排序后的数据就是全局有序的。并且它通过采样操作可以让数据比较均匀地分布到各个分区。</p>
<p>它的大致步骤是：对每个分区进行采样（蓄水池采样） -&gt; 判断每个分区的采样结果是否合格，如果不合格再次采样 -&gt; 把采样数据排序，每条采样数据都有权重，按权重，计算出分解边界数组<code>rangeBounds</code> -&gt; 按边界，把数据划分到不同分区<code>getPartition(key: Any)</code>。</p>
<h2 id="rangeBounds"><a class="header-anchor" href="#rangeBounds">¶</a>rangeBounds</h2>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> rangeBounds: <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (partitions &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="type">Array</span>.empty</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// This is the sample size we need to have roughly balanced output partitions, capped at 1M.</span></span><br><span class="line">    <span class="comment">// Cast to double to avoid overflowing ints or longs</span></span><br><span class="line">    <span class="comment">// 总采样点的个数，不超过 1e6，注意 partitions 是分区后的分区个数</span></span><br><span class="line">    <span class="keyword">val</span> sampleSize = math.min(samplePointsPerPartitionHint.toDouble * partitions, <span class="number">1e6</span>)</span><br><span class="line">    <span class="comment">// Assume the input partitions are roughly balanced and over-sample a little bit.</span></span><br><span class="line">    <span class="comment">// 每个分区的采样点个数，并乘了3进行过采样</span></span><br><span class="line">    <span class="keyword">val</span> sampleSizePerPartition = math.ceil(<span class="number">3.0</span> * sampleSize / rdd.partitions.length).toInt</span><br><span class="line">    <span class="comment">// 使用蓄水池采样法（见下）进行采样，返回总数据个数，和每个分区的采样情况(partitionId, 该分区数据总个数, sample)</span></span><br><span class="line">    <span class="keyword">val</span> (numItems, sketched) = <span class="type">RangePartitioner</span>.sketch(rdd.map(_._1), sampleSizePerPartition)</span><br><span class="line">    <span class="keyword">if</span> (numItems == <span class="number">0</span>L) &#123;</span><br><span class="line">      <span class="type">Array</span>.empty</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// If a partition contains much more than the average number of items, we re-sample from it</span></span><br><span class="line">      <span class="comment">// to ensure that enough items are collected from that partition.</span></span><br><span class="line">      <span class="keyword">val</span> fraction = math.min(sampleSize / math.max(numItems, <span class="number">1</span>L), <span class="number">1.0</span>)</span><br><span class="line">      <span class="keyword">val</span> candidates = <span class="type">ArrayBuffer</span>.empty[(<span class="type">K</span>, <span class="type">Float</span>)]</span><br><span class="line">      <span class="keyword">val</span> imbalancedPartitions = mutable.<span class="type">Set</span>.empty[<span class="type">Int</span>]</span><br><span class="line">      sketched.foreach &#123; <span class="keyword">case</span> (idx, n, sample) =&gt;</span><br><span class="line">        <span class="comment">// 如果一个分区采样过多，就重新采样它</span></span><br><span class="line">        <span class="keyword">if</span> (fraction * n &gt; sampleSizePerPartition) &#123;</span><br><span class="line">          imbalancedPartitions += idx</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// The weight is 1 over the sampling probability.</span></span><br><span class="line">          <span class="comment">// 权重 = 该分区数据总个数 / 采样点数</span></span><br><span class="line">          <span class="keyword">val</span> weight = (n.toDouble / sample.length).toFloat</span><br><span class="line">          <span class="keyword">for</span> (key &lt;- sample) &#123;</span><br><span class="line">            candidates += ((key, weight))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (imbalancedPartitions.nonEmpty) &#123;</span><br><span class="line">        <span class="comment">// Re-sample imbalanced partitions with the desired sampling probability.</span></span><br><span class="line">        <span class="comment">// 重新采样</span></span><br><span class="line">        <span class="keyword">val</span> imbalanced = <span class="keyword">new</span> <span class="type">PartitionPruningRDD</span>(rdd.map(_._1), imbalancedPartitions.contains)</span><br><span class="line">        <span class="keyword">val</span> seed = byteswap32(-rdd.id - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> reSampled = imbalanced.sample(withReplacement = <span class="literal">false</span>, fraction, seed).collect()</span><br><span class="line">        <span class="comment">// 以采样率的倒数做权重</span></span><br><span class="line">        <span class="keyword">val</span> weight = (<span class="number">1.0</span> / fraction).toFloat</span><br><span class="line">        candidates ++= reSampled.map(x =&gt; (x, weight))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 返回每个分区边界数据的数组（见下），数组长度为分区个数 - 1 (很好理解，切4份西瓜，需要3刀)</span></span><br><span class="line">      <span class="type">RangePartitioner</span>.determineBounds(candidates, math.min(partitions, candidates.size))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="蓄水池采样算法（Reservoir-Sampling）"><a class="header-anchor" href="#蓄水池采样算法（Reservoir-Sampling）">¶</a>蓄水池采样算法（Reservoir Sampling）</h2>
<ul>
<li>场景：数据流长度N很大且不可知，不能一次性存入内存；保证时间复杂度为O(N)；随机选取k个数，每个数被选中的概率为 k/N。</li>
<li>步骤：
<ol>
<li>如果数据总量小于k，则依次放入蓄水池。池子满了，进入步骤2。</li>
<li>当遍历到第i个数据时，在[0, i]范围内取以随机数d，若d的落在[0, k-1]范围内，则用该数据替换蓄水池中的第d个数据。</li>
<li>重复步骤2，直到遍历完。</li>
</ol>
</li>
</ul>
<p>想深究原理的看<a href="https://www.jianshu.com/p/7a9ea6ece2af" target="_blank" rel="noopener">这个</a>，下面是 spark 中对该算法是实现。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sketch</span></span>[<span class="type">K</span> : <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">K</span>],</span><br><span class="line">    sampleSizePerPartition: <span class="type">Int</span>): (<span class="type">Long</span>, <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Long</span>, <span class="type">Array</span>[<span class="type">K</span>])]) = &#123;</span><br><span class="line">  <span class="keyword">val</span> shift = rdd.id</span><br><span class="line">  <span class="comment">// val classTagK = classTag[K] // to avoid serializing the entire partitioner object</span></span><br><span class="line">  <span class="keyword">val</span> sketched = rdd.mapPartitionsWithIndex &#123; (idx, iter) =&gt;</span><br><span class="line">    <span class="keyword">val</span> seed = byteswap32(idx ^ (shift &lt;&lt; <span class="number">16</span>))</span><br><span class="line">    <span class="keyword">val</span> (sample, n) = <span class="type">SamplingUtils</span>.reservoirSampleAndCount(</span><br><span class="line">      iter, sampleSizePerPartition, seed)</span><br><span class="line">    <span class="type">Iterator</span>((idx, n, sample))</span><br><span class="line">  &#125;.collect()</span><br><span class="line">  <span class="keyword">val</span> numItems = sketched.map(_._2).sum</span><br><span class="line">  (numItems, sketched)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>核心步骤是通过 <code>SamplingUtils.reservoirSampleAndCount(xxx)</code> 得到采样结果</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reservoirSampleAndCount</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    input: <span class="type">Iterator</span>[<span class="type">T</span>],</span><br><span class="line">    k: <span class="type">Int</span>,</span><br><span class="line">    seed: <span class="type">Long</span> = <span class="type">Random</span>.nextLong())</span><br><span class="line">  : (<span class="type">Array</span>[<span class="type">T</span>], <span class="type">Long</span>) = &#123;</span><br><span class="line">  <span class="keyword">val</span> reservoir = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](k) <span class="comment">// 装采样点的蓄水池</span></span><br><span class="line">  <span class="comment">// Put the first k elements in the reservoir.</span></span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> (i &lt; k &amp;&amp; input.hasNext) &#123;</span><br><span class="line">    <span class="keyword">val</span> item = input.next()</span><br><span class="line">    reservoir(i) = item</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If we have consumed all the elements, return them. Otherwise do the replacement.</span></span><br><span class="line">  <span class="keyword">if</span> (i &lt; k) &#123;</span><br><span class="line">    <span class="comment">// If input size &lt; k, trim the array to return only an array of input size.</span></span><br><span class="line">    <span class="comment">// 如果数据总个数不足采样个数，那就全部采样了，然后返回</span></span><br><span class="line">    <span class="keyword">val</span> trimReservoir = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](i)</span><br><span class="line">    <span class="type">System</span>.arraycopy(reservoir, <span class="number">0</span>, trimReservoir, <span class="number">0</span>, i)</span><br><span class="line">    (trimReservoir, i)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// If input size &gt; k, continue the sampling process.</span></span><br><span class="line">    <span class="keyword">var</span> l = i.toLong</span><br><span class="line">    <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">XORShiftRandom</span>(seed)</span><br><span class="line">    <span class="comment">// 对整个 input 遍历一次</span></span><br><span class="line">    <span class="keyword">while</span> (input.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> item = input.next()</span><br><span class="line">      l += <span class="number">1</span></span><br><span class="line">      <span class="comment">// There are k elements in the reservoir, and the l-th element has been</span></span><br><span class="line">      <span class="comment">// consumed. It should be chosen with probability k/l. The expression</span></span><br><span class="line">      <span class="comment">// below is a random long chosen uniformly from [0,l)</span></span><br><span class="line">      <span class="keyword">val</span> replacementIndex = (rand.nextDouble() * l).toLong <span class="comment">// 取[0,l)的随机数d</span></span><br><span class="line">      <span class="comment">// 如果 d 在 k 的范围内，则用 item 替换池子里的第d个数据。</span></span><br><span class="line">      <span class="keyword">if</span> (replacementIndex &lt; k) &#123;</span><br><span class="line">        reservoir(replacementIndex.toInt) = item</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    (reservoir, l)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="按权重选择边界"><a class="header-anchor" href="#按权重选择边界">¶</a>按权重选择边界</h2>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Determines the bounds for range partitioning from candidates with weights indicating how many</span></span><br><span class="line"><span class="comment">  * items each represents. Usually this is 1 over the probability used to sample this candidate.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param candidates unordered candidates with weights</span></span><br><span class="line"><span class="comment">  * @param partitions number of partitions</span></span><br><span class="line"><span class="comment">  * @return selected bounds</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">determineBounds</span></span>[<span class="type">K</span> : <span class="type">Ordering</span> : <span class="type">ClassTag</span>](</span><br><span class="line">     candidates: <span class="type">ArrayBuffer</span>[(<span class="type">K</span>, <span class="type">Float</span>)],</span><br><span class="line">     partitions: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">   <span class="keyword">val</span> ordering = implicitly[<span class="type">Ordering</span>[<span class="type">K</span>]]</span><br><span class="line">   <span class="keyword">val</span> ordered = candidates.sortBy(_._1) <span class="comment">// 把采样点排好序</span></span><br><span class="line">   <span class="keyword">val</span> numCandidates = ordered.size</span><br><span class="line">   <span class="keyword">val</span> sumWeights = ordered.map(_._2.toDouble).sum</span><br><span class="line">   <span class="keyword">val</span> step = sumWeights / partitions</span><br><span class="line">   <span class="keyword">var</span> cumWeight = <span class="number">0.0</span></span><br><span class="line">   <span class="keyword">var</span> target = step</span><br><span class="line">   <span class="keyword">val</span> bounds = <span class="type">ArrayBuffer</span>.empty[<span class="type">K</span>]</span><br><span class="line">   <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">   <span class="keyword">var</span> j = <span class="number">0</span></span><br><span class="line">   <span class="keyword">var</span> previousBound = <span class="type">Option</span>.empty[<span class="type">K</span>]</span><br><span class="line">   <span class="keyword">while</span> ((i &lt; numCandidates) &amp;&amp; (j &lt; partitions - <span class="number">1</span>)) &#123;</span><br><span class="line">     <span class="keyword">val</span> (key, weight) = ordered(i)</span><br><span class="line">     cumWeight += weight</span><br><span class="line">     <span class="keyword">if</span> (cumWeight &gt;= target) &#123;</span><br><span class="line">       <span class="comment">// Skip duplicate values.</span></span><br><span class="line">       <span class="keyword">if</span> (previousBound.isEmpty || ordering.gt(key, previousBound.get)) &#123;</span><br><span class="line">         bounds += key</span><br><span class="line">         target += step</span><br><span class="line">         j += <span class="number">1</span></span><br><span class="line">         previousBound = <span class="type">Some</span>(key)</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     i += <span class="number">1</span></span><br><span class="line">   &#125;</span><br><span class="line">   bounds.toArray</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h2 id="getPartition-key-Any"><a class="header-anchor" href="#getPartition-key-Any">¶</a>getPartition(key: Any)</h2>
<p>有了边界，<code>getPartition(key: Any)</code>就很好计算了，其实就是个在有序区间找位置的过程。分区少就一个个比过去，如果区间数大于128，就使用二分查找获取分区位置。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">K</span>]</span><br><span class="line">  <span class="keyword">var</span> partition = <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> (rangeBounds.length &lt;= <span class="number">128</span>) &#123;</span><br><span class="line">    <span class="comment">// If we have less than 128 partitions naive search</span></span><br><span class="line">    <span class="keyword">while</span> (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) &#123;</span><br><span class="line">      partition += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 二分查找</span></span><br><span class="line">    partition = binarySearch(rangeBounds, k)</span><br><span class="line">    <span class="comment">// binarySearch either returns the match location or -[insertion point]-1</span></span><br><span class="line">    <span class="keyword">if</span> (partition &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      partition = -partition<span class="number">-1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (partition &gt; rangeBounds.length) &#123;</span><br><span class="line">      partition = rangeBounds.length</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 根据升序还是降序，返回相应的PartitionId。</span></span><br><span class="line">  <span class="keyword">if</span> (ascending) &#123;</span><br><span class="line">    partition</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    rangeBounds.length - partition</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark杂谈-使用textFile读取HDFS的分区规则</title>
    <url>/2019/11/27/spark/spark%E6%9D%82%E8%B0%88-%E4%BD%BF%E7%94%A8textFile%E8%AF%BB%E5%8F%96HDFS%E7%9A%84%E5%88%86%E5%8C%BA%E8%A7%84%E5%88%99/</url>
    <content><![CDATA[<p>使用 textFile 读取HDFS的数据分区规则</p>
<a id="more"></a>
<h1>跟着源码走</h1>
<p>测试文件：大小 516.06 MB ，54个 block，blockSize 大小是128M，但每个 block 里面的数据只有10M 左右</p>
<p><strong>1. 进入 <code>sc.textFile()</code></strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> textFile: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"hdfs://xxxx"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// textFile() 有个默认值：minPartitions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions)</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 它是取 defaultParallelism 和 2 的最小值 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">defaultMinPartitions</span></span>: <span class="type">Int</span> = math.min(defaultParallelism, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这个 defaultParallelism 不指定就是 totalCores，我这里是4</span></span><br><span class="line">scheduler.conf.getInt(<span class="string">"spark.default.parallelism"</span>, totalCores)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 所以 defaultMinPartitions 最终为2</span></span><br></pre></td></tr></table></figure>
<p><strong>2. 创建 <code>HadoopRDD</code></strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">HadoopRDD</span>(</span><br><span class="line">  <span class="keyword">this</span>,</span><br><span class="line">  confBroadcast,</span><br><span class="line">  <span class="type">Some</span>(setInputPathsFunc),</span><br><span class="line">  inputFormatClass,</span><br><span class="line">  keyClass,</span><br><span class="line">  valueClass,</span><br><span class="line">  minPartitions).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>3. 每个RDD 都有一个 <code>getPartitions</code> 函数，由它得到分区号</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">	...</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">  		<span class="keyword">val</span> allInputSplits = getInputFormat(jobConf).getSplits(jobConf, minPartitions)</span><br><span class="line">  	...</span><br><span class="line">   <span class="keyword">val</span> array = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Partition</span>](inputSplits.size)</span><br><span class="line">   <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until inputSplits.size) &#123;</span><br><span class="line">   		array(i) = <span class="keyword">new</span> <span class="type">HadoopPartition</span>(id, i, inputSplits(i))</span><br><span class="line">   &#125;</span><br><span class="line">   array</span><br><span class="line">   &#125;</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>4. 采用<code>FileInputFormat</code> 里的 <code>getSplits()</code> 划分分区，先计算 splitSize</strong></p>
<p><code>getPartitions</code> 的 核心是 <code>getSplits()</code>，下面是计算分区关键步骤</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 总大小除2，为258M</span></span><br><span class="line">long goalSize = totalSize / (long)(numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这是人为设定的分区最小值，这个很好理解</span></span><br><span class="line">long minSize = <span class="type">Math</span>.max(job.getLong(<span class="string">"mapreduce.input.fileinputformat.split.minsize"</span>, <span class="number">1</span>L), <span class="keyword">this</span>.minSplitSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">// HDFS 文件的块大小，128M</span></span><br><span class="line">long blockSize = file.getBlockSize();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算 splitSize</span></span><br><span class="line">long splitSize = <span class="keyword">this</span>.computeSplitSize(goalSize, minSize, blockSize);</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>假设文件大小为 20M： <code>splitSize = max（1，min(10,128)) = 10M</code></p>
</li>
<li>
<p>假设文件大小为 516M：<code>splitSize  = max(1, min(258,128)) = 128M </code>（本文）</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> long computeSplitSize(long goalSize, long minSize, long blockSize) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="type">Math</span>.max(minSize, <span class="type">Math</span>.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>5. 最后，按 splitSize 切分区</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 可以发现为了防止最后一个分区过小的问题，引入了数字 1.1，保证最后一个分区的大小大于 splitSize  的 10%</span></span><br><span class="line"><span class="keyword">for</span>(bytesRemaining = length; (double)bytesRemaining / (double)splitSize &gt; <span class="number">1.1</span>D; bytesRemaining -= splitSize) &#123;</span><br><span class="line">	splitHosts = <span class="keyword">this</span>.getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);</span><br><span class="line">	splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, splitSize, splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>L) &#123;</span><br><span class="line">	splitHosts = <span class="keyword">this</span>.getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, bytesRemaining, clusterMap);</span><br><span class="line">	splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, bytesRemaining, splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>6 分区结果</strong></p>
<p>每块128M，最后一块略大，符合预期。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs://xxxx:0+134217728</span><br><span class="line">hdfs://xxxx:134217728+134217728</span><br><span class="line">hdfs://xxxx:268435456+134217728</span><br><span class="line">hdfs://xxxx:402653184+138476793</span><br></pre></td></tr></table></figure>
<h1>小结</h1>
<ul>
<li>
<p>这里比较奇葩的是 minPartitions 这个设定，它最大只能是2。我觉得之所以这样设定，是防止文件切的过小。假设整个文件大小只有5M，公式：<code>Math.min(goalSize, blockSize)</code> blockSize假定128M，此时 splitSize 由 minPartitions 决定（不考虑人为设定的那个minSize）。那么它最多只能被切成2份。</p>
</li>
<li>
<p>当文件较大时（大于blockSize两倍），只和 blockSize 有关。尽管我的测试文件中每个 block 实际大小只有10M，然鹅这个并没有什么软用。</p>
</li>
<li>
<p>这是单文件情况，如果是读一个目录下的多文件，那就是单独对每个文件进行切分。（从源码可以发现，其中的 totalSize 是所有文件大小总和）。</p>
</li>
<li>
<p>当然这只是分区划分，实际读取数据没这么简单。假如我们是一条一条读，那么如果该分区最后一条数据没读完，它会接着向下一块继续读，参考<a href="https://hadoopi.wordpress.com/2013/05/27/understand-recordreader-inputsplit/" target="_blank" rel="noopener">它</a>。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS根目录扩容</title>
    <url>/2019/11/23/linux/CentOS%E6%A0%B9%E7%9B%AE%E5%BD%95%E6%89%A9%E5%AE%B9/</url>
    <content><![CDATA[<p>记录对 /root 目录的扩容</p>
<a id="more"></a>
<h1>问题：<code>/root</code> 的空间用满了</h1>
<p>本来打算直接动态扩容，也就是按鸟哥写的放大LV容量，把 <code>/home</code> 的空间分点给 <code>/root</code> 。结果发现 xfs 文件系统只支持动态增加，不能减少。因此咱只能备份重装了。</p>
<h1>解决：</h1>
<h2 id="1-我的版本"><a class="header-anchor" href="#1-我的版本">¶</a>1 我的版本</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /etc/redhat-release</span><br><span class="line">CentOS Linux release 7.6.1810 (Core)</span><br></pre></td></tr></table></figure>
<h2 id="2-分区情况"><a class="header-anchor" href="#2-分区情况">¶</a>2 分区情况</h2>
<p>CentOS 的 <code>/root</code> 和 <code>/home</code> 目录使用了LVM（逻辑卷分区）</p>
<p>我准备给 <code>/root</code> 加100G，把 <code>/home</code> 改为700G，预留 50G</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ df -hl</span><br><span class="line">Filesystem           Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/cl-root   50G   46G  5.0G  91% /</span><br><span class="line">...</span><br><span class="line">/dev/mapper/cl-home  849G  220G  629G  26% /home</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们要调的就是这个LV Size</span></span><br><span class="line">$ lvdisplay</span><br><span class="line">  ...</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/home</span><br><span class="line">  LV Name                home</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                &lt;849.07 GiB</span><br><span class="line">  Current LE             217361</span><br><span class="line"></span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/root</span><br><span class="line">  LV Name                root</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                50.00 GiB</span><br><span class="line">  Current LE             12800</span><br></pre></td></tr></table></figure>
<h2 id="3-备份-home"><a class="header-anchor" href="#3-备份-home">¶</a>3 备份 <code>/home</code></h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar 压缩，-p 保留权限，-z 使用 gzip</span></span><br><span class="line">$ tar cvpfz /mnt/data/homeback.tgz /home</span><br></pre></td></tr></table></figure>
<h2 id="4-删除-home"><a class="header-anchor" href="#4-删除-home">¶</a>4 删除 <code>/home</code></h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 干掉/home文件系统的进程</span></span><br><span class="line">$ fuser -km /home/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载/home，如果没用，加 -l 强制卸载</span></span><br><span class="line">$ umount /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 /home 的lv</span></span><br><span class="line">$ lvremove /dev/mapper/cl-home</span><br></pre></td></tr></table></figure>
<h2 id="5-扩容-root"><a class="header-anchor" href="#5-扩容-root">¶</a>5 扩容 <code>/root</code></h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我这里加 100G</span></span><br><span class="line">$ lvextend -L +100G /dev/mapper/cl-root</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刷新 /root文件系统</span></span><br><span class="line">$ xfs_growfs /dev/mapper/cl-root</span><br></pre></td></tr></table></figure>
<h2 id="6-恢复-home"><a class="header-anchor" href="#6-恢复-home">¶</a>6 恢复 <code>/home</code></h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分 700G 给它。（预留 50G 的 空闲空间）</span></span><br><span class="line">$ lvcreate -L 700G -n /dev/mapper/cl-home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建文件系统</span></span><br><span class="line">$ mkfs.xfs /dev/mapper/cl-home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂载 /home</span></span><br><span class="line">$ mount /dev/mapper/cl-home /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件恢复</span></span><br><span class="line">$ tar xvpfz /mnt/data/homeback.tgz -C /</span><br></pre></td></tr></table></figure>
<h2 id="7-检查结果"><a class="header-anchor" href="#7-检查结果">¶</a>7 检查结果</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ df -hl</span><br><span class="line">Filesystem           Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/cl-root  150G   46G  105G  31% /</span><br><span class="line">...</span><br><span class="line">/dev/mapper/cl-home  700G  220G  480G  32% /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查VG，还有 50G 剩余，稳稳的</span></span><br><span class="line">$ vgdisplay</span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               cl</span><br><span class="line">  Cur LV                3</span><br><span class="line">  ...</span><br><span class="line">  VG Size               &lt;930.51 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              238210</span><br><span class="line">  Alloc PE / Size       225648 / &lt;881.44 GiB</span><br><span class="line">  Free  PE / Size       12562 / 49.07 GiB</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 检查LV，和预想一样</span></span><br><span class="line">$ lvdisplay</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/root</span><br><span class="line">  LV Name                root</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                150.00 GiB</span><br><span class="line">  Current LE             38400</span><br><span class="line"></span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/home</span><br><span class="line">  LV Name                home</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                700.00 GiB</span><br><span class="line">  Current LE             179200</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>LVM</tag>
      </tags>
  </entry>
  <entry>
    <title>hbase-基本原理</title>
    <url>/2019/09/29/%E5%A4%A7%E6%95%B0%E6%8D%AE/hbase-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>简单介绍了hbase的基本原理：数据结构、读写操作、flush、合并与切分</p>
<a id="more"></a>
<h1>数据结构</h1>
<h2 id="逻辑结构"><a class="header-anchor" href="#逻辑结构">¶</a>逻辑结构</h2>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/luoji.png" alt></p>
<p>一个<strong>namespace</strong>可以有多个<strong>表</strong>，如<code>zxylearn:student</code>，代表<code>zxylearn</code>命名空间下的<code>student</code>表。</p>
<p>一个<strong>表</strong>可以有多个<strong>region</strong>，如上图有3个region。一个region一个文件夹。可以预分区（推荐）或者当一个region过大时会自动触发切分。</p>
<p>一个<strong>region</strong>可以有多个<strong>store</strong>（按列族划分），如上图有两个列族。一个store一个文件夹，文件夹名是列族名。</p>
<p>一个<strong>store</strong>可以有多个<strong>HFile</strong>，flush一次产生一个，可以合并（后面会讲）</p>
<p><strong>HFile</strong>中就是具体数据了，逻辑上是一行行序列化的数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># HDFS中的一个HFile的完整路径：</span><br><span class="line">/hbase/data/zxylearn/student/0fe474b3eb5fef1573d85acae8e5b787/info1/2dce02f62ea446f892a0b3cfc4e4db8a</span><br><span class="line"># namespace  zxylearn</span><br><span class="line"># 表名        student</span><br><span class="line"># region     0fe474b3eb5fef1573d85acae8e5b787</span><br><span class="line"># 列族名      info1</span><br><span class="line"># HFile      2dce02f62ea446f892a0b3cfc4e4db8a</span><br></pre></td></tr></table></figure>
<h2 id="物理结构"><a class="header-anchor" href="#物理结构">¶</a>物理结构</h2>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/wuli.png" alt></p>
<p>通过指定 <code>'namespace：表名'</code> 和 <code>'Row Key'</code>，可查找到数据的<code>列族</code>、<code>列名</code>、<code>timestamp</code>、<code>value</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):018:0&gt; get &apos;zxylearn:student&apos;,&apos;1001&apos;</span><br><span class="line">COLUMN                  CELL</span><br><span class="line"> info1:name             timestamp=1569722805794, value=zhangsan</span><br></pre></td></tr></table></figure>
<h1>写数据流程</h1>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/write.png" alt></p>
<ol>
<li>客户端向ZK请求返回 元数据表所在的RegionServer地址。（我们可以在ZK客户端中用<code>get /hbase/meta-region-server</code>看到它）</li>
<li>客户端接收地址，向它请求返回 待写数据所在的RegionServer地址。（我们可以在hbase客户端用<code>scan 'hbase:meta'</code>看到它：<code>column=info:server, timestamp=1569658344714, value=localhost:16020</code>）</li>
<li>向该地址请求写数据。</li>
<li>先将写操作记录到操作日志（<strong>WAL</strong>）中，接着将数据写入<strong>内存</strong>中。</li>
</ol>
<h1>Flush</h1>
<p>可以看出，写数据是将数据写入到内存中。当执行Flush刷写操作时，才会将数据写入磁盘，也就是HDFS中，形成一个HFile。</p>
<p><strong>Flush时机</strong></p>
<ul>
<li>大小超过限制：如RegionServer的全局内存大小，默认是堆大小的40%时刷写；单个region大小，默认128M。</li>
<li>时间：从最后一条数据写入后，1h（默认）没有新数据。</li>
</ul>
<p><strong>Flush细节</strong></p>
<ul>
<li><strong>flush会删除过期的数据</strong>。假设建表时数据版本数设置为1，那么写入磁盘的的数据最多只有一个版本；如果删除标记是DeleteColumn，那么会删除比它低版本的数据。(<strong>删除标记有多种</strong>，如 Delete：只删自己；DeleteColumn：删除自己与比自己低的)</li>
<li><strong>flush不会删除带删除标记的数据</strong>。原因：设想，我们原本想删除一条数据，给它打上删除标记。如果flush删除了，假如磁盘中的其它HFile中有该数据的旧版本，那么它们在<strong>合并操作</strong>（后面会讲）时就不会被删除了。</li>
</ul>
<p>反正记住这里会干掉不要的数据（版本数与删除标记决定），但<strong>不会干掉带删除标记的数据</strong>。</p>
<h1>读数据流程</h1>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/read.png" alt></p>
<ol>
<li>找到数据所在RS服务器的流程和写数据基本一样。</li>
<li>向该地址请求读数据。</li>
<li>读取cache（缓存）和MemStore（内存），查找该数据。</li>
<li>如果cache中没有，再去StoreFile（磁盘）中找该数据。</li>
<li>取时间戳最新的数据返回，并记录到cache中。</li>
</ol>
<p><strong>为什么在内存中找到了，还要去磁盘中找？</strong></p>
<p><strong>时间戳的缘故</strong>：我们用户自然是查找最新的数据，内存中的数据的时间戳不能保证一定比磁盘中的新，所有要把它们都找到，然后比较返回最新的数据。因此，这导致了<strong>hbase读数据比写数据还慢</strong>。所以，<strong>用cache缓存查到的数据</strong>，可以一定程度提高读数据的速度。</p>
<h1>compact(合并)与split(切分)</h1>
<h2 id="compact-合并"><a class="header-anchor" href="#compact-合并">¶</a>compact(合并)</h2>
<p>合并是<strong>将若干个小的HFile，合并成一个大的HFile</strong>。分<strong>Minor Compaction</strong> 和 <strong>Major Compaction</strong>。</p>
<ul>
<li>
<p><strong>Minor Compaction</strong> <strong>不会</strong>清理不要的数据和带删除标记的数据</p>
</li>
<li>
<p><strong>Major Compaction</strong> <strong>会</strong>清理不要的数据和带标记删除的数据。当HFile数大于等于3时，执行<code>compact</code>时执行的是它。</p>
</li>
</ul>
<p><strong>清理细节</strong></p>
<p>与flush不同，这里的清理不仅会清除不要的数据，还会<strong>清理带删除标记的数据</strong>（干掉它，和比它旧的数据）。</p>
<h2 id="split-切分"><a class="header-anchor" href="#split-切分">¶</a>split(切分)</h2>
<p><strong>切分是将大的region切分成若干个小的region</strong>。可以预分区（推荐）或者当一个region过大时会自动触发切分。</p>
<p><strong>为什么推荐只用一个列族呢？</strong></p>
<p><strong>多个列族可能导致小文件过多。<strong>假设一个列族的数据的很密集，另一列族很稀疏，那么在触发</strong>flush或者split</strong>时，密集的列族形成的HFile文件足够大没问题，但是稀疏的生成的就是小文件了，久而久之会形成过多小文件使效率降低。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>hbase-安装与配置</title>
    <url>/2019/09/28/%E5%A4%A7%E6%95%B0%E6%8D%AE/hbase-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>介绍hbase的安装与简单使用</p>
<a id="more"></a>
<h1>准备</h1>
<p>我的版本：</p>
<p><strong>hadoop 2.7.7</strong></p>
<p><strong>zookeeper 3.5.5</strong></p>
<p><strong>hbase 1.3.5</strong></p>
<p>先装好 hadoop 和 zookeeper</p>
<h1>配置 HBase</h1>
<h2 id="修改-hbase-env-sh"><a class="header-anchor" href="#修改-hbase-env-sh">¶</a>修改 <a href="http://hbase-env.sh" target="_blank" rel="noopener">hbase-env.sh</a></h2>
<p>修改<code>hbase-1.3.5/conf</code>下的<code>hbase-env.sh</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=$(/usr/libexec/java_home)</span><br><span class="line"><span class="comment"># 使用自己安装的ZK</span></span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure>
<h2 id="修改-hbase-site-xml"><a class="header-anchor" href="#修改-hbase-site-xml">¶</a>修改 hbase-site.xml</h2>
<p>修改<code>hbase-1.3.5/conf</code>下的<code>hbase-site.xml</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.master&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:60000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;localhost:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/<span class="built_in">local</span>/apache-zookeeper-3.5.5-bin/zkData&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h2 id="添加-core-site-xml-和-hdfs-site-xml"><a class="header-anchor" href="#添加-core-site-xml-和-hdfs-site-xml">¶</a>添加 core-site.xml 和 hdfs-site.xml</h2>
<p>直接在<code>hbase-1.3.5/conf</code>下创建<strong>软连接</strong>即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ln -s /usr/<span class="built_in">local</span>/hadoop-2.7.7/etc/hadoop/core-site.xml /usr/<span class="built_in">local</span>/hbase-1.3.5/conf/core-site.xml</span><br><span class="line"></span><br><span class="line">ln -s /usr/<span class="built_in">local</span>/hadoop-2.7.7/etc/hadoop/hdfs-site.xml /usr/<span class="built_in">local</span>/hbase-1.3.5/conf/hdfs-site.xml</span><br></pre></td></tr></table></figure>
<h2 id="修改-regionservers"><a class="header-anchor" href="#修改-regionservers">¶</a>修改 regionservers</h2>
<p>在<code>/hbase-1.3.5/conf/regionservers</code> 中添加<strong>集群节点的名字</strong>，用于群启与群关</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure>
<h1>使用</h1>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先开HDFS和ZK</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 单节点启动与关闭</span></span><br><span class="line">bin/hbase-daemon.sh start master</span><br><span class="line">bin/hbase-daemon.sh start regionserver</span><br><span class="line">bin/hbase-daemon.sh stop master</span><br><span class="line">bin/hbase-daemon.sh stop regionserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># 群启与群关</span></span><br><span class="line">bin/start-hbase.sh</span><br><span class="line">bin/stop-hbase.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化网页</span></span><br><span class="line">http://localhost:16010</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正常启动后的JPS</span></span><br><span class="line">977 ResourceManager</span><br><span class="line">865 NameNode</span><br><span class="line">4289 Jps</span><br><span class="line">4146 HRegionServer</span><br><span class="line">915 DataNode</span><br><span class="line">1028 NodeManager</span><br><span class="line">2360 QuorumPeerMain</span><br><span class="line">4062 HMaster</span><br></pre></td></tr></table></figure>
<h1>简单shell操作</h1>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 进入shell</span></span><br><span class="line">bin/hbase shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># 命名空间的相关操作</span></span><br><span class="line">create_namespace <span class="string">'zxylearn'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 表的相关操作</span></span><br><span class="line"><span class="comment"># 建表</span></span><br><span class="line">create <span class="string">'zxylearn:student'</span>,<span class="string">'info1'</span>,<span class="string">'info2'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 看表</span></span><br><span class="line">describe <span class="string">'zxylearn:student'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删表</span></span><br><span class="line"><span class="built_in">disable</span> <span class="string">'zxylearn:student'</span></span><br><span class="line">drop <span class="string">'zxylearn:student'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据的相关操作</span></span><br><span class="line"><span class="comment"># 增</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:name'</span>,<span class="string">'zhangsan'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:sex'</span>,<span class="string">'man'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1002'</span>,<span class="string">'info1:age'</span>,<span class="string">'22'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1002'</span>,<span class="string">'info2:addr'</span>,<span class="string">'wuhan'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1003'</span>,<span class="string">'info2:addr'</span>,<span class="string">'beijing'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查</span></span><br><span class="line">scan <span class="string">'zxylearn:student'</span>,&#123;STARTROW=&gt;<span class="string">'1001'</span>,STOPROW=&gt;<span class="string">'1003'</span>&#125;</span><br><span class="line">get <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span></span><br><span class="line">get <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:name'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 改</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:name'</span>,<span class="string">'zxy'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删</span></span><br><span class="line">delete <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:sex'</span></span><br><span class="line">deleteall <span class="string">'zxylearn:student'</span>, <span class="string">'1002'</span></span><br><span class="line">truncate <span class="string">'zxylearn:student'</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>配置</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>zookeeper-基本原理</title>
    <url>/2019/09/24/%E5%A4%A7%E6%95%B0%E6%8D%AE/zookeeper-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>介绍了zookeeper的基本原理和使用</p>
<a id="more"></a>
<h1>简介</h1>
<p>ZooKeeper是个分布式的服务协调框架。具体用途如：<strong>统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理</strong>等。它基于<strong>观察者</strong>的设计模式；zookeeper = 文件系统 + 监听通知机制。</p>
<p>特点：</p>
<ol>
<li>最终一致性：Client不论连接到哪个Server，得到的数据都是一样的。</li>
<li>原子性：事务中的所有操作全部执行或者全部不执行。</li>
<li>顺序性：同一个client的请求顺序执行。</li>
<li>分区容错性：zookeeper是分布式的，所以在部分节点出现故障时，可以自己恢复。</li>
</ol>
<h1>数据结构</h1>
<p><strong>znode</strong>：zk是层级树状结构，一个节点就是znode。默认可存<strong>1M数据</strong>。</p>
<ul>
<li>操作节点时可设置<strong>watcher</strong>。当节点状态发生变化时，就会触发watcher对应的操作，只触发一次。</li>
<li><strong>永久节点</strong>：创建后永久存在，除非主动删除；<strong>临时节点</strong>：临时创建的，会话结束节点自动被删除</li>
<li><strong>顺序节点</strong>：节点名称后面自动增加一个10位数字的序列号；<strong>非顺序节点</strong>：不加序列号</li>
</ul>
<p>节点衍生出分类是为了迎合需求。临时节点可以记录服务器是否上线：当服务器下线，临时节点的数据消除。顺序节点可用于同一服务器多次上下线，每次名字都不同。很明显2个2分类，两两组合有4种节点。</p>
<p>下面是一个znode的数据结构：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一个znode的数据结构</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">stat</span> /zxy</span><br><span class="line">cZxid = 0x39                                <span class="comment"># 节点创建时的zxid</span></span><br><span class="line">ctime = Mon Sep 23 16:38:23 CST 2019        <span class="comment"># 节点创建时的时间戳</span></span><br><span class="line">mZxid = 0x3d                                <span class="comment"># 节点修改时的zxid</span></span><br><span class="line">mtime = Mon Sep 23 16:38:44 CST 2019        <span class="comment"># 节点修改时的时间戳</span></span><br><span class="line">pZxid = 0x39                                <span class="comment"># 改变子节点时的zxid</span></span><br><span class="line">cversion = 0                                <span class="comment"># 子节点的版本号</span></span><br><span class="line">dataVersion = 3                             <span class="comment"># 数据版本：初始0，改变一次加1</span></span><br><span class="line">aclVersion = 0                              <span class="comment"># ACL版本（权限）</span></span><br><span class="line">ephemeralOwner = 0x0                        <span class="comment"># 数据拥有者：永久节点是0；临时节点是创建者的id</span></span><br><span class="line">dataLength = 13                             <span class="comment"># 数据长度</span></span><br><span class="line">numChildren = 0                             <span class="comment"># 子节点个数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对节点的操作无非是增删改查等，这里就不写了</span></span><br></pre></td></tr></table></figure>
<p>补充：</p>
<p><strong>zxid</strong>（ZooKeeper Transaction Id）：ZooKeeper每次状态变化将会产生一个叫zxid的时间戳。</p>
<h1>原理</h1>
<p>基本流程：</p>
<p>客户端发请求（可带watcher） -&gt;  zk 选举与恢复 (没leader时) -&gt; zk 读写数据 -&gt; 返回数据（可触发watcher）</p>
<p>两端主要是<strong>监听器的原理</strong>，中间主要用到<strong>ZAB协议</strong>。</p>
<h2 id="监听器原理"><a class="header-anchor" href="#监听器原理">¶</a>监听器原理</h2>
<p>watcher 相当于一个的炸弹，客户端发请求时：如<code>ls，get，set</code>等，可以给节点绑上炸弹。</p>
<p>如果触发了爆炸条件：<code>ls</code>就是该节点有增加删减子节点；<code>get set</code>就是该节点数据改变。</p>
<p>炸弹爆炸（只炸一次）也就是执行里面的回调函数。</p>
<p>很明显这里用户端至少需要启动两个线程：connect线程负责网络连接通信：绑炸弹并把任务发给zk与后续数据互传；listener线程监听炸弹爆炸信号。</p>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/zookeeper/zk.png" alt></p>
<h2 id="ZAB协议"><a class="header-anchor" href="#ZAB协议">¶</a>ZAB协议</h2>
<p>Zab协议（Zookeeper Atomic Broadcast），通过它来保证分布式事务的最终一致性。<br>
这个内容很多，详细的可以参考：</p>
<p><a href="https://www.jianshu.com/p/2bceacd60b8a" target="_blank" rel="noopener">Zookeeper——一致性协议:Zab协议</a></p>
<p><a href="https://www.cnblogs.com/felixzh/p/5869212.html" target="_blank" rel="noopener">Zookeeper的功能以及工作原理</a></p>
<p>主要功能：崩溃恢复(选举、数据恢复) 的 原子广播 (数据读写)</p>
<h3 id="崩溃恢复"><a class="header-anchor" href="#崩溃恢复">¶</a>崩溃恢复</h3>
<p>集群中必须有一个leader，leader出现故障时，采用投票选举新leader，它需要满足以下条件：</p>
<ul>
<li>新选举出来的 Leader 不能包含未提交的 Proposal 。</li>
<li>新选举的 Leader 节点中含有最大的 zxid 。</li>
<li>得到超过一半选票者称为 Leader，因此<strong>zk集群个数为奇数</strong></li>
</ul>
<p>并不是所有节点都是 leader 和 follower ，还有observer，它不参与选举。作用是：可以增加集群数量，又减少投票选举时间。</p>
<p>选出leader后，进行数据恢复也就是同步，这个没啥，就是让它们其它节点数据都和leader同步，毕竟咱要确保<strong>最终一致性</strong>。恢复完毕后，就可以处理客户端的请求了。</p>
<h3 id="读写数据"><a class="header-anchor" href="#读写数据">¶</a>读写数据</h3>
<p><strong>一般流程：</strong></p>
<p><strong>读请求</strong>，就是直接从当前节点中读取数据</p>
<p><strong>写请求</strong></p>
<ol>
<li>客户端发起一个写操作请求。</li>
<li>Leader 将客户端的请求转化为事务（Proposal），每个 Proposal 分配一个全局的ID，即zxid。</li>
<li>Leader 为每个 Follower 分配一个单独的队列，然后将需要广播的 Proposal 依次放到队列中取，并且根据 FIFO 策略进行消息发送。</li>
<li>Follower 接收到 Proposal 后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 Ack 响应消息。</li>
<li>Leader 接收到<strong>超过半数以上</strong> Follower 的 Ack 响应消息后，即认为消息发送成功，可以发送 commit 消息。</li>
<li>Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交。Follower 接收到 commit 消息后，会将上一条事务提交。</li>
</ol>
<p><strong>有意思的点</strong></p>
<ul>
<li>
<p>**如何保证消息有序：**在整个消息广播中，Leader会将每一个事务请求转换成对应的 proposal 来进行广播，并且在广播 事务Proposal 之前，Leader服务器会首先为这个事务Proposal分配一个全局单递增的唯一ID，称之为事务ID（即zxid），由于Zab协议需要保证每一个消息的严格的顺序关系，因此必须将每一个proposal按照其zxid的先后顺序进行排序和处理。</p>
</li>
<li>
<p>**用队列提高效率：**Leader 服务器与每一个 Follower 服务器之间都维护了一个单独的 FIFO 消息队列进行收发消息，使用队列消息可以做到异步解耦。 Leader 和 Follower 之间只需要往队列中发消息即可。如果使用完全同步的方式会引起阻塞，性能要下降很多。(我感觉这里应该不是FIFO 消息队列，应该是最小队列吧)</p>
</li>
<li>
<p><strong>记住超过半数</strong></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>hive-DML</title>
    <url>/2019/09/19/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive-DML/</url>
    <content><![CDATA[<p>hive中一些DML的操作</p>
<a id="more"></a>
<p>DML（数据操纵语言）主要指数据的增删查改</p>
<h1>数据导入</h1>
<p>有5种导入数据的方法，最常用的是 <strong>Load</strong> 和 <strong>Insert</strong></p>
<h2 id="Load"><a class="header-anchor" href="#Load">¶</a>Load</h2>
<p>从文件系统中导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> [<span class="keyword">local</span>] inpath <span class="string">'/xxxxxx'</span> </span><br><span class="line">[overwrite] <span class="keyword">into</span> <span class="keyword">table</span> tablename [<span class="keyword">partition</span> (partcol1=val1,…)];</span><br></pre></td></tr></table></figure>
<ul>
<li>local: 指本地文件系统，否则为HDFS</li>
<li>overwrite: 指覆盖表中已有数据，否则表示追加</li>
<li>partition: 表示上传到指定分区</li>
</ul>
<h2 id="Insert"><a class="header-anchor" href="#Insert">¶</a>Insert</h2>
<p>通过查询语句导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 覆盖</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tablename [<span class="keyword">partition</span>(partcol1=val1,partclo2=val2)] select_statement;</span><br><span class="line"><span class="comment"># 追加</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> tablename [<span class="keyword">partition</span>(partcol1=val1,partclo2=val2)] select_statement;</span><br></pre></td></tr></table></figure>
<h2 id="As-Select"><a class="header-anchor" href="#As-Select">¶</a>As Select</h2>
<p>查询语句中创建表并导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> tablename</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
<h2 id="Location"><a class="header-anchor" href="#Location">¶</a>Location</h2>
<p>创建表时通过Location指定数据的路径，再直接put数据到hdfs上</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /xxxxx /user/hive/warehouse/tablename;</span><br></pre></td></tr></table></figure>
<h2 id="Import"><a class="header-anchor" href="#Import">¶</a>Import</h2>
<p>只能导入export导出的数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">import table tablename partition(month='201909') from '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure>
<h1>数据导出</h1>
<p>最常用的是 <strong>Insert</strong> 和 <strong>Hadoop</strong></p>
<h2 id="Insert-v2"><a class="header-anchor" href="#Insert-v2">¶</a>Insert</h2>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将数据导入本地（并格式化处理），不加local就是导入HDFS</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/xxxxx'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tablename;</span><br></pre></td></tr></table></figure>
<h2 id="Hadoop"><a class="header-anchor" href="#Hadoop">¶</a>Hadoop</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop命令导出</span></span><br><span class="line">hive &gt; dfs -get /user/hive/warehouse/student/month=201909/000000_0 /xxxxxxx;</span><br></pre></td></tr></table></figure>
<h2 id="Export"><a class="header-anchor" href="#Export">¶</a>Export</h2>
<p>这个导出的数据除了数据还有元数据，可用Import导入</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive &gt; <span class="built_in">export</span> table tablename to <span class="string">'/user/hive/warehouse/export/student'</span>;</span><br></pre></td></tr></table></figure>
<h1>数据清除</h1>
<p>只能清除内部表的数据</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive &gt; truncate table tablename;</span><br></pre></td></tr></table></figure>
<h1>查询</h1>
<p>查询的关键字较多，要知道它们的<strong>顺序</strong>（重点）</p>
<p>写的顺序：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> ... <span class="keyword">join</span> <span class="keyword">on</span> ... <span class="keyword">where</span> ... <span class="keyword">group</span> <span class="keyword">by</span> ... <span class="keyword">having</span> ... <span class="keyword">order</span> <span class="keyword">by</span> ... <span class="keyword">limit</span> ...</span><br></pre></td></tr></table></figure>
<p>执行顺序：大体思路是 限定(where)，分组，限定（having），选择，排序</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">from -&gt; join on -&gt; where -&gt; group by -&gt; having -&gt; select -&gt; order by -&gt; limit</span><br></pre></td></tr></table></figure>
<h2 id="select…where…limit"><a class="header-anchor" href="#select…where…limit">¶</a>select…where…limit</h2>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 简单查询</span></span><br><span class="line"><span class="keyword">select</span> empno, ename <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) cnt <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal &gt;<span class="number">1000</span> <span class="keyword">limit</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure>
<h2 id="group-by-和-having"><a class="header-anchor" href="#group-by-和-having">¶</a>group by 和 having</h2>
<p>group by 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算emp表每个部门的平均工资</span></span><br><span class="line"><span class="keyword">select</span> t.deptno, <span class="keyword">avg</span>(t.sal) avg_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br></pre></td></tr></table></figure>
<p>where 作用在 分组（group by）和聚合（sum等）计算之前，选取哪些行，也就是在查询前筛选；having 对分组后<strong>计算的</strong>数据进行过滤。它<strong>只用于</strong>group by分组统计语句。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求每个部门的平均薪水大于2000的部门</span></span><br><span class="line"><span class="keyword">select</span> deptno, <span class="keyword">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal &gt; <span class="number">2000</span>;</span><br></pre></td></tr></table></figure>
<h2 id="join"><a class="header-anchor" href="#join">¶</a>join</h2>
<p>Hive支持通常的SQL JOIN语句，但是<strong>只支持等值连接，不支持非等值连接</strong>。 且 连接谓词中<strong>不支持or</strong>。<br>
这个非等值连接可以从以前学的<strong>reducejoin</strong>的流程思索原因，reducejoin是在shuffer时将数据按关联值相等的（on的条件）分为一组，再在reducer阶段进行处理。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 合并员工表和部门表</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>
<h2 id="排序"><a class="header-anchor" href="#排序">¶</a>排序</h2>
<h3 id="全局排序-Order-By"><a class="header-anchor" href="#全局排序-Order-By">¶</a>全局排序 Order By</h3>
<p>全局排序，只一个Reducer。全排很明显最后生成一个总的排序文件，<strong>1个reducer</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查询员工信息按工资降序排列</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
<h3 id="按reducer排序-sort-by"><a class="header-anchor" href="#按reducer排序-sort-by">¶</a>按reducer排序 sort by</h3>
<p>每个reducer端都会做排序，出来的数据是有序的。假如有n个Reducer，就会生成n个有序文件。当n=1时，它就是<code>Order By</code>。</p>
<p>扩展一波，Reducer个数默认按原始数据256M一个，当然也可手动设置其个数。</p>
<h3 id="分区排序-Distribute-By…Sort-By"><a class="header-anchor" href="#分区排序-Distribute-By…Sort-By">¶</a>分区排序 Distribute By…Sort By</h3>
<p>先分区，后排序。这个分区类型mapreduce的分区，多少个分区，就有多少个reduce任务，后面就生成多少个文件。说白了这个和上面的区别就是它通过<code>Distribute By</code>指定怎么分区，即指定怎么分reducer。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置reduce个数</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br><span class="line"><span class="comment"># 先按照部门编号分区，再按照员工编号降序排序</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/Users/zxy/IdeaProjects/bigdata-learning/hive-learning/data/output'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
<h3 id="分桶-Cluster-By"><a class="header-anchor" href="#分桶-Cluster-By">¶</a>分桶 Cluster By</h3>
<p>当distribute by 和 sorts by <strong>字段相同</strong>时，可以使用cluster by方式。但<strong>排序只能是升序排序</strong>。</p>
<p>可以从取名看出，我没用分桶排序。你可以理解 Cluster 就是把数据分区，然后每个分区生成一个文件，这样就好解释为啥只能升序排序，我理解它压根就不需要排序，只是把数据分到不同区就ok。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以下两种写法等价</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure>
<p><strong>细节一：来波小结理一下  分区表 分区排序 分桶(前两个分区意思截然不同)</strong></p>
<ul>
<li><code>partition(month='201909')</code> 这个是分区表，针对的是数据的存储路径</li>
<li><code>Distribute By...Sort By</code> 这个是分区排序，和<strong>MR中的分区</strong>概念一样，多少个分区，就有多少个reduce任务，后面就生成多少个文件，分区之后对区里的数据进行排序。</li>
<li><code>Cluster By</code> 分桶，针对的是数据文件，将大的数据集分区。</li>
</ul>
<p>所以将 Cluster By 理解为分区， Distribute By…Sort By 理解为分区排序，岂不美哉</p>
<p><strong>细节二：注意导入数据到分桶中，要用insert，且 设置<code>hive.enforce.bucketing=true</code>和<code> hive.enforce.bucketing=true</code></strong></p>
<p><strong>细节三：分桶抽样查询</strong></p>
<p><code>select * from tablename tablesample(bucket x out of y on id);</code></p>
<p>x 表示从哪个bucket开始抽取</p>
<p>y 表示抽样间隔，共抽取 总数/y 个桶，且x的值必须<strong>小于等于</strong>y的值</p>
<p>举例：如果 x = 1, y = 4 ，共16个桶，那么将抽取16/4个桶，分别是 1、5、9、13</p>
<h2 id="行转列、列转行"><a class="header-anchor" href="#行转列、列转行">¶</a>行转列、列转行</h2>
<p><strong>行转列</strong>：将不同行的聚合到一起</p>
<p><code>collect_set(col)</code>：函数<strong>只接受基本数据类型</strong>，它的主要作用是将某字段的值进行<strong>去重汇总</strong>(不去重用list)，产生array类型字段</p>
<p>例子：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    base,</span><br><span class="line">    <span class="keyword">concat_ws</span>(<span class="string">"|"</span>, collect_set(<span class="keyword">name</span>)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) base</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    constellation) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    base;</span><br></pre></td></tr></table></figure>
<p><strong>列转行</strong>：将列拆分成多行。</p>
<p><code>explode(col)</code> 将hive一列中复杂的array或者map结构拆分成多行</p>
<p><code>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</code> 用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p>
<p>例子：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure>
<h2 id="窗口函数"><a class="header-anchor" href="#窗口函数">¶</a>窗口函数</h2>
<p>基本结构：函数 over(范围)  。用前面的函数处理over中的规定的数据</p>
<p>除了count、sum等一些常用函数，还有<strong>只能配合over使用</strong>的函数：</p>
<ul>
<li><code>lag(col,n)</code>：往前第n行数据</li>
<li><code>lead(col,n)</code>：往后第n行数据</li>
<li><code>ntile(n)</code>：给数据编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。</li>
<li><code>rank()</code> 排序相同时会重复，总数不会变，如12225668</li>
<li><code>dense_rank()</code> 排序相同时会重复，总数会减少，如12223445</li>
<li><code>row_rank()</code> 单纯顺序计算，如12345678</li>
</ul>
<p>over里面可以规定窗口范围：</p>
<ul>
<li><code>()</code>：全部数据</li>
<li><code>(partition by xxx order by xxx)</code>：分区有序</li>
<li><code>(rows between xxxx and xxxx)</code>：手动指定范围
<ul>
<li><code>current row</code>：当前行</li>
<li><code>n preceding</code>：往前n行数据</li>
<li><code>n following</code>：往后n行数据</li>
<li><code>unbounded preceding</code>： 从起点开始</li>
<li><code>unbounded following</code>： 到终点结束</li>
</ul>
</li>
</ul>
<p>一些例子：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">business.name	business.orderdate	business.cost</span><br><span class="line">jack	2017-01-01	10</span><br><span class="line">tony	2017-01-02	15</span><br><span class="line">jack	2017-02-03	23</span><br><span class="line">tony	2017-01-04	29</span><br><span class="line">jack	2017-01-05	46</span><br><span class="line">jack	2017-04-06	42</span><br><span class="line">tony	2017-01-07	50</span><br><span class="line">jack	2017-01-08	55</span><br><span class="line">mart	2017-04-08	62</span><br><span class="line">mart	2017-04-09	68</span><br><span class="line">neil	2017-05-10	12</span><br><span class="line">mart	2017-04-11	75</span><br><span class="line">neil	2017-06-12	80</span><br><span class="line">mart	2017-04-13	94</span><br><span class="line"></span><br><span class="line"><span class="comment">#（1）查询在2017年4月份购买过的顾客及总人数，over()针对groupby后的全部数据</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    <span class="keyword">count</span>(*) <span class="keyword">over</span> () </span><br><span class="line"><span class="keyword">from</span> business </span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">    <span class="keyword">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) = <span class="string">'2017-04'</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">    <span class="keyword">name</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（2）查询顾客的购买明细 并 让cost按照日期进行累加</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>,</span><br><span class="line">    <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) </span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    business;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（3）查看顾客上次的购买时间，在窗口中分区排序</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>, </span><br><span class="line">    lag(orderdate, <span class="number">1</span>, <span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate)</span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（4）查询前20%时间的订单信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加分组号</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>, </span><br><span class="line">    ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line"><span class="keyword">from</span> business; t1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过滤出组号为1的数据</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>, </span><br><span class="line">    ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line"><span class="keyword">from</span> business) t1</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">    sorted = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（5）计算每个人消费的排名</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>,</span><br><span class="line">    <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">cost</span> <span class="keyword">desc</span>)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    business;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>hive-DDL</title>
    <url>/2019/09/17/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive-DDL/</url>
    <content><![CDATA[<p>hive中一些DDL的操作</p>
<a id="more"></a>
<h1>DDL</h1>
<p>DDL（数据定义语言）用来处理数据库中的各种对象，如数据库、表等</p>
<h2 id="数据库-database"><a class="header-anchor" href="#数据库-database">¶</a>数据库(database)</h2>
<h3 id="增"><a class="header-anchor" href="#增">¶</a>增</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个数据库，它在HDFS上的默认存储路径是/user/hive/warehouse/db_hive.db</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> db_hive;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 并 指定数据库在HDFS上存放的位置</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db_hive2 location <span class="string">'/db_hive2.db'</span>;</span><br></pre></td></tr></table></figure>
<h3 id="查"><a class="header-anchor" href="#查">¶</a>查</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示数据库</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示数据库详细信息</span></span><br><span class="line">desc database extended db_hive;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换数据库</span></span><br><span class="line"><span class="keyword">use</span> db_hive;</span><br></pre></td></tr></table></figure>
<h3 id="改"><a class="header-anchor" href="#改">¶</a>改</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改数据库</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> db_hive <span class="keyword">set</span> dbproperties(<span class="string">'createtime'</span>=<span class="string">'20190917'</span>);</span><br></pre></td></tr></table></figure>
<p>注意：数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。这个修改只是修改<code>DBPROPERTIES</code>里的键值对。</p>
<h3 id="删"><a class="header-anchor" href="#删">¶</a>删</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> db_hive;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 强制删除非空数据库</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> db_hive <span class="keyword">cascade</span>;</span><br></pre></td></tr></table></figure>
<h2 id="表-table"><a class="header-anchor" href="#表-table">¶</a>表(table)</h2>
<h3 id="增-v2"><a class="header-anchor" href="#增-v2">¶</a>增</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] <span class="comment">-- 注释</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] <span class="comment">-- 分区表</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="comment">-- 分桶表</span></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] <span class="comment">-- 不常用</span></span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] <span class="comment">-- 定义每行的格式</span></span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] <span class="comment">-- 指定存储文件类型</span></span><br><span class="line">[LOCATION hdfs_path] <span class="comment">-- 指定表在HDFS上的存储位置</span></span><br></pre></td></tr></table></figure>
<p><strong>一些细节：</strong></p>
<ul>
<li><strong>内部表与外部表</strong></li>
</ul>
<p><code>CREATE EXTERNAL TABLE</code> 用于创建外部表，默认是内部表（管理表）。它们的区别是 删除外部表并不会删除HDFS中的的数据，只会删除mysql中的元数据；而删除内部表都会删除。</p>
<p>可以这样理解外部表：HDFS上的数据是公有的，某个客户端建一了个hive表关联使用它，生成元数据，当该客户不用时，只删除他的元数据和hive表就行，公有数据仍然存在。</p>
<ul>
<li><strong>分区表</strong></li>
</ul>
<p>每个分区 对应一个HDFS文件系统上的独立的文件夹，该文件夹里包含该分区所有的数据。在查询时，可通过 WHERE 指定查询所需要的分区，查询效率会提高很多。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建表：</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test1(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据到指定分区：</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/xxxx'</span> <span class="keyword">into</span> <span class="keyword">table</span> test1 <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201909'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时，会数据会保存在 /user/hive/warehouse/test1/month=201909 中</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询指定分区的数据（可以直接将分区作为字段用）</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> test1 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201909'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二级分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test2(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>元数据和真实数据</strong></li>
</ul>
<p>元数据存在mysql中，包含数据库信息：ID、描述、HDFS路径、数据库名、所有者；分区信息；字段信息等等。而真实数据都存在HDFS中。</p>
<p><strong>它们可以自由独立存在</strong>，如外部表可以直接删除元数据。因此，创建数据时，如果直接放入分区中，由于元数据中没有分区信息，无法用where查到它，虽然数据存在。可以通过补充分区信息 或者 执行修复命令，让分区表和数据产生关联。</p>
<p>我的理解：先有数据，后有hive。hive要做的事就是关联到数据（生成元数据），然后CRUD它。</p>
<h3 id="改-v2"><a class="header-anchor" href="#改-v2">¶</a>改</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重命名</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">rename</span> <span class="keyword">to</span> new_table_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test1 <span class="keyword">add</span> <span class="keyword">columns</span>(newdesc <span class="keyword">string</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test1 <span class="keyword">change</span> <span class="keyword">column</span> <span class="keyword">id</span> <span class="keyword">desc</span> <span class="built_in">int</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test1 <span class="keyword">replace</span> <span class="keyword">columns</span>(<span class="keyword">name</span> <span class="keyword">string</span>, loc <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure>
<h3 id="查-v2"><a class="header-anchor" href="#查-v2">¶</a>查</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示表</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示表信息</span></span><br><span class="line">desc tablename;</span><br></pre></td></tr></table></figure>
<h3 id="删-v2"><a class="header-anchor" href="#删-v2">¶</a>删</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> test1;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>随笔-中秋入学有感</title>
    <url>/2019/09/16/%E9%9A%8F%E7%AC%94/%E9%9A%8F%E7%AC%94-20190916%E5%85%A5%E5%AD%A6/</url>
    <content><![CDATA[<p>很早想写，却迟迟未动笔。今有兴留以纪念。</p>
<a id="more"></a>
<p><strong>终于正式成为了研究生大军的一员～</strong></p>
<p>去年是中秋节结束进实验室的，今天亦是农历八月十六，按农历算刚好一年了呀。</p>
<p>这段时间，学了很多，杂七杂八的。学过人工智能，试过后台开发，如今又转战大数据。hh，可以说就差前端了（没系统学，但写后台多少会点）。。语言方面有：c++ ，Java，python ，scala。。可是，我觉得都只是会点皮毛而已。</p>
<p><strong>但是，我并不觉得是白学了。</strong></p>
<p>就拿现在大热的深度学习，我也学了可能大半年（算上大三下）。只有体验过，才知道深度学习没有那么玄乎；只有体验过，将来才可能不会说当初就应该学大热的深度学习了。毕竟，墙外的想进去，墙里面的想出来嘛。还记得，当初第一次看吴恩达的视频时，那感觉真像，发现了新大陆一样。可是渐渐也觉得有点厌倦。那时，我也喜欢在 leedcode 刷点算法题，对比我发现，比起调参，我可能更喜欢编程。比起不确定的准确率，一行行代码 让我更有成就感些～</p>
<p>后台开发 的学习让我对网站的整个流程有了认识。从有数据库的设计，到和前端的交互设计后台接口，再到网站部署，也算整了一个能用的东西出来。由于，后台更多是数据交互，它和大数据也是有很多联系之处的，而且主要都是基于Java的。hh，Java是很强大嘛。</p>
<p>一年的时光，我发现很多技术都是融合在一起的，后台和大数据都和数据打交道，集群的使用可以让人工智能的计算力更上一层楼。大数据更多的是分布式是思想，后台更专攻具体业务需求。很多后台人员可以过度到大数据，证明联系之深。同时人工智能的分布式计算环境的搭建，以后未尝不是一个热门需求呢。</p>
<p><strong>时光如白驹过隙。学的广，不如专精一门。</strong></p>
<p>如果说之前的学习，让我在几个大热领域都体验了一番，那么接下来要做的就是，在自己最喜欢的一个领域，专研下去。</p>
<p><strong>先定些小目标</strong></p>
<p>在第一个学期的前半个学期，把大数据的实用热门框架都过一遍，有个系统性的认识。后半个学期，看看是进阶下java，还是钻研下spark。在第二个学期，巩固大数据的几个最重要的框架，多研究java以及数据库。语言还是要学的精点好，学长说的好，就算你啥都不会，java玩的出神入化，照样抢着要。至于研二，到时再说吧～</p>
<p><strong>结尾了，打波鸡血</strong></p>
<p>《阿甘正传》里有个让我感触很深的情节。阿甘不知道该干什么，开始了一段跑步生涯。在这三年里，他做的仅仅只是不停的往前跑呀跑，跑呀跑。但是他的追随者从一个变两个，两个变三个，后面越来越多，越来越多，报道者也越来越多。你可能会说，这就是演电影吧？不！我觉得就算发生在现实，我相信结果同样会如此！</p>
<p>**可以说最容易的事是坚持，最难的也是坚持。**不管你学的是什么，只要你肯一直坚持下去，即使只是简单的每天跑，你也绝对会成功的！</p>
<p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1568649098110&amp;di=9c138a4f9097ca8373cfeba8edf1bdab&amp;imgtype=0&amp;src=http%3A%2F%2Fpic.rmb.bdstatic.com%2Fe61012997dcaf9d2ca611c60bfa03db7.jpg" alt></p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>hive-安装与配置</title>
    <url>/2019/09/16/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>介绍hive的安装，使用 mysql 做 hive 的元数据库</p>
<a id="more"></a>
<h1>准备</h1>
<p>我的版本：</p>
<p><strong>hadoop 2.7.7</strong></p>
<p><strong>hive 2.3.6</strong></p>
<p><strong>mysql 8.0.16</strong></p>
<p>先装好mysql（做hive的元数据库） 和 hadoop</p>
<h1>配置 Hive</h1>
<h2 id="修改-hive-env-sh"><a class="header-anchor" href="#修改-hive-env-sh">¶</a>修改 <a href="http://hive-env.sh" target="_blank" rel="noopener">hive-env.sh</a></h2>
<p>修改<code>apache-hive-2.3.6-bin/conf</code>下的<code>hive-env.sh</code>，加上hadoop路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HADOOP_HOME=/usr/local/hadoop-2.7.7</span><br><span class="line">export HIVE_CONF_DIR=/usr/local/apache-hive-2.3.6-bin/conf</span><br></pre></td></tr></table></figure>
<h2 id="创建-hive-site-xml"><a class="header-anchor" href="#创建-hive-site-xml">¶</a>创建 hive-site.xml</h2>
<p>在<code>apache-hive-2.3.6-bin/conf</code>下创建<code>hive-site.xml</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://localhost:3306/metastore?useSSL=false&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;metadata is stored in a MySQL server&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;MySQL JDBC driver class&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;user name for connecting to mysql server&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;你的密码&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;password for connecting to mysql server&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h2 id="添加-java连接-mysql-的jar包"><a class="header-anchor" href="#添加-java连接-mysql-的jar包">¶</a>添加 java连接 mysql 的jar包</h2>
<p>在<code>apache-hive-2.3.6-bin/lib</code>下添加 <a href="https://mvnrepository.com/artifact/mysql/mysql-connector-java" target="_blank" rel="noopener">java 连接 mysql 的jar包</a>（要对应mysql版本）。</p>
<p>我的是<code>mysql-connector-java-8.0.15.jar</code></p>
<h1>配置 mysql</h1>
<h2 id="配置远程登录权限-这个我没配，记着备用"><a class="header-anchor" href="#配置远程登录权限-这个我没配，记着备用">¶</a>配置远程登录权限(这个我没配，记着备用)</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 切换成mysql库</span><br><span class="line">use mysql; </span><br><span class="line"># 查询用户信息</span><br><span class="line">select User,Host,authentication_string from user; </span><br><span class="line"># 设置远程登录权限</span><br><span class="line">grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;xxxxxxxx&apos; with grant option; </span><br><span class="line"># 刷新配置信息</span><br><span class="line">flush privileges;</span><br><span class="line"># 退出</span><br><span class="line">exit</span><br></pre></td></tr></table></figure>
<h2 id="建-metastore-表"><a class="header-anchor" href="#建-metastore-表">¶</a>建 metastore 表</h2>
<p>表的名字与之前<code>hive-site.xml</code>中配置的目录名一样</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;value&gt;jdbc:mysql://localhost:3306/metastore?useSSL=false&lt;/value&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create database metastore</span><br></pre></td></tr></table></figure>
<h1>开始使用</h1>
<h2 id="初始化mysql元数据库（首次使用时）"><a class="header-anchor" href="#初始化mysql元数据库（首次使用时）">¶</a>初始化mysql元数据库（首次使用时）</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin/schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure>
<h2 id="使用hive"><a class="header-anchor" href="#使用hive">¶</a>使用hive</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  apache-hive-2.3.6-bin bin/hive</span><br><span class="line">Logging initialized using configuration in file:/usr/local/apache-hive-2.3.6-bin/conf/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">hive &gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>配置</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>flume-基本原理</title>
    <url>/2019/09/12/%E5%A4%A7%E6%95%B0%E6%8D%AE/flume-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>介绍了flume的基本原理和使用</p>
<a id="more"></a>
<h1>简介</h1>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/flume/agent.png" alt></p>
<p>Flume是一种<strong>分布式</strong>，<strong>可靠且可用</strong>的服务，用于<strong>有效地收集，聚合和传输大量日志数据</strong>，是基于<strong>流式</strong>的简单灵活的架构。</p>
<p>它具有可靠的<strong>可靠性机制</strong>和许多<strong>故障转移和恢复机制</strong>，具有强大的容错性。</p>
<p>它使用简单的<strong>可扩展</strong>数据模型，允许在线分析应用程序。</p>
<h1>流程图</h1>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/flume/flume.png" alt></p>
<h1>六大组件</h1>
<p>3大基本组件，3个辅助组件。这些组件都支持用户自定义。</p>
<h2 id="Source"><a class="header-anchor" href="#Source">¶</a>Source</h2>
<p>用于数据的收集。将数据捕获后可以先进行自定义处理，然后将数据封装到事件（event） 里（如event的body），最后将事件推入Channel中。</p>
<p><strong>常见的source</strong>:</p>
<p><strong>Avro Source</strong>、<strong>Exce Source</strong>、<strong>Spooling Directory Source</strong>、<strong>NetCat Source</strong>、Syslog Source、Syslog TCP Source、Syslog UDP Source、HTTP Source、<strong>HDFS Source</strong> 等等。</p>
<h2 id="Channel"><a class="header-anchor" href="#Channel">¶</a>Channel</h2>
<p>用于连接Source和Sink的组件，是数据的缓冲区。它可以将事件暂存到内存中也可以持久化到本地磁盘上， 直到Sink处理完该事件。</p>
<p><strong>常见的channel</strong>:</p>
<p><strong>Memory Channel</strong>、<strong>File Channel</strong>、<strong>Kafka Channel</strong>等等。</p>
<h2 id="Sink"><a class="header-anchor" href="#Sink">¶</a>Sink</h2>
<p>从channel中取出数据，再将数据存入相应的存储文件系统，数据库，或者提交到远程服务器。</p>
<p><strong>常见的sink</strong>:</p>
<p><strong>HDFS sink</strong>、<strong>Logger sink</strong>、<strong>Avro sink</strong>、<strong>File Roll sink</strong>、Null sink、HBase sink 等等。</p>
<h2 id="Interceptor"><a class="header-anchor" href="#Interceptor">¶</a>Interceptor</h2>
<p>对Source收集的数据，进行<strong>分类</strong>或者<strong>拦截</strong>。可以将多个Interceptor连接形成拦截器链。</p>
<p><strong>用法</strong></p>
<ul>
<li>
<p>分类：自定义分类逻辑，将分类属性(k,v类型)，加入event的headers中，然后使用<strong>MultiplexingChannelSelector</strong>选择器 选择放入哪个channel中。</p>
</li>
<li>
<p>拦截：自定义丢弃逻辑，将不要的event设为<strong>null</strong>即可。</p>
</li>
</ul>
<h2 id="ChannelSelector"><a class="header-anchor" href="#ChannelSelector">¶</a>ChannelSelector</h2>
<p>将event放入指定的channel中。</p>
<p><strong>2种ChannelSelector</strong>:</p>
<ul>
<li>
<p><strong>ReplicatingChannelSelector</strong>（默认） ：将事件放入所有channel中。（用于复制，也就是备份）</p>
</li>
<li>
<p><strong>MultiplexingChannelSelector</strong> ：结合<strong>Interceptor</strong>使用。根据header，将event放到指定的channel中。</p>
</li>
</ul>
<h2 id="sinkgroups中的SinkProcessor"><a class="header-anchor" href="#sinkgroups中的SinkProcessor">¶</a>sinkgroups中的SinkProcessor</h2>
<p>按照指定算法将event分配到sink组的sink中。需要先指定一个sink组，再选择SinkProcessor，它会根据配置的分配方式自动将event分到组里的sink中。注意默认的SinkProcessor中，没有sink组的概念，不需要配置，也就是一对一。</p>
<p><strong>2种SinkProcessor</strong>:</p>
<ul>
<li>
<p><strong>LoadBalanceSinkProcessor</strong> ：负载均衡。可选择分配方式：如随机分配、轮询分配等等</p>
</li>
<li>
<p><strong>FailoverSinkProcessor</strong> ：优先级分配（多用于故障转移）。指定sink的优先级，按优先级分配。</p>
</li>
</ul>
<h1>两个Transaction</h1>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/flume/transaction.png" alt></p>
<h2 id="Put-Transaction"><a class="header-anchor" href="#Put-Transaction">¶</a>Put Transaction</h2>
<p>它实现 source 将 event 发送至 channel，带有容错机制，可以分为以下阶段：</p>
<ul>
<li><strong>doPut</strong>: 将批数据写入临时缓冲区putList</li>
<li><strong>doCommit</strong>: 检查channel内存队列是否足够合并。</li>
<li><strong>doRollback</strong>: channel内存队列空间不足，回滚数据</li>
</ul>
<h2 id="Take-Transaction"><a class="header-anchor" href="#Take-Transaction">¶</a>Take Transaction</h2>
<p>它实现 sink 将 event 从 channel 中提取出来，带有容错机制，可以分为以下阶段：</p>
<ul>
<li><strong>doTake</strong>: 将数据提取到临时缓冲区takeList</li>
<li><strong>doCommit</strong>: 数据发送成功的前提下，清除临时缓冲区takeList</li>
<li><strong>doRollback</strong>: 数据发送过程中如果出现异常，rollback将临时缓冲区takeList中的数据归还给channel内存队列。</li>
</ul>
<h1>一些案例</h1>
<ul>
<li><strong>日志复制（备份）</strong></li>
</ul>
<p>单source，多channel，多sink</p>
<p>使用 ReplicatingChannelSelector，将每个event分到多个channel再传到sink中，实现复制（备份）。</p>
<ul>
<li><strong>日志分类</strong></li>
</ul>
<p>单source，多channel，多sink， 配置拦截器</p>
<p>使用 MultiplexingChannelSelector，根据拦截器将每个event，分到指定的channel中再传到sink中，实现分类。</p>
<ul>
<li><strong>负载均衡</strong></li>
</ul>
<p>单source，单channel，多sink（组成一个sink组）</p>
<p>使用LoadBalanceSinkProcessor，选择分配方式：如随机分配、轮询分配，将event递给相应sink。</p>
<ul>
<li><strong>故障转移</strong></li>
</ul>
<p>单source，单channel，多sink（组成一个sink组）</p>
<p>使用FailoverSinkProcessor，给sink组里的sink指定优先级，只有优先级最高的会接收，当它挂了，次优先级的才会接收。</p>
<ul>
<li><strong>日志聚合</strong></li>
</ul>
<p>多source，单channel，单sink</p>
<p>直接将多个源的数据，用一个channel接收。</p>
<ul>
<li><strong>可以使用第三方框架Ganglia对flume实现监控</strong></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title>spark基础-DataFrame和DataSet</title>
    <url>/2019/09/09/spark/spark%E5%9F%BA%E7%A1%80-DataFrame%E5%92%8CDataSet/</url>
    <content><![CDATA[<p>介绍了在spark中DataFrame和DataSet，以及它们之间的相互转换。</p>
<a id="more"></a>
<h1>概念分析</h1>
<p><strong>DataFrame</strong></p>
<p>类似传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。也就是普通RDD添加结构化信息得到。</p>
<p><strong>DataSet</strong></p>
<p>强类型的，存储的是对象。由<code>DataFrame</code>添加类属性得到。</p>
<p>相同点</p>
<ul>
<li>
<p>都是基于RDD的，所以都有RDD的特性，如懒加载，分布式，不可修改，分区等等。但执行sql性能比RDD高，因为spark自动会使用优化策略执行。说白了你手撸的干不过开发者写的。</p>
</li>
<li>
<p>均支持sparksql的操作，还能注册临时表，进行sql语句操作</p>
</li>
<li>
<p><code>DataFrame</code>和<code>Dataset</code>均可使用模式匹配获取各个字段的值和类型</p>
</li>
<li>
<p><code>DataFrame</code>也叫<code>Dataset[Row]</code>，每一行的类型是Row</p>
</li>
</ul>
<p>不同点</p>
<p>因为<code>DataFrame</code>也叫<code>Dataset[Row]</code>，所以我们理解了<strong>Row</strong>和<strong>普通对象</strong>的区别就好办了</p>
<ul>
<li>
<p><strong>Row</strong>的数据结构类似一个数组，只有顺序，切记。<strong>普通对象</strong>的数据结构也就是对象。</p>
</li>
<li>
<p>因此，访问<strong>Row</strong>只能通过如：<code>getInt(i: Int)</code>解析数据 或者 通过模式匹配得到数据；而<strong>普通对象</strong>可以通过 <code>.</code>号 直接访问对象中成员变量。</p>
</li>
<li>
<p>同理，<strong>Row</strong>中数据没类型，没办法在编译的时候检查是否有类型错误（弱类型的概念）；相反<strong>普通对象</strong>可以（强类型）。</p>
</li>
</ul>
<h1>RDD、DataFrame和DataSet转换</h1>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/dataframe2dataset.png" alt></p>
<p><strong>注意</strong></p>
<ul>
<li>
<p>原始RDD类型是 <code>RDD[(Int, String)]</code></p>
</li>
<li>
<p>DataFrame -&gt; RDD 时，变成了<code>RDD[Row]</code></p>
</li>
<li>
<p>DataSet -&gt; RDD时，变成了<code>RDD[User]</code></p>
</li>
</ul>
<h1>使用</h1>
<h2 id="SQL风格（主要）"><a class="header-anchor" href="#SQL风格（主要）">¶</a>SQL风格（主要）</h2>
<ul>
<li>创建一个DataFrame</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val df: DataFrame = spark.read.json(<span class="string">"people.json"</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>对DataFrame创建一个临时表(临时表是Session范围内有效，也可以创建全局的)</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>通过SQL语句实现对表的操作</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">spark.sql(<span class="string">"SELECT * FROM people"</span>).show()</span><br></pre></td></tr></table></figure>
<h2 id="DSL风格（次要）"><a class="header-anchor" href="#DSL风格（次要）">¶</a>DSL风格（次要）</h2>
<ul>
<li>创建一个DataFrame</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val df: DataFrame = spark.read.json(<span class="string">"people.json"</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>使用DataFrame的api</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">df.select(<span class="string">"name"</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">df.select($<span class="string">"name"</span>, $<span class="string">"age"</span> + <span class="number">1</span>).show()</span><br></pre></td></tr></table></figure>
<h1>小结</h1>
<p><code>DataFrame</code>和<code>Dataset</code>都是为了方便我们执行sql的，因此当我们把数据转化成它们之后，写好sql逻辑，剩下的就交给咱们spark吧！</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>sparksql</tag>
      </tags>
  </entry>
  <entry>
    <title>spark基础-窄、宽依赖和任务划分</title>
    <url>/2019/09/09/spark/spark%E5%9F%BA%E7%A1%80-%E7%AA%84%E3%80%81%E5%AE%BD%E4%BE%9D%E8%B5%96%E5%92%8C%E4%BB%BB%E5%8A%A1%E5%88%92%E5%88%86/</url>
    <content><![CDATA[<p>介绍了在spark中窄、宽依赖的划分以及任务划分</p>
<a id="more"></a>
<h1>窄依赖和宽依赖</h1>
<h2 id="窄依赖"><a class="header-anchor" href="#窄依赖">¶</a>窄依赖</h2>
<ul>
<li>每一个父RDD的Partition最多被子RDD的一个Partition使用</li>
<li>独生子女：一个爹RDD只有一个子</li>
</ul>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/dependence/Narrow-dependence.png" alt></p>
<h2 id="宽依赖"><a class="header-anchor" href="#宽依赖">¶</a>宽依赖</h2>
<ul>
<li>每一个父RDD的Partition被子RDD的多个Partition使用，伴随shuffle</li>
<li>超生：一个爹RDD有多个子</li>
</ul>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/dependence/Wide.jpg" alt></p>
<h2 id="个人理解"><a class="header-anchor" href="#个人理解">¶</a>个人理解</h2>
<p>由于还没学shuffle，所以从宏观简单思考。学一个东西不能死记硬背，最好的理解就是：问问自己<strong>为啥要分窄依赖和宽依赖？</strong></p>
<p>先分析例子：</p>
<ul>
<li>例1：用map时，一个分区里的数据经过函数，形成新的数据，大家你搞你的我搞我的，互不干扰。</li>
<li>例2：用合并操作时，多个分区合到一个分区，同样，各走各的，顶走跑之前计算下新偏移量（这个偏移别人没跑完我也知道），也可以说是互不干扰</li>
<li>例3：用groupbykey时，这下可不是互不干扰了，因为需要比较洗牌，你得等你的伙伴（另一个分区）算完了，才能执行groupbykey。</li>
</ul>
<p>因此我觉得这就是所谓的宽依赖：<strong>别的分区没跑完，不能执行下一步，需要等待</strong>。只有当大家都准备好了，才可以一起进行洗牌。由于分区里的数据顺序之前是乱的，所以shuffle时一般都会拆开，然后送到不同的子分区。这就造成了结果——超生。说实话，如果你从结果出发去思考，是不好区分例2例3的。</p>
<p>接着，划分窄依赖（<strong>别的分区没跑完，可以执行下一步</strong>）和宽依赖（<strong>别的分区没跑完，不可以执行下一步</strong>）的原因显而易见。我们可以把窄依赖的步骤划分到一起，它可以一路执行，不需要等待，直到宽依赖步骤卡住（必须等其它分区执行完）。这个从窄依赖一路执行到宽依赖的过程，可以在逻辑上划分成一个<strong>stage</strong>。这也就是常说的<strong>宽依赖是划分Stage的依据</strong>。</p>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/dependence/stage.jpg" alt></p>
<h1>任务划分</h1>
<p>RDD任务的切分，分为：Application、Job、Stage和Task，而且每一层都是<strong>1对n</strong>的关系</p>
<h2 id="4个名词"><a class="header-anchor" href="#4个名词">¶</a>4个名词</h2>
<ul>
<li><strong>Application</strong>：初始化一个SparkContext即生成一个Application</li>
<li><strong>Job</strong>：一个Action算子就会生成一个Job</li>
<li><strong>Stage</strong>：根据RDD之间的依赖关系的不同将Job划分成不同的Stage，遇到一个宽依赖则划分一个Stage。</li>
<li><strong>Task</strong>：Stage是一个TaskSet，将Stage划分的结果发送到不同的Executor执行即为一个Task。</li>
</ul>
<h2 id="个人理解-v2"><a class="header-anchor" href="#个人理解-v2">¶</a>个人理解</h2>
<p>同样思考为啥要划分这么多东西？</p>
<ul>
<li><strong>Application</strong></li>
</ul>
<p>一个spark不止跑一个程序吧，所以一个程序一个 Application理所当然，进而生成一个AppMaster管理它。</p>
<ul>
<li><strong>Job</strong></li>
</ul>
<p>一个程序有许多转换算子和行动算子。只有执行到<strong>行动操作才真正改变数据</strong>，所以把截止到行动算子的算子划一个job合情合理吧。而且我们从源码也可以看到，执行一个行动操作，就会执行<code>sc.runJob(...)</code></p>
<ul>
<li><strong>Stage</strong></li>
</ul>
<p>在一个Job中，有的可一路执行到宽依赖的，不需要等待，按这个划分为一个Stage。这个不理解的再看看上面的分析。</p>
<ul>
<li><strong>Task</strong></li>
</ul>
<p>在一个Stage中，我们观察最后一组分区，也就是shuffer前的，由于到这里都是可以一路执行的，所以按最后一组分区的个数，一个分区划一个Task。此时都划到分区了，自然不用划分了。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark基础-Yarn流程解析</title>
    <url>/2019/09/02/spark/spark%E5%9F%BA%E7%A1%80-Yarn%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>介绍了在spark中Yarn的工作流程和一些总结</p>
<a id="more"></a>
<h1>流程</h1>
<p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/spark-yarn.png" alt="YARN-Cluster流程图" title="YARN-Cluster流程图"></p>
<p>主要流程和<a href="https://zouxxyy.github.io/2019/08/31/hadoop-Yarn%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/#more" target="_blank" rel="noopener">Yarn的流程</a>一样，不同的就是紫色部分。这里采用的是spark的<strong>yarn-cluster</strong>模式，driver在APPMaster中。</p>
<h1>细节</h1>
<h2 id="解耦思想"><a class="header-anchor" href="#解耦思想">¶</a>解耦思想</h2>
<ul>
<li>
<p>ResourceManager管理资源调度，与NodeManager直接联系；Driver负责执行计算，与Executor也就是一个个Task直接联系。</p>
</li>
<li>
<p><strong>计算和资源调度解耦</strong>：ResourceManager和Driver靠中间件AppMaster联系起来；Executor和NodeManager靠中间件Container联系起来</p>
</li>
<li>
<p>此时计算框架是<strong>可插拔</strong>的，如：spark计算框架（紫色部分）代替mapreduce。</p>
</li>
</ul>
<h2 id="Client和Cluster模式"><a class="header-anchor" href="#Client和Cluster模式">¶</a>Client和Cluster模式</h2>
<p>spark上yarn有两种管理模式，<strong>YARN-Client</strong>和<strong>YARN-Cluster</strong>。</p>
<p>主要区别是：SparkContext初始化位置不同，也就是了Driver所在位置的不同。</p>
<table>
<thead>
<tr>
<th style="text-align:center">client</th>
<th style="text-align:center">master</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">driver在Client上</td>
<td style="text-align:center">driver在AppMaster上</td>
</tr>
<tr>
<td style="text-align:center">日志可以直接在Client上看到</td>
<td style="text-align:center">日志在某个节点上</td>
</tr>
<tr>
<td style="text-align:center">Client连接不能断开</td>
<td style="text-align:center">Client连接可以断开</td>
</tr>
<tr>
<td style="text-align:center">适合交互和调试</td>
<td style="text-align:center">适合生产环境</td>
</tr>
</tbody>
</table>
<h1>其它管理模式</h1>
<h2 id="local模式"><a class="header-anchor" href="#local模式">¶</a>local模式</h2>
<p>单机模式 <code>--master local[*] </code></p>
<h2 id="Standalone模式"><a class="header-anchor" href="#Standalone模式">¶</a>Standalone模式</h2>
<p>不用Yarn，用Spark自带的Standalone资源管理器，它把节点分成<strong>Master</strong>和<strong>Worker</strong>。类似RM和NM，但它没有AppMaster。也分为Client模式和cluster模式。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop-Yarn流程解析</title>
    <url>/2019/08/31/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop-Yarn%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>介绍了Yarn的工作流程和一些总结</p>
<a id="more"></a>
<h1>Yarn运行机制流程图</h1>
<p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/Yarn.png?raw=true" alt="Yarn运行机制流程图" title="Yarn运行机制流程图"></p>
<h1>具体步骤</h1>
<p>（1）作业提交</p>
<ol start="0">
<li>Client调用<code>job.waitForCompletion</code>方法，向整个集群提交MapReduce作业。</li>
<li>Client向RM申请一个作业id。</li>
<li>RM给Client返回该job资源的提交路径(<code>hdfs://.../.staging/</code>)和作业<code>application_id</code>。</li>
<li>Client在该路径提交jar包、切片信息和配置文件。</li>
<li>Client提交完资源后，向RM申请运行MrAppMaster。</li>
</ol>
<p>（2）作业初始化</p>
<ol start="5">
<li>当RM收到Client的请求后，将该job添加到资源调度器中，将job初始化成task。</li>
<li>某一个空闲的NM领取到该Job。</li>
<li>在该NM中创建Container，并产生MRAppmaster(一个job创建一个)，它管理该job。</li>
<li>下载之前Client提交的资源到本地。</li>
</ol>
<p>（3）任务分配</p>
<ol start="9">
<li>MRAppMaster向RM申请运行多个MapTask任务资源。</li>
<li>RM将运行MapTask任务分配给另外两个NodeManager，另外两个NodeManager分别领取任务并创建容器。</li>
</ol>
<p>（4）任务执行</p>
<ol start="11">
<li>MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</li>
<li>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</li>
<li>ReduceTask向MapTask获取相应分区的数据。</li>
<li>程序运行完毕后，MR会向RM申请注销自己。</li>
</ol>
<h1>一些细节</h1>
<h2 id="4大组件"><a class="header-anchor" href="#4大组件">¶</a>4大组件</h2>
<ul>
<li><strong>ResourceManager</strong>：总的老大：处理客户端请求，监控NodeManager，启动或监控ApplicationMaster，资源的分配与调度</li>
<li><strong>NodeManager</strong>：单个节点的老大：管理单个节点的资源，处理来自ResourceManager、ApplicationMaster的命令</li>
<li><strong>ApplicationMaster</strong>：单个job的老大：负责数据切分，为应用程序申请资源并分配内部的任务，任务的监控与容错</li>
<li><strong>Container</strong>：资源抽象：如内存、cpu、磁盘、网络等</li>
</ul>
<h2 id="3种资源调度器"><a class="header-anchor" href="#3种资源调度器">¶</a>3种资源调度器</h2>
<ul>
<li><strong>FIFO</strong>：先进先出</li>
<li><strong>Capacity Scheduler</strong>：多FIFO队列，会对同一用户提交资源进行限定，会把任务分配给更闲的队列。</li>
<li><strong>Fair Scheduler</strong>：多队列，按缺额排序，缺额大者优先执行</li>
</ul>
<h2 id="任务推测执行机制"><a class="header-anchor" href="#任务推测执行机制">¶</a>任务推测执行机制</h2>
<ul>
<li>问题：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成。</li>
<li>办法：为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。</li>
<li>前提：每个Task只能有一个备份任务，当前Job已完成的Task必须不小于0.05（5%）。</li>
<li>不适用：任务间存在严重的负载倾斜；特殊任务，比如任务向数据库中写数据。</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>yarn</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop-HDFS流程解析</title>
    <url>/2019/08/31/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop-HDFS%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>介绍了HDFS读写数据、NameNode和SecondaryNameNode、DataNode和NameNode的交互</p>
<a id="more"></a>
<h1>HDFS写数据</h1>
<h2 id="流程图"><a class="header-anchor" href="#流程图">¶</a>流程图</h2>
<p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/readHDFS.png?raw=true" alt="HDFS写数据" title="HDFS写数据"></p>
<h2 id="具体步骤"><a class="header-anchor" href="#具体步骤">¶</a>具体步骤</h2>
<ol>
<li>客户端调用DS模块向NameNode请求上传文件。</li>
<li>NameNode会检查目标文件和父目录是否已存在，再返回是否可以上传</li>
<li>假设文件为200M，客户端请求上传第一个 Block ，希望得到DataNode服务器位置。</li>
<li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3，用它们存储数据。</li>
<li>客户端通过FSDataOutputStream模块请求dn1建立上传数据通道，dn1收到请求会继续请求dn2，然后dn2请求dn3，直到将这个通信管道建立完成。</li>
<li>dn3、dn2、dn1逐级应答客户端。</li>
<li>客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。（类似队列，以Packet为单位）</li>
<li>当一个Block（0-128M）传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</li>
<li>向NameNode汇报上传完毕。</li>
</ol>
<h2 id="注意事项"><a class="header-anchor" href="#注意事项">¶</a>注意事项</h2>
<ul>
<li>DataNode位置选择，以默认3副本为例：第一个副本是最近的一般是它自己；第二个副本选择同一机架（同一路由）的不同节点；第三个副本是另一机架的随机节点。</li>
<li>数据传递以包为单位，第一个节点收到一个包，就把包传递给下一个DataNode。并不是等数据传完，再传递。</li>
</ul>
<h1>HDFS读数据</h1>
<h2 id="流程图-v2"><a class="header-anchor" href="#流程图-v2">¶</a>流程图</h2>
<p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/writeHDFS.png?raw=true" alt="HDFS读数据" title="HDFS读数据"></p>
<h2 id="具体步骤-v2"><a class="header-anchor" href="#具体步骤-v2">¶</a>具体步骤</h2>
<ol>
<li>客户端调用DS模块向NameNode请求下载文件。</li>
<li>NameNode会检查目标文件是否存在，再通过查询元数据，返回文件块所在的DataNode地址。</li>
<li>客户端通过FSDataInputStream模块向dn1（就近挑选）请求读取 Block1。</li>
<li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li>
<li>当一个Block（0-128M）传输完成之后，客户端再次请求下载Block2。（重复执行2-4步）。</li>
<li>向NameNode汇报下载完毕。</li>
</ol>
<h2 id="注意事项-v2"><a class="header-anchor" href="#注意事项-v2">¶</a>注意事项</h2>
<ul>
<li>如果块的第一个副本请求失败，会向第二个副本请求，依次类推。</li>
</ul>
<h1>NameNode和SecondaryNameNode</h1>
<h2 id="流程图-v3"><a class="header-anchor" href="#流程图-v3">¶</a>流程图</h2>
<p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/NNand2NN.png?raw=true" alt="NameNode和SecondaryNameNode" title="NameNode和SecondaryNameNode"></p>
<h2 id="具体步骤-v3"><a class="header-anchor" href="#具体步骤-v3">¶</a>具体步骤</h2>
<p>第一阶段：NameNode</p>
<ol>
<li>第一次启动NameNode（格式化）后，会创建Fsimage（镜像文件）和Edits（编辑日志）文件。以后启动，会直接加载镜像文件和编辑日志到内存，此时会进行合并操作。</li>
<li>假设此时客户端提出了增删改的请求。</li>
<li>NameNode记录之前的编辑日志（edits_n），更新新日志到滚动日志（edits_inprogress_n）中。</li>
<li>日志记录完毕后，NameNode在内存中对数据进行增删改。</li>
</ol>
<p>第二阶段：SecondaryNameNode</p>
<ol>
<li>Secondary NameNode向NameNode询问是否需要CheckPoint。</li>
<li>如果需要，Secondary NameNode请求执行CheckPoint。</li>
<li>NameNode滚动日志。</li>
<li>将滚动前的编辑日志（edits_001）和镜像文件(fsimage)拷贝到Secondary NameNode。</li>
<li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</li>
<li>合并后，生成新的镜像文件fsimage.chkpoint。</li>
<li>拷贝fsimage.chkpoint到NameNode。</li>
<li>NameNode将fsimage.chkpoint重新命名成fsimage。</li>
</ol>
<h2 id="注意事项-v3"><a class="header-anchor" href="#注意事项-v3">¶</a>注意事项</h2>
<ul>
<li>Fsimage 和 Edits文件？</li>
</ul>
<p>fsimage是NameNode内存中元数据序列化后形成的文件。Edits中记录客户端更新元数据信息的每一步操作。每次执行增删改时，先改日志再改文件。好处是：如果保证中途gg，可以保证操作不丢失，便于复原。</p>
<ul>
<li>为啥要Secondary NameNode？</li>
</ul>
<p>首先要知道只有NameNode重启时，edit.log才会合并到fsimage文件中，所以运行时间久了就会有3个问题：edis.log文件会变的很大；NameNode下次重启会花费很长时间；fsimage文件文件很旧，如果中途挂掉就很睿智。</p>
<p>为了解决上述问题，SecondaryNameNode诞生，每隔一定时间辅助合并NameNode的edit.log到fsimage文件中。从上述流程图就可以发现，它做的就是这个。</p>
<ul>
<li>什么时候执行CheckPoint？</li>
</ul>
<p>（1） 用户定时 （2）edit.log 满了</p>
<ul>
<li>Secondary NameNode是热备份吗？</li>
</ul>
<p>不是，可以发现Secondary NameNode合并的是滚动前的edis，它总是比NameNode的编辑日志少一点。</p>
<h1>DataNode和NameNode</h1>
<h2 id="流程图-v4"><a class="header-anchor" href="#流程图-v4">¶</a>流程图</h2>
<p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/DataNode.png?raw=true" alt="NameNode和DataNode" title="NameNode和DataNode"></p>
<h2 id="具体步骤-v4"><a class="header-anchor" href="#具体步骤-v4">¶</a>具体步骤</h2>
<ol>
<li>DataNode启动后向NameNode注册。</li>
<li>NameNode告知注册成功。</li>
<li>DataNode周期性（1小时）的向NameNode上报所有的块信息。</li>
<li>DataNode每3秒发送一次心跳，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。</li>
<li>超过10分钟没有收到心跳，表示该节点不可用。</li>
</ol>
<h2 id="注意事项-v4"><a class="header-anchor" href="#注意事项-v4">¶</a>注意事项</h2>
<ul>
<li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度、校验和 以及时间戳。</li>
<li>节点增加：新节点配置好后，自动向NameNode注册的。</li>
<li>节点退役：NameNode可以通过白名单指定需要的节点；通过黑名单指定不要的节点。</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop-MapReduce流程解析</title>
    <url>/2019/08/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop-MapReduce%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>介绍了MapReduce的详细流程和一些总结</p>
<a id="more"></a>
<h1>流程图</h1>
<p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/MapTask.png?raw=true" alt="MapTask流程图" title="MapTask流程图"></p>
<p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/ReduceTask.png?raw=true" alt="ReduceTask流程图" title="ReduceTask流程图"></p>
<p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/Shuffer.png?raw=true" alt="Shuffer流程图" title="Shuffer流程图"></p>
<h1>具体步骤</h1>
<p>逻辑上可以这样划分：1-10是MapTask ；11-16是ReduceTask；7-14是shuffer</p>
<h3 id="1-待处理文本"><a class="header-anchor" href="#1-待处理文本">¶</a><strong>1. 待处理文本</strong></h3>
<p>这里假设是<code>/user/input</code>目录下的<code>ss.txt</code>	文件，大小为<strong>200M</strong>。</p>
<h3 id="2-客户端submit（）"><a class="header-anchor" href="#2-客户端submit（）">¶</a><strong>2. 客户端submit（）</strong></h3>
<p>发生在client端，主要获取3个信息：</p>
<p>（1）<strong>Job.split</strong> ：找到文件ss.txt，根据切片算法，得到切片的元数据信息（起始位置，长度以及所在节点等）如把ss.txt分成两片 0-128M 和 128M-200M</p>
<p>（2）<strong>Job.xml</strong>：任务的配置信息</p>
<p>（3）<strong>wc.jar</strong>：任务的jar包</p>
<p>（可以在<code>/tmp/hadoop-zxy/mapred/staging/zxy1248702679/.staging/</code>下找到它们）</p>
<h3 id="3-提交信息"><a class="header-anchor" href="#3-提交信息">¶</a><strong>3. 提交信息</strong></h3>
<p>将刚刚获取的任务规划信息，提交到资源管理器上，我们这里用Yarn。</p>
<h3 id="4-RM计算MapTask数量"><a class="header-anchor" href="#4-RM计算MapTask数量">¶</a><strong>4. RM计算MapTask数量</strong></h3>
<p>接着向Yarn的RM申请资源，RM根据任务规划信息用户Job分成Task，并把任务下发给节点。这里我们数据分成了2片，根据默认规则，会有2个MapTask各自处理一片数据。</p>
<h3 id="5-根据采用的InputFormat读取数据"><a class="header-anchor" href="#5-根据采用的InputFormat读取数据">¶</a><strong>5. 根据采用的InputFormat读取数据</strong></h3>
<p>这里采用默认的TextInputFormat类，按行读取每条记录。key是行偏移量，value是该行的内容。</p>
<h3 id="6-执行Mapper的map"><a class="header-anchor" href="#6-执行Mapper的map">¶</a><strong>6. 执行Mapper的map()</strong></h3>
<p>根据用户的代码执行map逻辑，把结果写入Context中。</p>
<h3 id="7-向环形缓存区写入数据"><a class="header-anchor" href="#7-向环形缓存区写入数据">¶</a><strong>7. 向环形缓存区写入数据</strong></h3>
<p>环形缓存区取一点：一边写索引，一边写真实数据。达到80%时发生溢写</p>
<h3 id="8-分区、排序"><a class="header-anchor" href="#8-分区、排序">¶</a><strong>8. 分区、排序</strong></h3>
<p>一种2次排序，先按区号排，再对key排序（快排）。得到一组按区排好序的数据。注意：这步是在环形缓存区就可以执行的，且排序排的是索引，真实数据不用动。且此时可以使用第一次Combiner合并操作。</p>
<h3 id="9-溢出写入文件"><a class="header-anchor" href="#9-溢出写入文件">¶</a><strong>9. 溢出写入文件</strong></h3>
<p>环形缓存区达到80%时，溢写到磁盘上。注意写磁盘前已经完成了分区、排序、合并、压缩等操作。此时生成第一组溢写文件<code>spillN.out</code> 与元数据<code>spillN.out.index</code>。</p>
<h3 id="10-MapTask的归并排序"><a class="header-anchor" href="#10-MapTask的归并排序">¶</a><strong>10. MapTask的归并排序</strong></h3>
<p>将多组溢写文件，以分区为单位进行归并排序，写入磁盘形成大文件<code>output/file.out</code>，与索引文件<code>output/file.out.index</code>。此时一个MapTask任务完成，得到一个分区有序的数据。注意：在归并排序时可以使用第二次Combiner合并操作。</p>
<h3 id="11-启动ReduceTask"><a class="header-anchor" href="#11-启动ReduceTask">¶</a><strong>11. 启动ReduceTask</strong></h3>
<p>假设分区数为2，此时启动2个ReduceTask，一个ReduceTask处理一个区的数据。</p>
<h3 id="12-copy数据"><a class="header-anchor" href="#12-copy数据">¶</a><strong>12. copy数据</strong></h3>
<p>ReduceTask从各个MapTask上拷贝它要处理的区的数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<h3 id="13-ReduceTask的归并排序"><a class="header-anchor" href="#13-ReduceTask的归并排序">¶</a><strong>13. ReduceTask的归并排序</strong></h3>
<p>把同区的数据复制到同一个ReduceTask后，对它们进行归并排序</p>
<h3 id="14-分组"><a class="header-anchor" href="#14-分组">¶</a><strong>14. 分组</strong></h3>
<p>默认把key相同的数据分到一组。用户可以继承WritableComparator，自定义分组规则。</p>
<h3 id="15-执行Reducer的Reduce"><a class="header-anchor" href="#15-执行Reducer的Reduce">¶</a><strong>15. 执行Reducer的Reduce()</strong></h3>
<p>根据用户的代码执行reduce逻辑，把结果写入Context中。注意：一次读一组，value是迭代器对象，包含一个组的全部数据。</p>
<h3 id="16-根据采用的OutputFormat读取数据"><a class="header-anchor" href="#16-根据采用的OutputFormat读取数据">¶</a><strong>16. 根据采用的OutputFormat读取数据</strong></h3>
<p>这里采用默认的TextOutputFormat类，按行写入key和value，key和value用tab分开。</p>
<h1>一些总结</h1>
<h3 id="1个逻辑"><a class="header-anchor" href="#1个逻辑">¶</a><strong>1个逻辑</strong></h3>
<p><strong>先分区 -&gt; 再排序 -&gt; 再分组</strong></p>
<p>分区：用户定义分区数后，默认按hash分区。用户也可以继承<code>Partitioner</code>，自定义分区规则。ReduceTask的个数一般等于分区数。</p>
<p>排序：默认对key排序，key必须实现<code>WritableComparable</code>接口。用户可以重写<code>WritableComparable</code>接口的<code>compareTo()</code>方法，定义自己的排序规则。</p>
<p>分组：默认把key相同的数据分到一组。用户也可以继承WritableComparator，自定义分组规则。用于reduce阶段，一次读取一组.</p>
<h3 id="2次合并"><a class="header-anchor" href="#2次合并">¶</a><strong>2次合并</strong></h3>
<p>Combiner的父类就是Reducer，它可以通过对Map阶段的局部结果进行汇总，减少输出。</p>
<p>时机： 2次，<strong>分区排序后、MapTask的归并排序时</strong>。</p>
<p>条件：不能影响业务逻辑 且 输入输出的范型一致</p>
<h3 id="3次排序"><a class="header-anchor" href="#3次排序">¶</a><strong>3次排序</strong></h3>
<p>MapTask：</p>
<p><strong>分区排序</strong>：在缓行缓冲区进行，是一种2次排序。先按分区号排序，再对key排序（快排）。</p>
<p><strong>归并排序</strong>：对每组溢写的数据，进行的按区，归并排序。</p>
<p>ReduceTask：</p>
<p><strong>归并排序</strong>：对从MapTask拷贝的同区数据，进行的归并排序。</p>
<h3 id="分片和分区"><a class="header-anchor" href="#分片和分区">¶</a><strong>分片和分区</strong></h3>
<p>分片：<strong>分片数决定MapTask的个数</strong>。在客户端即完成，举FileInputFormat切片机制为例：简单的按文件长度进行切片，切片大小等于块大小（默认128M），切片时是对文件单独切片。</p>
<p>分区：<strong>分区数决定ReduceTask的个数</strong>。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop-单节点伪分布式搭建</title>
    <url>/2019/08/24/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop-%E5%8D%95%E8%8A%82%E7%82%B9%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<p>在mac上搭建hadoop伪分布式</p>
<a id="more"></a>
<p>操作系统： macOS Mojave 10.14.5</p>
<p>JDK : 1.8</p>
<p>hadoop: 2.7.7</p>
<h2 id="1-Java和Hadoop安装"><a class="header-anchor" href="#1-Java和Hadoop安装">¶</a>1. Java和Hadoop安装</h2>
<p>下载和安装相信都没问题</p>
<p>注意的就是：</p>
<ul>
<li>环境变量设置好，我是mac所以javahome是$(/usr/libexec/java_home);我是zsh所以修改.zshrc，修改完别忘了source。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME=$(/usr/libexec/java_home)</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop-2.7.7</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>
<ul>
<li>由于我将hadoop放在<code>/usr/local/</code>目录下，所以需要更改hadoop文件夹权限</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chown -R zxy:admin /usr/local/hadoop-2.7.7</span><br></pre></td></tr></table></figure>
<h2 id="2-配置SSH"><a class="header-anchor" href="#2-配置SSH">¶</a>2. 配置SSH</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -P <span class="string">""</span></span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">ssh localhost</span><br></pre></td></tr></table></figure>
<h2 id="3-伪分布式配置"><a class="header-anchor" href="#3-伪分布式配置">¶</a>3. 伪分布式配置</h2>
<h3 id="core-site-xml"><a class="header-anchor" href="#core-site-xml">¶</a>core-site.xml</h3>
<p>修改 <code>/usr/local/hadoop-2.7.7/etc/hadoop/core-site.xml</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/local/hadoop-2.7.7/data/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p><strong>fs.defaultFS</strong>  HDFS 的NameNode地址</p>
</li>
<li>
<p><strong>hadoop.tmp.dir</strong>  hadoop 临时文件地址，自己指定</p>
</li>
</ul>
<h3 id="hdfs-site-xml"><a class="header-anchor" href="#hdfs-site-xml">¶</a>hdfs-site.xml</h3>
<p>修改 <code>/usr/local/hadoop-2.7.7/etc/hadoop/hdfs-site.xml</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>dfs.replication</strong>  HDFS文件存储的副本个数，默认3。因为我们这只有一个节点，所以设置1.（单一节点至多存一份节点）</li>
</ul>
<h3 id="yarn-site-xml"><a class="header-anchor" href="#yarn-site-xml">¶</a>yarn-site.xml</h3>
<p>修改 <code>/usr/local/hadoop-2.7.7/etc/hadoop/yarn-site.xml</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;localhost&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 开启聚合日志 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;http://localhost:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>arn.log-aggregation-enable</strong> 开启日志聚合</li>
<li><strong>yarn.resourcemanager.hostname</strong>  yarn的ResourceManager地址</li>
</ul>
<h3 id="mapred-site-xml"><a class="header-anchor" href="#mapred-site-xml">¶</a>mapred-site.xml</h3>
<p>修改 <code>/usr/local/hadoop-2.7.7/etc/hadoop/mapred-site.xml</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;localhost:10020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;localhost:19888&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><a href="http://mapreduce.framework.name" target="_blank" rel="noopener">mapreduce.framework.name</a></strong>  采用yarn管理MR</li>
<li><strong>mapreduce.jobhistory.address</strong>  历史服务器端口地址</li>
<li><strong>mapreduce.jobhistory.webapp.address</strong>  历史服务器web端地址</li>
</ul>
<h3 id="检查JAVA-HOME"><a class="header-anchor" href="#检查JAVA-HOME">¶</a>检查JAVA_HOME</h3>
<p><a href="http://hadoop-env.sh" target="_blank" rel="noopener">hadoop-env.sh</a>、<a href="http://mapred-env.sh" target="_blank" rel="noopener">mapred-env.sh</a>、<a href="http://yarn-env.sh" target="_blank" rel="noopener">yarn-env.sh</a>，在这三个文件检查是否添加JAVA_HOME路径，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=$JAVA_HOME</span><br></pre></td></tr></table></figure>
<h2 id="4-使用"><a class="header-anchor" href="#4-使用">¶</a>4. 使用</h2>
<ul>
<li>开HDFS</li>
</ul>
<p>第一次使用要格式化(仅限第一次使用时，以后要格式化需删除log、data目录下的文件)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure>
<p>开启namenode、datanode</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>
<ul>
<li>开yarn</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line">yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure>
<ul>
<li>开historyserver</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>
<ul>
<li>可以用jps查看效果</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jps</span><br><span class="line">35953 JobHistoryServer</span><br><span class="line">32930</span><br><span class="line">35682 NodeManager</span><br><span class="line">35990 Jps</span><br><span class="line">35559 DataNode</span><br><span class="line">35624 ResourceManager</span><br><span class="line">35502 NameNode</span><br></pre></td></tr></table></figure>
<ul>
<li>测试</li>
</ul>
<p>创建一个文件夹zxytest，里面随便放一个文件，上传到hdfs测试wordcount</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -put zxytest /</span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /zxytest /zxyout</span><br></pre></td></tr></table></figure>
<ul>
<li>关闭</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mr-jobhistory-daemon.sh stop historyserver</span><br><span class="line">yarn-daemon.sh stop resourcemanager</span><br><span class="line">yarn-daemon.sh stop nodemanager</span><br><span class="line">hadoop-daemon.sh stop namenode</span><br><span class="line">hadoop-daemon.sh stop datanode</span><br></pre></td></tr></table></figure>
<ul>
<li>可视化地址</li>
</ul>
<p>所有任务: <a href="http://localhost:8088/" target="_blank" rel="noopener">http://localhost:8088/</a></p>
<p>DataNode: <a href="http://localhost:50070/" target="_blank" rel="noopener">http://localhost:50070/</a></p>
<p>历史服务器: <a href="http://localhost:19888/" target="_blank" rel="noopener">http://localhost:19888/</a></p>
<h2 id="更新"><a class="header-anchor" href="#更新">¶</a>更新</h2>
<p>由于新版可以在UI界面直接处理文件夹，所以我把版本升到3.2.1～</p>
<p>但是在网页界面访问数据使用的用户名，默认值是一个不真实存在的用户（dr.who），此用户权限很小，不能访问不同用户的数据，这保证了数据安全。为了方便，我选择修改 <code>core-site.xml</code>，改为自己的用户名，这样就能直接增删改了～</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;zxy&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>配置</tag>
      </tags>
  </entry>
  <entry>
    <title>3种Linux命令后台执行方法：&amp;、nohup、tmux</title>
    <url>/2019/06/26/linux/3%E7%A7%8DLinux%E5%91%BD%E4%BB%A4%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8C%E6%96%B9%E6%B3%95%EF%BC%9A-%E3%80%81nohup%E3%80%81tmux/</url>
    <content><![CDATA[<p>让命令在后台跑起来！</p>
<a id="more"></a>
<h1>&amp;</h1>
<p>用法：<code>指令 &amp;</code></p>
<p>说明： 将指令放入后台执行，会将输出打印到前台，<strong>当执行该指令的终端gg时，它也gg</strong></p>
<p>终止方法：</p>
<ul>
<li><code>jobs</code> 查看它 -&gt; <code>fg %num</code> 取出它 -&gt; <code>Ctrl+c</code>终止它</li>
<li>直接退出终端</li>
</ul>
<h1>nohup</h1>
<p>用法：<code>nohup 指令 &amp;</code></p>
<p>说明： 将指令放入后台执行，不会将输出打印到前台，<strong>当执行该指令的终端gg时，它不gg</strong></p>
<p>终止方法：</p>
<ul>
<li>未退出终端时：<code>jobs</code> 查看它 -&gt; <code>fg %num</code> 取出它 -&gt; <code>Ctrl+c</code>终止它</li>
<li>退出终端时：在新终端连接中，找到PID号，kill它</li>
</ul>
<h1>tmux</h1>
<p>简介：</p>
<p>tmux可以在后台新建一个终端，并且用户退出后创建的终端仍然存在</p>
<p>用法：</p>
<ul>
<li>创建session</li>
</ul>
<p><code>tmux new -s $session_name</code></p>
<ul>
<li>列出session</li>
</ul>
<p><code>tmux ls</code></p>
<ul>
<li>临时退出session</li>
</ul>
<p><code>Ctrl+b d</code> (按完 Ctrl+b 松开手)</p>
<ul>
<li>进入已存在的session</li>
</ul>
<p><code>tmux a -t $session_name</code></p>
<ul>
<li>删除指定session</li>
</ul>
<p><code>tmux kill-session -t $session_name</code></p>
<h1>总结</h1>
<ul>
<li>
<p>&amp;  简单，安全，退出终端，程序自动结束</p>
</li>
<li>
<p>nohup 退出终端后，必须通过pid号杀程序</p>
</li>
<li>
<p>tmux 谁用谁知道，一般情况，时间长的程序，我都用它</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>用jupyter notebook打开服务器（告别黑框框)</title>
    <url>/2019/04/11/linux/%E7%94%A8jupyter-notebook%E6%89%93%E5%BC%80%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%88%E5%91%8A%E5%88%AB%E9%BB%91%E6%A1%86%E6%A1%86/</url>
    <content><![CDATA[<p>几步完成服务器内jupyter notebook的使用</p>
<a id="more"></a>
<h1>为啥要这样</h1>
<p>可以远程用浏览器看服务器的文件目录，以及各种jupyter的好处：直接改代码，调试，它内嵌的读图也是特别舒服。</p>
<h1>服务器安装jupyter notebook</h1>
<p>我是给服务器装了<a href="https://zouxxyy.github.io/2019/04/10/linux%E4%B8%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%AE%89%E8%A3%85anaconda3-pytorch-fastai/#more" target="_blank" rel="noopener">anaconda</a>，然后就自带了jupyter notebook。</p>
<h1>配置jupyter notebook</h1>
<ol>
<li>登陆服务器</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh username@address_of_remote</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>生成配置文件</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ jupyter notebook --generate-config</span><br></pre></td></tr></table></figure>
<p>该指令会自动创建这个：<code>~/.jupyter/jupyter_notebook_config.py </code></p>
<ol start="3">
<li>生成密码</li>
</ol>
<p>打开python，创建密码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</span><br><span class="line">passwd()</span><br><span class="line">Enter password:</span><br><span class="line">Verify password:</span><br><span class="line"><span class="string">'sha1:xxxxxxxxxxxxxxxxxxxxxxx'</span></span><br></pre></td></tr></table></figure>
<p>把密码<code>'sha1:xxxxxxxxxxxxxxxxxxxxxxx'</code>复制下来</p>
<ol start="4">
<li>修改配置文件</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim ~/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure>
<p>加上下面这几行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c.NotebookApp.allow_remote_access = <span class="literal">True</span></span><br><span class="line">c.NotebookApp.ip = <span class="string">'*'</span></span><br><span class="line">c.NotebookApp.password = <span class="string">u'sha1:xxxxxxxxxxxxxxxxxxxx'</span> <span class="comment">#你的密码</span></span><br><span class="line">c.NotebookApp.open_browser = <span class="literal">False</span></span><br><span class="line">c.NotebookApp.port = <span class="number">8888</span></span><br></pre></td></tr></table></figure>
<ol start="5">
<li>启动jupyter</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ jupyter notebook</span><br></pre></td></tr></table></figure>
<h1>远程访问</h1>
<ul>
<li>
<p>方法一：</p>
<p>本地浏览器访问<code>http://address_of_remote:8888</code> （此方法可能由于防火墙问题出现失败）</p>
</li>
<li>
<p>方法二：</p>
<ol>
<li>在本地终端中输入:</li>
</ol>
<p><code>ssh username@address_of_remote -L127.0.0.1:1234:127.0.0.1:8888 </code></p>
<ol start="2">
<li>本地浏览器访问<code>http://localhost:1234</code></li>
</ol>
</li>
</ul>
<p>看看效果，虚浮了。相逢恨晚，造化弄人啊～</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/4/11/16a0a4fa59021e90?w=1293&amp;h=200&amp;f=png&amp;s=25969" alt></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title>linux下命令行安装anaconda3+pytorch+fastai</title>
    <url>/2019/04/10/linux/linux%E4%B8%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%AE%89%E8%A3%85anaconda3-pytorch-fastai/</url>
    <content><![CDATA[<p>几步完成 anaconda3+pytorch+fastai 的安装</p>
<a id="more"></a>
<h1>为啥用anaconda</h1>
<p>anaconda 确实很好用，切换python版本也方便。而且如果服务器用的人多，用anaconda搞个自己的环境很舒服。</p>
<h1>anaconda3安装</h1>
<ol>
<li><strong><a href="https://www.anaconda.com/distribution/" target="_blank" rel="noopener">官网</a>选择要下载的版本</strong>。我用的是python3.7做base（如果后面想用python2.7直接加个环境就行）</li>
</ol>
<p><img src="https://user-gold-cdn.xitu.io/2019/4/10/16a0537dfe227c8d?w=1122&amp;h=485&amp;f=png&amp;s=62781" alt></p>
<p>把下载地址拷贝下来，用<code>wget</code>下载</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><strong>安装</strong></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bash Anaconda3-2019.03-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<p>这个过程需要</p>
<ol>
<li>
<p>按回车阅读license，并同意</p>
</li>
<li>
<p>选择安装地址，我就是按默认的<code>/home/zxy/anaconda3</code></p>
</li>
<li>
<p>是否 conda init ，这步同意了会自动在<code>/home/zxy/.bashrc</code>改你的环境变量。</p>
</li>
<li>
<p><strong>激活环境变量的修改</strong></p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>激活下修改，就安装完毕了，然后可以rm掉安装包。</p>
<ol start="4">
<li><strong>一些细节</strong></li>
</ol>
<p>同意了<code>conda init</code>后，会<strong>自动</strong>在<code>/home/zxy/.bashrc</code>加下面这些东西。可以用<code>cat ~/.bashrc</code>自己看看</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span></span><br><span class="line"><span class="comment"># !! Contents within this block are managed by 'conda init' !!</span></span><br><span class="line">__conda_setup=<span class="string">"<span class="variable">$('/home/zxy/anaconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)</span>"</span></span><br><span class="line"><span class="keyword">if</span> [ $? -eq 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">eval</span> <span class="string">"<span class="variable">$__conda_setup</span>"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">"/home/zxy/anaconda3/etc/profile.d/conda.sh"</span> ]; <span class="keyword">then</span></span><br><span class="line">        . <span class="string">"/home/zxy/anaconda3/etc/profile.d/conda.sh"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">export</span> PATH=<span class="string">"/home/zxy/anaconda3/bin:<span class="variable">$PATH</span>"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">unset</span> __conda_setup</span><br><span class="line"><span class="comment"># &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</span></span><br></pre></td></tr></table></figure>
<p>有了它呢，我们直接<code>python</code>测试看看</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(base) [zxy@gpu-server5 ~]$ python</span><br><span class="line">Python 3.7.3 (default, Mar 27 2019, 22:11:17) </span><br><span class="line">[GCC 7.3.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> or <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br></pre></td></tr></table></figure>
<p>OK! 可以用了！</p>
<h1>pytorch安装</h1>
<p><strong><a href="https://pytorch.org/" target="_blank" rel="noopener">官网</a>选择下载的版本</strong>。这里要根据自己的cuda版本选择</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/4/10/16a0535a5d05c267?w=825&amp;h=316&amp;f=png&amp;s=40869" alt></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit=9.0 -c pytorch</span><br></pre></td></tr></table></figure>
<p>因为有了<code>anaconda</code>，安装<code>pytorch</code>一句话完事。</p>
<p>(ps：国内兄弟，可以先添加<a href="https://mirror.tuna.tsinghua.edu.cn/help/anaconda/" target="_blank" rel="noopener">清华镜像</a>，再把后面的-c pytorch去掉。速度飞起～)</p>
<h1>fastai安装</h1>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install -c fastai fastai</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM(8)-虚拟机字节码执行引擎</title>
    <url>/2019/03/29/java/JVM-8-%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E/</url>
    <content><![CDATA[<p>本文是《深入理解Java虚拟机（第二版）》第八章的笔记</p>
<a id="more"></a>
<h1>运行时栈帧结构</h1>
<p>在活动线程中，只有栈顶的栈时有效的，称为<strong>当前栈帧</strong>，与这个栈帧相关联的方法称为<strong>当前方法</strong>。下面对栈帧的4个主要部分进行分析。</p>
<h2 id="局部变量表"><a class="header-anchor" href="#局部变量表">¶</a>局部变量表</h2>
<p>存放方法参数和方法内部定义的局部变量</p>
<p>一些细节：</p>
<ul>
<li>最小存储单元(Slot)，一个Slot可以存32位以内的数据类型。</li>
<li><code>boolean、byte、char、short、int、float、reference、returnAddress</code>占一个Slot；<code>long、double</code>占两个Slot，是非原子的，但它是<strong>线程安全</strong>的，因为它是栈中的，是线程私有的。</li>
<li>对于实例方法，第一个Slot是传递所属对象实例的引用，也就是我们常用的   <code>this</code>。</li>
<li>注意局部变量<strong>没有初始值</strong>哦。</li>
<li>当一个变量的pc寄存器的值大于Slot的作用域时，Slot是可以<strong>复用</strong>的。</li>
</ul>
<h2 id="操作数栈"><a class="header-anchor" href="#操作数栈">¶</a>操作数栈</h2>
<ul>
<li>虚拟机字节码执行引擎是“<strong>基于栈的执行引擎</strong>”，这个栈就是操作数栈。</li>
<li>对比<strong>基于寄存器</strong>的执行引擎</li>
</ul>
<p>优点：可移植性、代码更加紧凑、编译器实现更紧凑。确定就是速度更慢。</p>
<ul>
<li>下面栈帧的部分操作数栈与上面的栈帧的部分局部变量是重叠的。</li>
</ul>
<h2 id="动态连接"><a class="header-anchor" href="#动态连接">¶</a>动态连接</h2>
<p>指向运行时常量池中该栈帧所属方法的引用，这个引用的为了支持方法调用过程的动态连接。具体内容在下面的方法调用中解释。</p>
<h2 id="方法返回地址"><a class="header-anchor" href="#方法返回地址">¶</a>方法返回地址</h2>
<p>方法退出（也就是<strong>当前栈帧出栈</strong>）的两种方式：</p>
<ul>
<li><code>return</code>正常退出</li>
<li>异常退出，根据异常表得返回出口</li>
</ul>
<h1>方法调用</h1>
<p>方法调用不等同于方法的执行，方法调用阶段唯一的任务就是确定被调用方法的版本。说白了就是找方法，方法唯一就直接确定（<strong>解析</strong>）。方法不唯一：<strong>重载（静态分配）、重写（动态分配）</strong></p>
<h2 id="解析调用"><a class="header-anchor" href="#解析调用">¶</a>解析调用</h2>
<ul>
<li>
<p>调用目标在程序代码写好、编译器进行编译时就确定好的。这类方法时调用称为解析。是静态的。</p>
</li>
<li>
<p><strong>非虚方法</strong>：静态方法、私有方法、实例构造器、父类方法、final方法。它们都是采用解析调用。反之其它就是虚方法。</p>
</li>
</ul>
<h2 id="分派调用"><a class="header-anchor" href="#分派调用">¶</a>分派调用</h2>
<p><code>Human man = new Man();</code></p>
<p><code>Human</code>是静态类型（外观类型），<code>Man</code>是实际类型</p>
<h3 id="静态分派"><a class="header-anchor" href="#静态分派">¶</a>静态分派</h3>
<p>依赖静态类型来定位方法执行的版本的分配动作称为静态分配。最典型的应用是方法重载。</p>
<ul>
<li>编译器在<strong>重载</strong>时是根据参数的<strong>静态类型</strong>作为依据的。</li>
<li>由于字面量没有显式的静态类型，它重载时可能会有多种选择，只是选一个更好的版本。</li>
<li>静态分配和解析不是互斥的，例如静态方法也是可以重载的。</li>
<li>注意：静态分配更严格，<strong>一定是要用静态类型做参数</strong>。</li>
</ul>
<h3 id="动态分派"><a class="header-anchor" href="#动态分派">¶</a>动态分派</h3>
<p>依赖实际类型来定位方法执行的版本的分配动作称为动态分配。最典型的应用是方法重写。</p>
<ul>
<li>虚拟器在<strong>重写</strong>时是对象的<strong>实际类型</strong>作为依据的。</li>
<li>注意：动态分配更宽松，如果实际类型中没有对应的方法，就会向上找父类里的相同方法来调用。</li>
</ul>
<p>一个测试例子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MixTest</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Human</span></span>&#123; &#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Man</span> <span class="keyword">extends</span> <span class="title">Human</span></span>&#123;&#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Woman</span> <span class="keyword">extends</span> <span class="title">Human</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Father</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">choice</span><span class="params">(Human arg)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"father choose human"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">choice</span><span class="params">(Man arg)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"father choose man"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">choice</span><span class="params">(Woman arg)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"father choose woman"</span>); <span class="comment">// 和同一类里的同名方法是重载关系</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Son</span> <span class="keyword">extends</span> <span class="title">Father</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">choice</span><span class="params">(Human arg)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"son choose human"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">choice</span><span class="params">(Man arg)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"son choose man"</span>); <span class="comment">// 和父类的同名同参数方法是重写关系</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">choice</span><span class="params">(Woman arg)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"son choose woman"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Human woman = <span class="keyword">new</span> Woman();</span><br><span class="line">        Man man = <span class="keyword">new</span> Man();</span><br><span class="line">        Father father = <span class="keyword">new</span> Father();</span><br><span class="line">        Father son = <span class="keyword">new</span> Son();</span><br><span class="line">        father.choice(woman); <span class="comment">// 重写：对象类型选Father(实际类型) 重载：参数类型选 Human（静态类型）</span></span><br><span class="line">        son.choice(man); <span class="comment">// 重写：对象类型选Son(实际类型) 重载：参数类型选 Man（静态类型）</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 程序输出：</span></span><br><span class="line"><span class="comment">father choose human</span></span><br><span class="line"><span class="comment">son choose man</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM(7)-虚拟机类加载机制</title>
    <url>/2019/03/29/java/JVM-7-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<p>本文是《深入理解Java虚拟机（第二版）》第七章的笔记</p>
<a id="more"></a>
<h1>类加载的时机</h1>
<p>类加载后要初始化。所以可以通过判断啥时候要<strong>初始化</strong>，得出类加载的时机。</p>
<h2 id="有且只有5种情况需要对类进行初始化"><a class="header-anchor" href="#有且只有5种情况需要对类进行初始化">¶</a><strong>有且只有5种情况</strong>需要对类进行初始化</h2>
<ol>
<li><code>new</code>实例化对象、读取或设置静态字段（被<strong>final修饰放入常量池</strong>时除外）、调用类的静态方法 （且类没有初始化）。</li>
<li>使用<code>java.lang.reflect</code>包的方法对类进行反射调用，且类没有初始化</li>
<li>初始化一个类时，如果父类没有初始化，会触发父类的初始化</li>
<li>含main方法的类会优先初始化</li>
<li>当使用JDK1.7的动态语言支持时，如果一个<code>java.lang.invoke.MethodHandle</code>实例最后的解析结果<code>REF_getStatic、REF_pubStatic、REF_invokeStatic</code>的方法句柄，并且这个句柄对应的类没有初始化，会触发其初始化。（黑人问号。。）</li>
</ol>
<h2 id="被动引用的例子"><a class="header-anchor" href="#被动引用的例子">¶</a>被动引用的例子</h2>
<ul>
<li>用子类引用父类的静态字段<code>SubClass.value</code>(value在父类中)，只会初始化父类。</li>
<li>通过数组来定义引用类，不会触发次类的初始化</li>
<li><code>final</code>修饰的静态常量会存入常量池，引用它不会触发初始化(可以与上面的第一种情况比较)</li>
</ul>
<h2 id="与接口初始化的比较"><a class="header-anchor" href="#与接口初始化的比较">¶</a>与接口初始化的比较</h2>
<ul>
<li>接口不能使用static{}语句块，但有<code>&lt;clinit&gt;()</code>类构造器</li>
<li>与第三条不同：初始化一个接口时，如果父类接口没有初始化，父类接口不会初始化。</li>
</ul>
<h1>类加载的过程</h1>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/28/169c226c3ad50652?w=1339&amp;h=609&amp;f=png&amp;s=39162" alt></p>
<h2 id="加载"><a class="header-anchor" href="#加载">¶</a>加载</h2>
<p>在加载期间虚拟机完成以下三件事：</p>
<ul>
<li>通过类的全限定名获取定义此类的二进制字节流</li>
<li>通过这个二进制流代表的静态存储结构转化为方法区的运行时数据</li>
<li>在内存中生成一个代表这个类的<code>java.lang.Class</code>对象，作为方法区这个类的各种数据的访问入口（HotSpot虚拟机中Class对象是 在方法区中）</li>
</ul>
<h2 id="验证"><a class="header-anchor" href="#验证">¶</a>验证</h2>
<ul>
<li>文件格式验证</li>
<li>元数据验证</li>
<li>字节码验证</li>
<li>符号引用验证</li>
</ul>
<h2 id="准备"><a class="header-anchor" href="#准备">¶</a>准备</h2>
<p>为类的变量（<strong>static修饰</strong>）分配内存并设置变量的初始值。这些变量所用的内存都在方法区中分配。</p>
<p>注意：这里的初始值是其<strong>默认值</strong>。如果是<code>static final</code>修饰，就初始化为指定值，存在常量区。</p>
<h2 id="解析"><a class="header-anchor" href="#解析">¶</a>解析</h2>
<p>将虚拟机中的<strong>符号引用</strong>替换为<strong>直接引用</strong></p>
<p>主要的解析动作</p>
<ul>
<li>类或者接口的解析</li>
<li>字段解析</li>
<li>类方法的解析</li>
<li>接口方法的解析</li>
</ul>
<h2 id="初始化"><a class="header-anchor" href="#初始化">¶</a>初始化</h2>
<p>执行<code>&lt;clinit&gt;()</code>方法的过程。和<code>&lt;init&gt;()</code>比较下：</p>
<ul>
<li><code>&lt;clinit&gt;()</code>：类的初始化，类变量的赋值动作和静态语句块合并一起。</li>
<li><code>&lt;init&gt;()</code>：类的实例化，也就是类的构造方法。初始化实例用的。</li>
</ul>
<p>一些细节：</p>
<ul>
<li>定义在静态语句块后面的变量，静态语句块可以赋值但不能访问。如果访问会报错“<strong>非法前向引用</strong>”。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class Clinit &#123;</span><br><span class="line">    static &#123;</span><br><span class="line">        i = 0;</span><br><span class="line">        // System.out.println(i); // 非法前向引用</span><br><span class="line">    &#125;</span><br><span class="line">    static int i = 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>优先执行父类的<code>&lt;clinit&gt;()</code>方法</li>
<li>接口的父类<code>&lt;clinit&gt;()</code>方法不需要先执行，并且接口的实现类在初始化时也不会执行接口的<code>&lt;clinit&gt;()</code>方法。</li>
<li>如果多个线程同时初始化一个类，只有一个线程会执行类的<code>&lt;clinit&gt;()</code>方法。</li>
</ul>
<h1>类加载器</h1>
<p>类加载器干的事：通过类的全限定名获取定义此类的二进制字节流。只有被同一个类加载器所加载的类才有可能相等。</p>
<p>优势：<strong>类层次划分</strong>、OSGi、热部署、<strong>代码加密</strong> 等等。</p>
<p>类加载器分类</p>
<ul>
<li>启动类加载器：c++实现，是虚拟机等一部分，加载<code>&lt;JAVA_HOME&gt;/lib</code>目录下的类</li>
<li>扩展类加载器：加载<code>&lt;JAVA_HOME&gt;/lib/ext</code>目录下的类</li>
<li>应用程序类加载器：加载用户路径上指定的类库</li>
<li>自定义类加载器：用户自定义的类加载器</li>
</ul>
<h2 id="双亲委派模型"><a class="header-anchor" href="#双亲委派模型">¶</a>双亲委派模型</h2>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/28/169c226f3fdffe17?w=468&amp;h=359&amp;f=png&amp;s=122552" alt></p>
<ul>
<li>要求：除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。</li>
<li>过程：就是把加载的活<strong>推拖</strong>给父类加载器，一直推到最顶层。然后让最顶层加载器加载，如果它干不了，再递给子，直到推给发起者，如果它也干不了，就发出 <code>ClassNotFoundException</code>异常。</li>
<li>优点：使类有类层次性。如在双亲委派模型下，Object类是由最顶层的启动类加载器加载，所以它在每种加载器中都是同一个类，使Object这一最基础的类的性能得以保证。</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM(4)-虚拟机性能监控与故障处理工具</title>
    <url>/2019/03/25/java/JVM-4-%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<p>本文是《深入理解Java虚拟机（第二版）》第四章的笔记</p>
<a id="more"></a>
<p>我的JDK工具地址</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zxysMac:~ elwg$ <span class="built_in">cd</span> /Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/bin</span><br><span class="line">zxysMac:bin elwg$ ls</span><br><span class="line">appletviewer	javah		jjs		jvisualvm	schemagen</span><br><span class="line">extcheck	javap		jmap		keytool		serialver</span><br><span class="line">idlj		javapackager	jmc		native2ascii	servertool</span><br><span class="line">jar		jcmd		jps		orbd		tnameserv</span><br><span class="line">jarsigner	jconsole	jrunscript	pack200		unpack200</span><br><span class="line">java		jdb		jsadebugd	policytool	wsgen</span><br><span class="line">javac		jdeps		jstack		rmic		wsimport</span><br><span class="line">javadoc		jhat		jstat		rmid		xjc</span><br><span class="line">javafxpackager	jinfo		jstatd		rmiregistry</span><br></pre></td></tr></table></figure>
<h1>JDK的命令行工具</h1>
<table>
<thead>
<tr>
<th>名称</th>
<th>全名</th>
<th>主要功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>jps</td>
<td>JVM Process Status</td>
<td>显示指定系统的所有的虚拟机进程</td>
</tr>
<tr>
<td>jstat</td>
<td>JVM Statistics Monitoring Tool</td>
<td>用于收集虚拟机各方面的运行数据</td>
</tr>
<tr>
<td>jinfo</td>
<td>Configuration Info for Java</td>
<td>显示虚拟机配置信息</td>
</tr>
<tr>
<td>jmap</td>
<td>Memory Map for Java</td>
<td>生成虚拟机的内存转储快照（heapdump文件）</td>
</tr>
<tr>
<td>jhat</td>
<td>JVM Heap Dump Browser</td>
<td>用于分析heapdump文件</td>
</tr>
<tr>
<td>jstack</td>
<td>Stack Trace for Java</td>
<td>显示虚拟机的线程快照</td>
</tr>
</tbody>
</table>
<h2 id="jps：虚拟机进程状况工具"><a class="header-anchor" href="#jps：虚拟机进程状况工具">¶</a>jps：虚拟机进程状况工具</h2>
<ul>
<li>
<p>功能：可以列出运行的虚拟机进程、看到虚拟机执行的主类、本地虚拟机唯一ID</p>
</li>
<li>
<p>命令格式：<br>
<code>jps [ option ] [ hostid ]</code></p>
</li>
<li>
<p>主要选项：</p>
</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/23/169a9a09b0013234?w=918&amp;h=205&amp;f=png&amp;s=96305" alt></p>
<ul>
<li>例子：我在IDEA中运行列一个java文件，测试如下：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(base) zxysMac:~ elwg$ jps -l</span><br><span class="line">16387 org.jetbrains.idea.maven.server.RemoteMavenServer</span><br><span class="line">44422 sun.tools.jps.Jps</span><br><span class="line">44392 com.zouxxyy.jvm.chap4.JpsTest</span><br><span class="line">31368 org.jetbrains.kotlin.daemon.KotlinCompileDaemon</span><br><span class="line">16217</span><br><span class="line">44393 org.jetbrains.jps.cmdline.Launcher</span><br></pre></td></tr></table></figure>
<h2 id="jstat：虚拟机统计信息监视工具"><a class="header-anchor" href="#jstat：虚拟机统计信息监视工具">¶</a>jstat：虚拟机统计信息监视工具</h2>
<ul>
<li>功能：可以显示本地或远程虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据</li>
<li>命令格式：<code>jstat [ option vmid [interval[s|ms] [count]] ]</code></li>
<li>主要选项：</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/23/169a9aa3cfd87699?w=923&amp;h=459&amp;f=png&amp;s=275718" alt></p>
<h2 id="jinfo：Java配置信息工具"><a class="header-anchor" href="#jinfo：Java配置信息工具">¶</a>jinfo：Java配置信息工具</h2>
<ul>
<li>
<p>功能：实时查看和调整虚拟机各项参数</p>
</li>
<li>
<p>命令格式：<code>jinfo [ option ] pid</code></p>
</li>
<li>
<p>例子：看有没有用SerialGC</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(base) zxysMac:~ elwg$ jinfo -flag UseSerialGC 46521</span><br><span class="line">-XX:-UseSerialGC</span><br></pre></td></tr></table></figure>
<h2 id="jmap：Java内存映像工具"><a class="header-anchor" href="#jmap：Java内存映像工具">¶</a>jmap：Java内存映像工具</h2>
<ul>
<li>功能：生成虚拟机堆存转储快照（heapdump文件）</li>
<li>命令格式：<code>jmap [ option] vmid</code></li>
<li>主要选项：</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/23/169a9b5409980aff?w=933&amp;h=403&amp;f=png&amp;s=202391" alt></p>
<h2 id="jhat：虚拟机堆快照分析工具"><a class="header-anchor" href="#jhat：虚拟机堆快照分析工具">¶</a>jhat：虚拟机堆快照分析工具</h2>
<p>与jmap配合使用，也就是分析堆存转储快照（heapdump文件）。</p>
<h2 id="jstack：Java堆栈跟踪工具"><a class="header-anchor" href="#jstack：Java堆栈跟踪工具">¶</a>jstack：Java堆栈跟踪工具</h2>
<ul>
<li>功能：用于生成虚拟机的线程快照</li>
<li>命令格式：<code>jstack [ option ] vmid</code></li>
<li>主要选项：</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/23/169a9b95790bdcd5?w=920&amp;h=188&amp;f=png&amp;s=81541" alt></p>
<h1>JDK的可视化工具</h1>
<h2 id="JConsole：Java监视与管理控制台"><a class="header-anchor" href="#JConsole：Java监视与管理控制台">¶</a>JConsole：Java监视与管理控制台</h2>
<p><strong>内存监控，相当于可视化的<code>jstat</code>命令</strong></p>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/25/169b291ef7f36929?w=858&amp;h=628&amp;f=png&amp;s=106536" alt><br>
<strong>线程监控，相当于可视化的<code>jstack</code>命令</strong>，以下举几个简单的例子</p>
<ul>
<li>监听用户键盘输入</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/25/169b2a74ba142acd?w=811&amp;h=134&amp;f=png&amp;s=29000" alt></p>
<ul>
<li>监听wait()</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/25/169b2a7c8556a816?w=819&amp;h=206&amp;f=png&amp;s=45681" alt></p>
<ul>
<li>监听死锁</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/25/169b2bdcdb14d52d?w=844&amp;h=306&amp;f=png&amp;s=35573" alt></p>
<h2 id="VisualVM：多合一故障处理工具"><a class="header-anchor" href="#VisualVM：多合一故障处理工具">¶</a>VisualVM：多合一故障处理工具</h2>
<p>书上说它很强大，我还没下载成功。它几乎集成里上面的所有功能啊。</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/25/169b2e570ebb1cc5?w=1239&amp;h=665&amp;f=png&amp;s=126621" alt></p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM(3)-垃圾收集器与内存分配策略</title>
    <url>/2019/03/25/java/JVM-3-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/</url>
    <content><![CDATA[<p>本文是《深入理解Java虚拟机（第二版）》第三章的笔记</p>
<a id="more"></a>
<h1>如何判断对象为垃圾对象</h1>
<h2 id="引用计数法"><a class="header-anchor" href="#引用计数法">¶</a>引用计数法</h2>
<p>给对象添加一个引用计数器，每当有个对象引用它时，计数器加1；引用失效时计数器减1；为0时对象不可能被再引用。</p>
<p>没有使用的原因：很难解决<strong>对象之间相互循环引用</strong>的问题</p>
<h2 id="可达性分析算法"><a class="header-anchor" href="#可达性分析算法">¶</a>可达性分析算法</h2>
<p>看作为GC Roots的对象作为起始点，和<strong>它连通的是可用的，反之不可用</strong>。</p>
<p>如下情况的对象可以作为GC Roots：</p>
<ul>
<li>虚拟机栈(栈桢中的本地变量表)、本地方法栈中JNI（Native方法）中的引用的对象</li>
<li>方法区中的类静态属性引用的对象</li>
<li>方法区中的常量引用的对象</li>
</ul>
<p>当对象不可达时，会执行<code>fianlize()</code>，但该方法自会被自动调用一次，有点像c++的析构函数，书上说最好别用。</p>
<h2 id="引用的分类"><a class="header-anchor" href="#引用的分类">¶</a>引用的分类</h2>
<ul>
<li>强引用：<code>Objece obj = new Object()</code>，永远不会被垃圾搜集回收</li>
<li>软引用：<code>SoftReference</code>类实现，可以存活第一次回收，但第二次就GG</li>
<li>弱引用：<code>WeakReference</code>类实现，第一次就GG</li>
<li>虚引用：<code>PhantomReference</code>类实现，<strong>不能通过它取得实例</strong>，唯一作用就是当该对象被回收时会收到一个系统通知。</li>
</ul>
<h1>如何回收</h1>
<h2 id="垃圾收集算法"><a class="header-anchor" href="#垃圾收集算法">¶</a>垃圾收集算法</h2>
<ul>
<li>标记-清除算法：（老年代）先标记，再清除。缺点：效率低，内存碎片化</li>
<li>复制算法：（新生代）HotSpot虚拟机把内存分为Eden和2块Survivor区，把在Eden和Survivor中 存活的对象 复制到空闲的一块Survivor中。不够时可以向老年代担保。</li>
<li>标记-整理算法：（老年代）标记 -&gt; 移动到一起 -&gt; 清理</li>
<li>分代收集算法：把内存分为新生代和老年代。根据年代选则合适的算法。</li>
</ul>
<h2 id="垃圾收集器"><a class="header-anchor" href="#垃圾收集器">¶</a>垃圾收集器</h2>
<table>
<thead>
<tr>
<th>新生代</th>
<th>老年代</th>
<th>特殊</th>
</tr>
</thead>
<tbody>
<tr>
<td>Serial（单线程）</td>
<td>Serial Old（单线程，CMS备胎）</td>
<td>G1（Region优先级回收）</td>
</tr>
<tr>
<td>ParNew（多线程）</td>
<td>CMS（并发）</td>
<td></td>
</tr>
<tr>
<td>Parallel Scavenge（关注吞吐量）</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>并行（Parallel）：多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态。</li>
<li>并发（Concurrent）：用户线程与垃圾收集线程同时执行。</li>
</ul>
<h1>如何分配</h1>
<p>以HosSpot收集器的分配和回收策略为例</p>
<ul>
<li>优先分配在Eden</li>
<li><strong>大对象直接进入老年代</strong></li>
<li>长期存活的对象进入老年代</li>
<li>动态对象年龄判定</li>
<li>空间分配担保</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title>IntelliJ IDEA 实用快捷键</title>
    <url>/2019/03/22/%E5%B7%A5%E5%85%B7/IntelliJ-IDEA-%E5%AE%9E%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/</url>
    <content><![CDATA[<p>IntelliJ IDEA 的小技巧 （ mac 版）</p>
<a id="more"></a>
<h1>码代码</h1>
<ol>
<li><strong>缩写功能</strong></li>
</ol>
<p>一般取首字母即可</p>
<p>如 <code>public static void main(String[] args){}</code>可以用 <code>psvm</code>出来</p>
<ol start="2">
<li><strong>加 surround，如 if、catch</strong></li>
</ol>
<p>选中目标代码段 <code>command + option + t</code> 再选择 surround 类型</p>
<ol start="3">
<li><strong>格式化代码</strong></li>
</ol>
<p>选中目标代码段 <code>command + option + l</code></p>
<ol start="3">
<li><strong>删除一行</strong></li>
</ol>
<p><code>command + d</code></p>
<h1>调试</h1>
<ol>
<li><strong>跳转到上（下）一次位置</strong></li>
</ol>
<p>上一次 <code>command + option + ← </code></p>
<p>下一次 <code>command + option + → </code></p>
<ol start="2">
<li><strong>查看类继承关系</strong></li>
</ol>
<p>选中类名 <code>control + h</code></p>
<ol start="3">
<li><strong>查看类或者方法的调用情况</strong></li>
</ol>
<p>选中类或者方法名 <code>control + option + h</code></p>
<p>或者 <code>command + 左键</code></p>
<ol start="4">
<li><strong>全局查找文件</strong></li>
</ol>
<p><code>shift + shift</code></p>
<ol start="5">
<li><strong>全局查找内容</strong></li>
</ol>
<p><code>command + shift + f</code></p>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM(2)-Java内存区域与内存溢出异常</title>
    <url>/2019/03/22/java/JVM-2-Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E4%B8%8E%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E5%BC%82%E5%B8%B8/</url>
    <content><![CDATA[<p>本文是《深入理解Java虚拟机（第二版）》第二章的笔记</p>
<a id="more"></a>
<h1>运行时数据区域</h1>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/21/1699e42932e70b62?w=652&amp;h=625&amp;f=png&amp;s=45434" alt><br>
我们这节关注中间灰色部分，注意绿色部分<strong>线程共享</strong>，黄色部分<strong>线程私有</strong></p>
<h2 id="程序计数器"><a class="header-anchor" href="#程序计数器">¶</a>程序计数器</h2>
<p>一块较小的内存，是<strong>当前线程</strong>（线程私有）执行的字节码的<strong>行号指示器</strong>。</p>
<ul>
<li>如果执行的是Java方法，计数器记录的是正在执行的虚拟机字节码指令的地址；如果是Native（本地方法），计数器值为空(Undefined)</li>
<li>唯一没有<code>OutOfMemoryError</code>的区域</li>
</ul>
<h2 id="Java虚拟机栈"><a class="header-anchor" href="#Java虚拟机栈">¶</a>Java虚拟机栈</h2>
<p>虚拟机栈是描述Java方法执行的内存模型</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/21/1699f1c41f3b3ba5?w=551&amp;h=419&amp;f=png&amp;s=23533" alt><br>
每个方法调用至完成，对应一个栈帧在虚拟机栈中入栈到出栈的过程。</p>
<p>局部变量表：</p>
<ul>
<li>存放<strong>基本数据类型</strong>、<strong>对象引用</strong>、<strong>returnAddress</strong></li>
<li>局部变量表所需的内存空间在<strong>编译期</strong>完成，该空间是确定的，方法运行期间<strong>不改变</strong>。</li>
</ul>
<p>两种异常：</p>
<ul>
<li><code>StackOverflowError</code>: 请求的栈深度超过允许范围</li>
<li><code>OutOfMemoryError</code>: 需要的内存超过允许范围</li>
</ul>
<h2 id="本地方法栈"><a class="header-anchor" href="#本地方法栈">¶</a>本地方法栈</h2>
<p>本地方法栈是描述<strong>本地方法</strong>执行的内存模型。它和Java虚拟机栈很类似，区别就是它是为本地方法服务的。</p>
<h2 id="Java堆"><a class="header-anchor" href="#Java堆">¶</a>Java堆</h2>
<p>存放对象示例，也称作（GC堆），是垃圾回收器管理的主要区域。</p>
<ul>
<li>细分 新生代、老生代；Eden空间、From Survivor空间、To Survivor空间等</li>
<li>线程共享的堆里可能划分出私有的分配缓冲区（TLAB）。</li>
</ul>
<h2 id="方法区"><a class="header-anchor" href="#方法区">¶</a>方法区</h2>
<p>存储<strong>已被虚拟机加载的类信息</strong>、常量、静态变量、即时编译后的代码等数据。</p>
<p><strong>运行时常量池</strong> ：<br>
存放编译器生成的各种字面常量和符号引用。</p>
<p>例子：String类在常量池的数据结构类似于HashSet,是唯一的。</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/21/169a02588d5caac4?w=612&amp;h=354&amp;f=png&amp;s=27965" alt></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s1 = <span class="string">"abc"</span>;</span><br><span class="line">String s2 = <span class="string">"abc"</span>; <span class="comment">// 存放在方法区的常量池（字节码常量）唯一</span></span><br><span class="line"></span><br><span class="line">System.out.println(s1 == s2); <span class="comment">// true</span></span><br><span class="line"></span><br><span class="line">String s3 = <span class="keyword">new</span> String(<span class="string">"abc"</span>); <span class="comment">// 用了new，所以在堆中创建</span></span><br><span class="line"></span><br><span class="line">System.out.println(s1 == s3); <span class="comment">// false</span></span><br><span class="line"></span><br><span class="line">System.out.println(s1 == s3.intern()); <span class="comment">// true</span></span><br></pre></td></tr></table></figure>
<p>注意哦jdk1.8 String常量池搬到了堆中。如果想对<code>intern()</code>了解更多，可以看我的这篇<a href="https://zouxxyy.github.io/2019/03/22/JDK1-8-String%E5%B8%B8%E9%87%8F%E6%B1%A0%E8%AF%A6%E8%A7%A3/#more" target="_blank" rel="noopener">博客</a>。</p>
<h1>HotSpot对象揭秘</h1>
<h2 id="对象创建"><a class="header-anchor" href="#对象创建">¶</a>对象创建</h2>
<p>对象创建简单描述：</p>
<p>new -&gt; 根据常量池中符号看是否需要加载 -&gt; 类加载 -&gt; 分配内存 -&gt; 初始化 -&gt;构造方法</p>
<p>给对象分配内存的两种方法：</p>
<ul>
<li>指针碰撞</li>
<li>空闲列表</li>
</ul>
<p>解决创建对象时线程安全问题两种方法：</p>
<ul>
<li>分配内存动作进行同步处理</li>
<li>TLAB（共享堆中按线程划分的线程私有部分）上分配，它分配完，再同步锁定</li>
</ul>
<h2 id="对象的内存布局"><a class="header-anchor" href="#对象的内存布局">¶</a>对象的内存布局</h2>
<p>3块区域：</p>
<ul>
<li>
<p>对象头</p>
<ul>
<li>自身运行时的数据（MarkWord）：HashCode、GC分代年龄、锁状态标志、线程持有锁、偏向线程ID、偏向时间戳等</li>
<li>类型指针：对象指向它的类元数据的指针，虚拟机可以根据它来确定该对象是哪个类的实例。</li>
</ul>
<p>注意如果对象是数组，那么对象头中还有一块用于记录数组长度的数据。</p>
</li>
<li>
<p>实例数据</p>
</li>
<li>
<p>对齐填充 ：当实例部分没对齐时，通过对齐填充来补全</p>
</li>
</ul>
<h2 id="对象的访问定位"><a class="header-anchor" href="#对象的访问定位">¶</a>对象的访问定位</h2>
<ul>
<li>句柄访问（2级指针）</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/21/169a038a164fdc67?w=604&amp;h=404&amp;f=png&amp;s=33178" alt><br>
可以看出来要到实例数据必须再经过一个指针，这么做到好处是，当对象移动时，我们只需要改变局柄池里的指针，不需要改变reference；缺点是速度慢。</p>
<ul>
<li>直接指针</li>
</ul>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/21/169a038c26fd8733?w=602&amp;h=408&amp;f=png&amp;s=28283" alt><br>
直接指针顾名思义直接指向实例数据，好处是速度快；缺点就是对象移动要改reference。</p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title>JDK1.8 String常量池详解</title>
    <url>/2019/03/22/java/JDK1-8-String%E5%B8%B8%E9%87%8F%E6%B1%A0%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>本文是 JDK1.8 String常量池详解，带图哦</p>
<a id="more"></a>
<p><strong>jdk 1.8</strong></p>
<h1>先抛结论</h1>
<h2 id="1-只在常量池上创建常量"><a class="header-anchor" href="#1-只在常量池上创建常量">¶</a>1.只在常量池上创建常量</h2>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/22/169a32568b4c1f09?w=531&amp;h=366&amp;f=png&amp;s=15995" alt></p>
<h2 id="2-只在堆上创建对象"><a class="header-anchor" href="#2-只在堆上创建对象">¶</a>2.只在堆上创建对象</h2>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/22/169a32d01c0c56b4?w=577&amp;h=389&amp;f=png&amp;s=19131" alt></p>
<h2 id="3-在堆上创建对象，在常量池上创建常量"><a class="header-anchor" href="#3-在堆上创建对象，在常量池上创建常量">¶</a>3.在堆上创建对象，在常量池上创建常量</h2>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/22/169a32fd14b320a8?w=553&amp;h=362&amp;f=png&amp;s=18505" alt></p>
<h2 id="4-在堆上创建对象，在常量池上创建引用"><a class="header-anchor" href="#4-在堆上创建对象，在常量池上创建引用">¶</a>4.在堆上创建对象，在常量池上创建引用</h2>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/22/169a331a8ed3ecb4?w=561&amp;h=362&amp;f=png&amp;s=20883" alt></p>
<p>注意：</p>
<ul>
<li>
<p>常量池有两种情况：引用（指针） 或 常量。<strong>如果该位置已经是引用或常量了，之后的操作都不会改变里面的情况！！！</strong></p>
</li>
<li>
<p>调用<code>intern()</code>（jdk1.8）: <strong>如果常量池里是空的，就创建引用（指向堆，参考结论4）；非空，不操作。返回值都是常量池里的内容。</strong></p>
</li>
<li>
<p>堆中可以有任意个相同的字符串，常量池只能有一个（引用 或 常量）。</p>
</li>
<li>
<p>&quot; &quot; 和intern() 其实很像。区别就是在常量池为空时，“ ”是把值加进去，intern()是把引用加进去。</p>
</li>
</ul>
<h1>根据结论解决例子</h1>
<h2 id="例一"><a class="header-anchor" href="#例一">¶</a>例一</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s1 = <span class="keyword">new</span> String(<span class="string">"zxy"</span>);    <span class="comment">// 结论3</span></span><br><span class="line">s1.intern(); <span class="comment">// 常量池非空，返回值是常量池里的内容</span></span><br><span class="line">String s2 = <span class="string">"zxy"</span>; <span class="comment">// 常量池非空，返回值是常量池里的内容</span></span><br><span class="line">System.out.println(s1 == s2); <span class="comment">//false</span></span><br><span class="line">System.out.println(s1.intern() == s2); <span class="comment">// true</span></span><br></pre></td></tr></table></figure>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/22/169a351b50cc98d6?w=569&amp;h=355&amp;f=png&amp;s=18514" alt></p>
<h2 id="例二"><a class="header-anchor" href="#例二">¶</a>例二</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s1 = <span class="string">"zxy"</span>; <span class="comment">// 加到常量池</span></span><br><span class="line">String s2 = <span class="keyword">new</span> String(<span class="string">"zxy"</span>); <span class="comment">// 加到堆，常量池有东西所以啥也不干</span></span><br><span class="line">System.out.println(s1 == s2); <span class="comment">// false</span></span><br><span class="line">System.out.println(s1 == s2.intern()); <span class="comment">// true 常量池非空，intern返回常量池里的内容</span></span><br></pre></td></tr></table></figure>
<p><img src="https://user-gold-cdn.xitu.io/2019/3/22/169a35d23404be5b?w=468&amp;h=373&amp;f=png&amp;s=16487" alt></p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title>java多线程编程chap5-7</title>
    <url>/2019/03/14/java/java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8Bchap5-7/</url>
    <content><![CDATA[<p>本文是《java多线程编程核心技术》第五、六、七章的学习笔记。</p>
<a id="more"></a>
<h1>定时器 Timer</h1>
<p>Timer类主要负责计划任务的功能</p>
<p>一些方法：</p>
<ul>
<li><code>schedule(TimerTask task, Date time)</code> 在指定的日期执行某一次任务。</li>
<li><code>schedule(TimerTask task, long delay)</code>	以执行此方法的当前时间为参考时间，在此时间基础上延迟指定的毫秒数后执行一次TimerTask任务</li>
<li><code>schedule(TimerTask task, long delay, long period)</code>	以执行此方法的当前时间为参考时间，在此时间基础上延迟指定的毫秒数，再以某一间隔时间无限次数地执行某一TimerTask任务</li>
<li><code>scheduleAtFixedRate(TimerTask task, Date firstTime, long period)</code>	在指定的日期之后，按指定的间隔周期，无限循环的执行某一任务</li>
</ul>
<p><code>schedule</code>和<code>scheduleAtFixedRate</code>的异同：</p>
<p>同：</p>
<ul>
<li>多次调用<code>schedule</code>或<code>scheduleAtFixedRate</code>执行多个任务的话，则都是以队列的方式一个一个的被顺序执行。所以不用考虑非线程安全的问题。</li>
<li>如果任务延时了，那么下一次任务的执行时间都是上一次任务结束的时间（都带<code>period</code>参数时）</li>
</ul>
<p>异：</p>
<ul>
<li>不延时时，<code>schedule</code>下一次任务的执行时间是上一次任务开始加上<code>period</code>的时间，<code>scheduleAtFixedRate</code>下一次任务的执行时间是上一次任务结束的时间。</li>
</ul>
<h1>单例模式与多线程</h1>
<p>单例模式是23个设计模式中的一种，本章将它与多线程结合考虑</p>
<ul>
<li>立即加载/“饿汉模式”：调用方法前，实例已经被创建了。</li>
<li>延迟加载/“懒汉模式”：调用get()方法时实例才被创建。最常见的实现办法是在get()方法中进行new实例化。可是在多线程环境中会出问题，即创建多个实例。</li>
</ul>
<p>解决方案</p>
<ul>
<li>声明synchronized关键字，但运行效率非常低</li>
<li>同步代码块，但效率也非常低</li>
<li>针对某些重要代码单独的同步，效率提升，但是会创建多个实例</li>
<li>使用DCL双检查锁</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyObject</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> MyObject myObject = <span class="keyword">new</span> MyObject();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">MyObject</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MyObject <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (myObject != <span class="keyword">null</span>) &#123;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">                <span class="keyword">synchronized</span> (MyObject.class) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (myObject == <span class="keyword">null</span>) myObject = <span class="keyword">new</span> MyObject();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> myObject;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>使用静态内置类</li>
<li>使用static代码块</li>
<li>使用enum枚举数据类型</li>
</ul>
<h1>拾遗增补</h1>
<p>线程状态：<br>
<img src="https://user-gold-cdn.xitu.io/2019/3/14/16979f1bac8b4b94?w=843&amp;h=634&amp;f=png&amp;s=182243" alt="方法与线程状态的转换图" title="方法与线程状态的转换图"><br>
线程组：</p>
<ul>
<li>线程组中可以有线程对象，也可以有线程组，组中还可以有线程。作用是可批量管理线程或线程组对象，有效地对线程或线程组对象进行组织。</li>
<li>实例化一个线程组时如果不指定所属的线程组，它会自动归属到当前线程对象所属的线程组中。</li>
</ul>
<p>线程中出现异常的处理：</p>
<ul>
<li><code>setUncaughtExceptionHandler()</code>给指定线程对线设置异常处理器</li>
<li><code>setDefaultUncaughtExceptionHandler()</code>对指定线程类的所有线程对象设置异常处理器</li>
</ul>
<br>
<br>
<p><font size="6"><center><strong>完结撒花！</strong></center></font></p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>java多线程编程chap3-4</title>
    <url>/2019/03/13/java/java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8Bchap3-4/</url>
    <content><![CDATA[<p>本文是《java多线程编程核心技术》第三、四章的学习笔记。</p>
<a id="more"></a>
<h1>线程间通信</h1>
<h2 id="等待-通知机制"><a class="header-anchor" href="#等待-通知机制">¶</a>等待/通知机制</h2>
<p><code>wait()</code>和<code>notify()</code>: <code>wait</code>使线程停止运行，<code>notify</code>使停止的线程继续运行。</p>
<p>注意事项：</p>
<ul>
<li><code>wait()</code>自动锁放对象锁；<code>notify()</code>不释放；它们必须存在与同步块中。</li>
<li>wait 后线程会进入等待池，需要由同一对象的 notify 方法唤醒。</li>
<li>当线程处于wait状态时，调用<code>interrup()</code>方法会出现异常。</li>
<li>notify()随机唤醒线程，<code>notifyAll()</code>唤醒全部wait线程.</li>
<li><code>wait(n)</code>，使线程等待一段时间，如果没被唤醒，超过时间自动唤醒。</li>
</ul>
<p>可以用<code>wait()</code>和<code>notify()</code>实现生产者和消费者</p>
<ul>
<li>假死：所有线程都呈WAITING状态。</li>
<li>原因：notify唤醒的是异类，如“生产者”唤醒“生产者”。</li>
<li>解决：将<code>notify()</code>改为<code>notifyAll()</code></li>
</ul>
<p>通过管道进行线程间通信：一个线程发送数据到输出管道，另一个线程从输入管道中读数据。</p>
<ul>
<li>字节流：<code>PipedInputStream</code>和<code>PipedOutputStream</code></li>
<li>字符流：<code>PipedReader</code>和<code>PipedWriter</code></li>
</ul>
<h2 id="方法join的使用"><a class="header-anchor" href="#方法join的使用">¶</a>方法join的使用</h2>
<p><code>join()</code>：使当前线程堵塞，等待线程对象销毁后，再执行当前线程后面的代码。</p>
<ul>
<li><code>join()</code>与<code>interrupt()</code>方法如果彼此遇到，则会出现异常。</li>
<li><code>join(long)</code>可设定等待的时间。注意：<code>join(long)</code>内部使用<code>wait(long)</code>实现，所以<code>join(long)</code>会<strong>释放锁</strong>；而<code>Thread.sleep(long)</code><strong>不会释放锁</strong>。</li>
</ul>
<h2 id="ThreadLocal的使用"><a class="header-anchor" href="#ThreadLocal的使用">¶</a>ThreadLocal的使用</h2>
<ul>
<li><code>ThreadLocal</code>类：让每个线程可以绑定自己的值</li>
<li>解决<code>get()</code>返回null的问题：覆盖	<code>initialValue()</code>方法使变量具有初始值。</li>
<li><code>InheritableThreadLocal</code>类可在子线程中取得父线程继承下来的值。</li>
<li>还可以对子线程继承下来的值进行修改。</li>
</ul>
<h1>Lock的使用</h1>
<h2 id="使用ReentrantLock类"><a class="header-anchor" href="#使用ReentrantLock类">¶</a>使用ReentrantLock类</h2>
<p>基本用法：</p>
<ul>
<li>同步： <code>lock()</code> 让线程持有了“对象监视器”，和synchronized一样，线程之间还是顺序执行的。</li>
<li>等待/通知： <code>condition.await()</code> 等待；<code>condition.siganl()</code>通知。注意调用<code>condition.await()</code>前须先调用<code>lock.lock()</code>。它比wait更灵活，因为它可以用不同的<code>condition</code>对象实现通知部分线程。</li>
</ul>
<p>公平锁和非公平锁</p>
<ul>
<li>公平锁： <code>new ReentranLock(true)</code> 获取锁是先进先得的</li>
<li>非公平锁： <code>new ReentranLock(false)</code> 获取锁是随机的</li>
</ul>
<p>一些方法：</p>
<ul>
<li><code>int getHoldCount()</code>	查询当前线程保持此锁定的个数，也就是调用lock()方法的次数。</li>
<li><code>int getQueueLength()</code>	返回正在等待获取此锁定的线程估计数</li>
<li><code>int getWaitQueueLength(Condition condition)</code>	返回等待与此锁定相关的给定条件Conditon的线程估计数</li>
<li><code>boolean hasQueueThread(Thread thread)</code>	查询指定的线程是否正在等待获取此锁定</li>
<li><code>boolean hasQueueThreads()</code>	查询是否有线程正在等待获取此锁定</li>
<li><code>boolean hasWaiters(Condition)</code>		查询是否有线程正在等待与此锁定有关的condition条件</li>
<li><code>boolean isFair()</code>	判断是不是公平锁</li>
<li><code>boolean isHeldByCurrentThread()</code>		查询当前线程是否保持此锁定</li>
<li><code>boolean isLocked()</code>	查询此锁定是否由任意线程保持</li>
<li><code>void lockInterruptibly()</code>	如果当前线程未被中断，则获取锁定，如果已经被中断则出现异常</li>
<li><code>boolean tryLock()</code>	仅在调用时锁定未被另一个线程保持的情况下，才获取该锁定</li>
<li><code>boolean tryLock(long timeout,TimeUnit unit)</code>	如果锁定在给定等待时间内没有被另一个线程保持，且当前线程未被中断，则获取该锁定</li>
<li><code>void awaitUninterruptibly()</code>	await状态时调用thread.interrupt()不会报错</li>
</ul>
<h2 id="使用ReentrantReadWriteLock类"><a class="header-anchor" href="#使用ReentrantReadWriteLock类">¶</a>使用ReentrantReadWriteLock类</h2>
<p><code>ReentrantLock</code>类完全互斥，即同一时间只有一个线程可执行lock后面的任务。</p>
<p><code>ReentrantReadWriteLock</code>特点：</p>
<ul>
<li>读读共享</li>
<li>写写互斥</li>
<li>写读互斥</li>
<li>读写互斥</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>java多线程编程chap1-2</title>
    <url>/2019/03/11/java/java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8Bchap1-2/</url>
    <content><![CDATA[<p>本文是《java多线程编程核心技术》第一、二章的学习笔记。</p>
<a id="more"></a>
<h1>第一章：Java多线程技能</h1>
<h2 id="使用多线程"><a class="header-anchor" href="#使用多线程">¶</a>使用多线程</h2>
<ul>
<li>实现多线程有两种方式：继承<code>Thread</code>类、实现<code>Runnable</code>接口</li>
<li><code>Thread.java</code>中的<code>start()</code>方法是异步执行，<code>thread.run</code>是同步；而且<code>start()</code>方法的顺序不代表线程启动的顺序，它是随机的。</li>
<li>在方法前加<code>synchronized</code>可以让线程安全。</li>
</ul>
<h2 id="一些方法"><a class="header-anchor" href="#一些方法">¶</a>一些方法</h2>
<ul>
<li><code>currentThread()</code> 返回代码段正在被哪个线程调用</li>
<li><code>isAlive()</code> 判断当前线程是否处于活动状态</li>
<li><code>sleep()</code> 让线程休眠</li>
<li><code>getID()</code> 获得线程的唯一标志</li>
<li><code>yield()</code> 让当前线程放弃cpu资源，但时间不确定</li>
</ul>
<h2 id="停止线程"><a class="header-anchor" href="#停止线程">¶</a>停止线程</h2>
<p>方式：</p>
<ul>
<li>使用退出标识，使线程正常退出，也就是当run方法完成后线程退出</li>
<li>使用interrupt方法中断线程</li>
</ul>
<p>第二种方式要注意：</p>
<ul>
<li><code>interrupt()</code> 实例方法，让线程中断，再用异常法检测中断使程序停止</li>
<li><code>interrupted()</code>是静态方法，测试当前线程是否已经是中断状态，执行后具有将状态标志清除为false的功能。</li>
<li><code>isInterrupted()</code>是实例方法，测试Thread对象是否已经是中断状态，但不清除状态标志。</li>
<li>如果在sleep状态下停止某一线程，会进入catch语句（不需要判断是否中断），并且清除停止状态，使它变为false.</li>
</ul>
<h2 id="线程的优先级"><a class="header-anchor" href="#线程的优先级">¶</a>线程的优先级</h2>
<p><code>getPriority()</code>查看优先级，<code>setPriority()</code>设置优先级，有1～10级<br>
线程优先级的特点：</p>
<ul>
<li>继承性: 如过线程A启动线程B，则B和A优先级一样</li>
<li>规则性: CPU尽量倾向于把资源优先级高的线程</li>
<li>随机性: 优先级不等同于执行顺序，优先级较高的不一定先执行完</li>
</ul>
<h2 id="守护线程"><a class="header-anchor" href="#守护线程">¶</a>守护线程</h2>
<p>Java的线程分为两种：User Thread(用户线程)、Daemon Thread(守护线程)。</p>
<p>当最后一个非守护线程结束时，守护线程随着JVM一同结束工作。Daemon作用是为其他线程提供便利服务，守护线程最典型的应用就是GC(垃圾回收器)，它就是一个很称职的守护者。</p>
<h1>第二章：对象及变量的并发访问</h1>
<h2 id="synchronized同步方法"><a class="header-anchor" href="#synchronized同步方法">¶</a>synchronized同步方法</h2>
<ul>
<li>方法内的变量为线程安全，而实例变量非线程安全。</li>
<li>不同线程调用<strong>同一对象</strong>里的<strong>synchronized</strong>方法时，是同步的(排队进行)。也就是必须等线程执行完<code>synchronized</code>方法时，下一线程才能执行<code>synchronized</code>方法。</li>
<li>可重入锁: 一个线程得到对象锁后，再次请求此对象锁时是可以得到该对象的锁的。子类也可以调用父类的同步方法。</li>
<li>出现异常，锁自动释放</li>
<li>子类方法不会继承<code>synchronized</code>关键字，需要手动在子类方法中加上。</li>
</ul>
<h2 id="synchronized同步语句块"><a class="header-anchor" href="#synchronized同步语句块">¶</a>synchronized同步语句块</h2>
<ul>
<li>多个线程处理同一个对象的<code>synchronized(this)</code>同步语句块时，只能排队执行</li>
<li>同一个对象的<code>synchronized(this)</code>和<code>synchronized</code>同步方法之间也是同步的。即<strong>同一时间只有一个线程可执行synchronized代码块或方法中的代码</strong></li>
<li><code>synchronized(非this对象x)</code>，是将x对象作为“对象监视器”
<ul>
<li>当多个线程同时执行<code>synchronized(x){}</code>同步代码块时呈同步效果</li>
<li>当其他线程执行x对象中<code>synchronizd</code>同步方法时呈同步效果</li>
<li>当其他线程执行x对象方法里的<code>synchronized(this)</code>代码块时呈同步效果</li>
</ul>
</li>
<li>静态同步	<code>synchronized</code>方法与<code>synchronized(class)</code>代码块是对Class类进行持锁。</li>
<li>当线程在等带根本不可能被释放的锁是，就会出现死锁。</li>
<li>如果同时持有相同的锁对象，线程之间就是同步的。注意对象的属性改变，运行结果还是同步的。</li>
</ul>
<h2 id="volatile关键字"><a class="header-anchor" href="#volatile关键字">¶</a>volatile关键字</h2>
<p><code>volatile</code>: 强制的从公共内存进行取值,而不是从线程私有数据栈中取得变量的值。可以使变量在多个线程间可见，但缺点是不支持原子性。</p>
<p><code>synchronized</code>和<code>volatile</code>比较:</p>
<ul>
<li>关键字volatile是线程同步的轻量级实现，所以volatile性能比synchronized好，并且volatile只能修饰变量，而synchronized可修饰方法和代码块。</li>
<li>多线程访问volatile不会发生阻塞，而synchronized会出现阻塞</li>
<li>volatile能保证数据可见性，不保证原子性;synchronized可以保证原子性，也可以间接保证可见性，因为synchronized会将私有内存和公共内存中的数据做同步。</li>
<li>volatile解决的是变量在多个线程间的可见性，synchronized解决的是多个线程访问资源的同步性（互斥加可见）。</li>
</ul>
<p>原子类:</p>
<p>如<code>AtomicBoolean</code>，<code>AtomicInteger</code>，<code>AtomicLong</code>等。一个原子类型就是一个原子操作可用的类型，可在没有锁的情况下做到线程安全。但原子类的方法间的调用却不是原子的，需要用同步。</p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>corejava基础知识(6)-视图</title>
    <url>/2019/03/07/java/corejava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-6-%E8%A7%86%E5%9B%BE/</url>
    <content><![CDATA[<p>本文是对 java 视图的总结。</p>
<a id="more"></a>
<h1>集合包装器视图</h1>
<p><code>Array</code>包装成<code>List</code>:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Card[] cardArray = <span class="keyword">new</span> Card[<span class="number">52</span>];</span><br><span class="line">List&lt;Card&gt; cardList = Arrays.asList(cardArray);</span><br></pre></td></tr></table></figure>
<p><code>Arrays</code>类的静态方法 <code>asList</code> 将返回一个包装了 <code>List</code> 的包装器。 注意返回的是<strong>视图对象</strong>，它只能使用<code>get</code> ，<code>set</code>方法。</p>
<p><code>Map</code>包装成<code>Set</code>:</p>
<ul>
<li><code>Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet()</code>返回<code>Map.Entry</code>对象的一个集视图。</li>
<li><code>Set&lt;K&gt; keySet()</code> 返回映射中所有键的一个集视图。</li>
<li><code>Col1ection&lt;V&gt; values()</code> 返回映射中所有值的一个集合视图。</li>
</ul>
<p>可以从集视图中删除元素，映射中对应的键和值也会被删除， 不过不能增加任何元素。</p>
<h1>子范围视图</h1>
<p>可以为集合建立子范围视图</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List group2 =staff.subList(<span class="number">10</span>, <span class="number">20</span>); <span class="comment">// 列表的子范围</span></span><br><span class="line"></span><br><span class="line"><span class="function">SortedSet&lt;E&gt; <span class="title">subSet</span><span class="params">(E from, E to)</span></span>; <span class="comment">//有序集根据排序顺序建立子视图</span></span><br><span class="line"><span class="function">SortedSet&lt;E&gt; <span class="title">headSet</span><span class="params">(E to)</span></span>;</span><br><span class="line"><span class="function">SortedSet&lt;E&gt; <span class="title">tailSet</span><span class="params">(E from)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">SortedMap&lt;K, V&gt; <span class="title">subMap</span><span class="params">(K from, K to)</span></span>; <span class="comment">//和有序集同理</span></span><br><span class="line"><span class="function">SortedMap&lt;K, V&gt; <span class="title">headMap</span><span class="params">(K to)</span></span>;</span><br><span class="line"><span class="function">SortedMap&lt;K, V&gt; <span class="title">tailMap</span><span class="params">(K from)</span></span>;</span><br></pre></td></tr></table></figure>
<p>注意：</p>
<ul>
<li>范围是左闭右开</li>
<li>可以将任何操作应用于子范围， 并且能够自动地反映到原范围。</li>
<li>对顺序集合对子集合添加元素时，新元素必须在子范围内</li>
</ul>
<h1>不可修改的视图</h1>
<p>顾名思义生成不可修改的视图，如果发现试图对集合进行修改，就抛出一个异常。当然原对象肯定还是可以修改的。</p>
<p>例如：生成一个不可修改的Map视图:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TreeMap&lt;String, Integer&gt; scores = <span class="keyword">new</span> TreeMap&lt;&gt;();</span><br><span class="line">...</span><br><span class="line">Map&lt;String, Integer&gt; umMap = Collections.synchronizedSortedMap(scores);</span><br></pre></td></tr></table></figure>
<h1>同步视图</h1>
<p>类库的设计者使用视图机制来确保常规集合的线程安全，而不是实现线程安全的集合类。</p>
<p>例如：新建一个线程安全的Map:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Map&lt;String, Employee〉map = Collections.synchronizedMap(<span class="keyword">new</span> HashMap&lt;String, Employee&gt;());</span><br></pre></td></tr></table></figure>
<h1>受查视图</h1>
<p>受查视图可以查出 将错误类型的元素混人泛型集合 这种情况。</p>
<p>例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; safestrings = Collections.checkedList(strings，String,<span class="class"><span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>
<p>当加入错误类型时，会得到错误报告。</p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>corejava基础知识(5)-集合</title>
    <url>/2019/03/07/java/corejava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-5-%E9%9B%86%E5%90%88/</url>
    <content><![CDATA[<p>本文是对 java 集合的总结。书的思路是将接口与实现分开讲；然后从父讲起，再讲子对父的提升。我觉得这种方式很好理解加记忆，所以本文也是这样整理的。</p>
<a id="more"></a>
<h1>集合框架</h1>
<p>主要思路是让<strong>接口与实现分离</strong></p>
<p>集合框架的接口：两个基本接口 <code>Collection</code> 和 <code>Map</code></p>
<img src="https://i.loli.net/2020/01/10/1GljYX6Bu3D8qJU.png" alt="interface.png" style="zoom:67%;">
<p>集合框架的类：</p>
<img src="https://i.loli.net/2020/01/10/sr8g2VMGUb6kDu9.png" alt="me.png" style="zoom:67%;">
<img src="https://i.loli.net/2020/01/10/RjoyvpBwYmJIldU.png" alt="shixian.png" style="zoom:67%;">
<h1>接口</h1>
<h2 id="两个基本接口：Collection-和-Map"><a class="header-anchor" href="#两个基本接口：Collection-和-Map">¶</a>两个基本接口：<code>Collection</code> 和 <code>Map</code></h2>
<h3 id="Collection"><a class="header-anchor" href="#Collection">¶</a>Collection</h3>
<p>以下是<code>Collection</code>接口内的一些方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">interface</span> <span class="title">class</span> <span class="title">java</span>.<span class="title">util</span>.<span class="title">Collection</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(java.lang.Object)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(java.lang.Object)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(java.lang.Object)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">isEmpty</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(java.lang.Object)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">abstract</span> java.util.<span class="function">Iterator <span class="title">iterator</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(java.util.Collection)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">containsAll</span><span class="params">(java.util.Collection)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">removeAll</span><span class="params">(java.util.Collection)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">retainAll</span><span class="params">(java.util.Collection)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">removeIf</span><span class="params">(java.util.function.Predicate)</span></span>;</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看出来就是 <strong>大小，加，删，是否含某个元素，是否空 等等</strong>，这些集合最基础的功能</p>
<p><code>Collection</code>中有<code>Iterator</code>接口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">interface</span> <span class="title">class</span> <span class="title">java</span>.<span class="title">util</span>.<span class="title">Iterator</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">abstract</span> java.lang.<span class="function">Object <span class="title">next</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">forEachRemaining</span><span class="params">(java.util.function.Consumer)</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Map"><a class="header-anchor" href="#Map">¶</a>Map</h3>
<p>以下是<code>Map</code>接口内的一些方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">interface</span> <span class="title">class</span> <span class="title">java</span>.<span class="title">util</span>.<span class="title">Map</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(java.lang.Object, java.lang.Object)</span></span>;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">abstract</span> java.lang.<span class="function">Object <span class="title">get</span><span class="params">(java.lang.Object)</span></span>;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">abstract</span> java.lang.<span class="function">Object <span class="title">put</span><span class="params">(java.lang.Object, java.lang.Object)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(java.lang.Object)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">isEmpty</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">replace</span><span class="params">(java.lang.Object, java.lang.Object, java.lang.Object)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">replaceAll</span><span class="params">(java.util.function.BiFunction)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">putAll</span><span class="params">(java.util.Map)</span></span>;</span><br><span class="line">  <span class="keyword">public</span> java.lang.<span class="function">Object <span class="title">putIfAbsent</span><span class="params">(java.lang.Object, java.lang.Object)</span></span>;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">abstract</span> java.util.<span class="function">Set <span class="title">keySet</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">containsValue</span><span class="params">(java.lang.Object)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">containsKey</span><span class="params">(java.lang.Object)</span></span>;</span><br><span class="line">  <span class="keyword">public</span> java.lang.<span class="function">Object <span class="title">getOrDefault</span><span class="params">(java.lang.Object, java.lang.Object)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">forEach</span><span class="params">(java.util.function.BiConsumer)</span></span>;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看出来就是 <strong>大小，加映射，删映射，用键查值，是否空</strong> 等等，这些映射最基础的功能。还有些视图的部分，下篇文章具体讲</p>
<h2 id="Collection的子接口"><a class="header-anchor" href="#Collection的子接口">¶</a>Collection的子接口</h2>
<h3 id="List"><a class="header-anchor" href="#List">¶</a>List</h3>
<p>有序集合，新加支持随机访问功能：</p>
<ul>
<li>add(n, 元素)</li>
<li>remove(n)</li>
<li>get(n)</li>
<li>set(n, 元素)</li>
</ul>
<h3 id="Set"><a class="header-anchor" href="#Set">¶</a>Set</h3>
<ol>
<li>元素不重复，所以要适当的定义<code>equals</code>方法；</li>
<li>子接口<code>SortedSet</code>，故名思义是排好序的，所以要适当的定义比较器</li>
<li><code>SortedSet</code>的子接口<code>NavigableSet</code>中设计了一些搜素，遍历的方法</li>
</ol>
<h3 id="Queue"><a class="header-anchor" href="#Queue">¶</a>Queue</h3>
<ol>
<li>入队尾，出对头</li>
<li>子接口<code>Qeque</code>双端队列，两头都可以出入</li>
</ol>
<h2 id="Map的子接口"><a class="header-anchor" href="#Map的子接口">¶</a>Map的子接口</h2>
<p>其实和<code>Set</code>差不多</p>
<ol>
<li>子接口<code>SortedMap</code>，故名思义是排好序的，所以要适当的定义比较器</li>
<li><code>SortedMap</code>的子接口<code>NavigableMap</code>中设计了一些搜素，遍历的方法</li>
</ol>
<h1>对接口的实现</h1>
<h2 id="List-v2"><a class="header-anchor" href="#List-v2">¶</a>List</h2>
<h3 id="LinkedList-链表"><a class="header-anchor" href="#LinkedList-链表">¶</a>LinkedList(链表)</h3>
<p>java中的链表都是双向的（有前驱后驱）。重点讲下它的迭代器<code>ListIterator</code></p>
<ul>
<li>初始位置： |ABCD</li>
<li><code>iter.next</code>:  A|BCD</li>
<li><code>iter.previous</code>: |ABCD</li>
<li><code>iter.add</code> : 把元素加到光标<code>|</code>之前，注意是之前</li>
<li><code>iter.remove</code>必须在<code>it.next</code>或<code>it.previous</code>使用之后使用，把光标<strong>经过</strong>的元素删除</li>
</ul>
<p>链表使用注意：</p>
<ul>
<li>使用链表是可以减少在列表中间插入删除元素所付出的代价。</li>
<li>避免使用以整数索引链表中的位置，这样效率很低</li>
</ul>
<h3 id="ArrayList-数组列表"><a class="header-anchor" href="#ArrayList-数组列表">¶</a>ArrayList(数组列表)</h3>
<p><code>ArrayList</code>： 动态泛型数组列表，可以自动调节数组容量。当需要随机访问时使用它效率更高，不用链表。<br>
讲下<code>capacity</code>和<code>size</code>的区别：</p>
<ul>
<li><code>new ArrayList&lt;&gt;(100)</code>	100 是<code>capacity</code>，是初始容量，如果实际size超过它，容量会自动增加</li>
<li><code>array.size()</code>返回的是数组列表内元素实际的个数。</li>
</ul>
<p>p.s. 这和c++的 <code>vector</code> 很像</p>
<h2 id="Set-v2"><a class="header-anchor" href="#Set-v2">¶</a>Set</h2>
<h3 id="HashSet-散列集"><a class="header-anchor" href="#HashSet-散列集">¶</a>HashSet(散列集)</h3>
<p><img src="https://i.loli.net/2020/01/10/Vk8HwoeMAu2v5rf.png" alt="hash.png"></p>
<ul>
<li><code>HashSet</code>: 基于散列表的Set，内部用链表数组实现。初始化时可以指定容量和装填因子。</li>
<li>由于HashSet访问是随机的，所以当不需要关心集合中的顺序时才使用它。</li>
<li>子类<code>LinkedHashSet</code>，里面的元素之间是双向链表，迭代器是按<strong>插入顺序</strong>进行访问的。</li>
</ul>
<h3 id="TreeSet-树集"><a class="header-anchor" href="#TreeSet-树集">¶</a>TreeSet(树集)</h3>
<p>-<code>TreeSet</code>: 实现<code>SortedSet</code>接口，内部基于<strong>红黑树</strong>。由于是有顺序的，所以用户一般要自定义比较器。迭代器是按<strong>排序顺序</strong>进行访问的。</p>
<ul>
<li>当不需要顺序时，用<code>HashSet</code>，因为它更快。</li>
<li>可以实现升级版<code>NavigableSet</code>接口，higher(v),lower(v),pollFirst(),pollLast(),反向迭代 等等。</li>
</ul>
<h2 id="Queue-v2"><a class="header-anchor" href="#Queue-v2">¶</a>Queue</h2>
<p>一般用它的升级版<code>Deque</code>，而<code>Deque</code>可以由<code>ArrayDeque</code>和<code>LinkedList</code>实现。下面主要介绍<code>PriorityQueue</code></p>
<h3 id="PriorityQueue-优先级队列"><a class="header-anchor" href="#PriorityQueue-优先级队列">¶</a>PriorityQueue(优先级队列)</h3>
<ul>
<li><code>PriorityQueue</code>：优先级队列，内部基于<strong>堆</strong>（heap）。</li>
<li>按任意顺序插入，却总是按排序顺序输出<strong>最小值</strong>。</li>
<li><code>PriorityQueue&lt;Type&gt; pq = new PriorityQueue();</code>，从左边可以看出这货是类。</li>
</ul>
<h2 id="Map-v2"><a class="header-anchor" href="#Map-v2">¶</a>Map</h2>
<h3 id="HashMap"><a class="header-anchor" href="#HashMap">¶</a>HashMap</h3>
<ul>
<li>顾名思义和<code>HashSet</code>差不多，只不过是对建进行散列。</li>
<li>子类<code>LinkedHashMap</code>，注意链表散射映射，迭代器是按<strong>访问顺序</strong>进行访问的。每次调用 get 或 put, 受到影响的条目将从当前的位置删除，并放到条目链表的尾部。</li>
</ul>
<h3 id="TreeMap"><a class="header-anchor" href="#TreeMap">¶</a>TreeMap</h3>
<ul>
<li>顾名思义和<code>HashMap</code>差不多，只不过是对建进行<strong>红黑树</strong>排序。</li>
<li>同样 当不需要顺序时，用<code>HashMap</code>，因为它更快.</li>
</ul>
<p>一些技巧：</p>
<ul>
<li>遍历Map：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">scores.forEach((k, v) -&gt; <span class="comment">// 访问 k, v 的最快方法</span></span><br><span class="line">        System.out.println(<span class="string">"key="</span> + k + <span class="string">", value"</span> + v));</span><br></pre></td></tr></table></figure>
<ul>
<li>用 Map 计数：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">scores.put(<span class="string">"Jack"</span>, scores.getOrDefault(<span class="string">"Jack"</span>, <span class="number">0</span>) + <span class="number">1</span>); <span class="comment">// 方法1</span></span><br><span class="line">scores.merge(<span class="string">"Jack"</span>, <span class="number">1</span>, Integer::sum); <span class="comment">// 方法2</span></span><br></pre></td></tr></table></figure>
<h3 id="WeekHashMap"><a class="header-anchor" href="#WeekHashMap">¶</a>WeekHashMap</h3>
<p>弱散列映射：当对键的唯一引用来自散列条目时，这一数据结构将与垃圾回收器协同工作一起删除键 / 值对。也就是当某个值没用了它会被自动回收。</p>
<h3 id="IdentityHashMap"><a class="header-anchor" href="#IdentityHashMap">¶</a>IdentityHashMap</h3>
<ul>
<li><code>IdentityHashMap</code> 中 键的散列值 不是用 hashCode 函数计算的，而是用 <code>System.identityHashCode</code> 方法计算的（<code>Object.hashCode</code>方法根据对象的内存地址来计算散列码时所使用的方式）。</li>
<li>所以，在对两个对象进行比较时，IdentityHashMap 使用 == , 而不使用equal。也就是不同的键对象，即使内容相同，也被视为是不同的对象。</li>
<li>在实现对象遍历算法 (如对象串行化)时，可以它用来跟踪每个对象的遍历状况。</li>
</ul>
<h1>遗留的集合</h1>
<p>看看就好，遇到时再查书</p>
<img src="https://i.loli.net/2020/01/10/1dk29Jrt5H4Yvqe.png" alt="image.png" style="zoom: 67%;">
<p>本文示例代码：<a href="https://github.com/Zouxxyy/java-learning/tree/master/corejava-learning/src/com/zouxxyy/corejava/chap9" title="github" target="_blank" rel="noopener">github</a></p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>corejava基础知识(4)-通配符</title>
    <url>/2019/03/05/java/corejava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86(4)-%E9%80%9A%E9%85%8D%E7%AC%A6/</url>
    <content><![CDATA[<p>本文是对 java 通配符的总结</p>
<a id="more"></a>
<h1>通配符</h1>
<h2 id="泛型的局限性与漏洞"><a class="header-anchor" href="#泛型的局限性与漏洞">¶</a>泛型的局限性与漏洞</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pair&lt;Employee&gt; employeePair= <span class="keyword">new</span> Pair&lt;&gt;(<span class="keyword">new</span> Employee(<span class="string">"员工1"</span>), <span class="keyword">new</span> Employee(<span class="string">"员工2"</span>));</span><br><span class="line">Pair&lt;Manager&gt; managerPair= <span class="keyword">new</span> Pair&lt;&gt;(<span class="keyword">new</span> Manager(<span class="string">"经理1"</span>, <span class="number">100</span>), <span class="keyword">new</span> Manager(<span class="string">"经理2"</span>, <span class="number">200</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//测试1</span></span><br><span class="line"><span class="comment">//employeePair = managerPair; //错误， `Pair&lt;S&gt;`和`Pair&lt;T&gt;`没有什么联系。</span></span><br><span class="line"><span class="comment">//managerPair = employeePair // 错误， `Pair&lt;S&gt;`和`Pair&lt;T&gt;`没有什么联系。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//测试2</span></span><br><span class="line">Pair pair = employeePair; <span class="comment">// 泛型与原始内容兼容，但是，看下面</span></span><br><span class="line"><span class="comment">//pair.getFirs t().getName(); // error, 是Employee类，但是不能调用方法!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//测试3</span></span><br><span class="line">employeePair.setFirst(<span class="keyword">new</span> Manager(<span class="string">"经理3"</span>, <span class="number">300</span>)); <span class="comment">// employeePair里经理竟然和员工组成一对！</span></span><br><span class="line">System.out.println(employeePair.getFirst().getName()); <span class="comment">// 经理3</span></span><br><span class="line">System.out.println(employeePair.getSecond().getName()); <span class="comment">// 员工2</span></span><br><span class="line"><span class="comment">// System.out.println(((employeePair.getFirst())).getSalary()); // error</span></span><br><span class="line">System.out.println(((Manager)(employeePair.getFirst())).getSalary()); <span class="comment">// 300.0 ，添加类强制类型转换后可以调用，这与普通类继承规则一样</span></span><br></pre></td></tr></table></figure>
<p>完整代码：<a href="https://github.com/Zouxxyy/java-learning/blob/master/corejava-learning/src/com/zouxxyy/corejava/chap8/inherit/testInherit.java" title="github" target="_blank" rel="noopener">继承示例</a></p>
<ol>
<li>无论 S 和 T 有什么关系，<code>Pair&lt;S&gt;</code>和<code>Pair&lt;T&gt;</code>没有什么联系。</li>
<li>泛型与原始内容兼容，但是原始内容里的类型参数这个对象无法调用方法</li>
<li>泛型里的类型参数可以继承，和类的继承规则一样</li>
</ol>
<h2 id="extends-Object（上边界限定通配符）"><a class="header-anchor" href="#extends-Object（上边界限定通配符）">¶</a>?  extends Object（上边界限定通配符）</h2>
<p>可以看出来原始泛型遇上继承时会有些漏洞，比如会出现经理员工在同一<code>Pair</code>的情况。于是Java专家引入了类型通配符 <code>?</code></p>
<p>我们把刚刚的第一行改为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pair&lt;? extends Employee&gt; employeePair= <span class="keyword">new</span> Pair&lt;&gt;(<span class="keyword">new</span> Employee(<span class="string">"员工1"</span>), <span class="keyword">new</span> Employee(<span class="string">"员工2"</span>));</span><br></pre></td></tr></table></figure>
<p>此时，如果再向里面添加<code>	Manager</code>时就会发生错误：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">employeePair.setFirst(<span class="keyword">new</span> Manager(<span class="string">"经理3"</span>, <span class="number">300</span>)); <span class="comment">// 错误</span></span><br><span class="line">employeePair.setFirst(<span class="keyword">new</span> Employee(<span class="string">"员工"</span>)); <span class="comment">// 错误，甚至添加员工都不行</span></span><br></pre></td></tr></table></figure>
<p>但是访问可以：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Employee employee = employeePair.getFirst(); <span class="comment">// 正确</span></span><br></pre></td></tr></table></figure>
<h3 id="分析"><a class="header-anchor" href="#分析">¶</a>分析</h3>
<p>首先永远记住<strong>只能超类接收子类！！！反过来不行！！！</strong>（这个可以解释下面的一切）</p>
<p><strong>类型擦除</strong> 后 <code>Pair&lt;? extends Employee&gt;</code> 的方法有：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">? <span class="function">extends Employee <span class="title">getFirst</span><span class="params">()</span> </span>&#123;...&#125; <span class="comment">// 访问器</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setFirst</span><span class="params">(? extends Employee)</span> </span>&#123;...&#125; <span class="comment">// 更改器</span></span><br></pre></td></tr></table></figure>
<ul>
<li>访问器的返回值是<code>? extends Employee</code> ，可以用子类<code>Employee</code>接收</li>
<li>更改器的接收值是<code>? extends Employee</code> ，极端情况是<code>Employee</code>的<code>最下面的子类</code>，而<code>最下面的子类</code>只能接收更下面的子类（无），因此 <strong>拒绝接收任何特定的类型！</strong></li>
</ul>
<h3 id="小结"><a class="header-anchor" href="#小结">¶</a>小结</h3>
<p>简单说就是：<br>
可以 <code>Employee</code> &lt;-- <code>? extends Employee</code> ，但是反过来不行！</p>
<p>所以这就是大家说的使用<code>? extends Object</code> 可以 <strong>安全的访问泛型对象</strong>。我的理解核心是：如果<code>T</code>作为<strong>返回值</strong>，用<code>? extends Object</code>更安全。</p>
<h2 id="super-Object（下边界限定通配符）"><a class="header-anchor" href="#super-Object（下边界限定通配符）">¶</a>?  super Object（下边界限定通配符）</h2>
<p>这个和上面正好相反，更改器能用，访问器不能用。</p>
<h3 id="分析-v2"><a class="header-anchor" href="#分析-v2">¶</a>分析</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">? <span class="function"><span class="keyword">super</span> Employee <span class="title">getFirst</span><span class="params">()</span> </span>&#123;...&#125; <span class="comment">// 访问器</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setFirst</span><span class="params">(? <span class="keyword">super</span> Employee)</span> </span>&#123;...&#125; <span class="comment">// 更改器</span></span><br></pre></td></tr></table></figure>
<ul>
<li>访问器的返回值是<code>? super Employee</code> ，极端情况是<code>Object</code>，只能用<code>Object</code>接收</li>
<li>更改器的接收值是<code>? super Employee</code> ，可以接收<code>Employee 和 它的子类</code></li>
</ul>
<h3 id="小结-v2"><a class="header-anchor" href="#小结-v2">¶</a>小结</h3>
<p>简单说就是：<br>
可以 <code>? super Employee</code> &lt;-- <code>Employee</code> ，但是反过来不行！</p>
<p>所以这就是大家说的使用<code>? super Object</code> 可以 <strong>安全的更改泛型对象</strong>。我的理解核心是：如果<code>T</code>作为<strong>方法参数</strong>，用<code>? super Object</code>更安全。</p>
<h3 id="共同特点"><a class="header-anchor" href="#共同特点">¶</a>共同特点</h3>
<p>右边的值传给左边接收：</p>
<p><code>? super Employee</code> &lt;-- <code>Employee</code> &lt;-- <code>? extends Employee</code></p>
<p>是不是完全符合 <strong>只能超类接收子类</strong>，知道原理记起来就简单。</p>
<h3 id="例子"><a class="header-anchor" href="#例子">¶</a>例子</h3>
<p>举书上的一个例子作为练习：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;T extends Comparable&lt;? <span class="keyword">super</span> T&gt;&gt; <span class="function">T <span class="title">min</span><span class="params">(T[] a)</span> <span class="comment">//计算T[]的最小值</span></span></span><br></pre></td></tr></table></figure>
<p>理解：</p>
<ul>
<li><code>T extends...</code> 好理解，就是<code>T</code>要实现后面的接口。</li>
<li>实现<code>Comparable&lt;? super T&gt;</code> 接口里的方法<code>int compareTo(? super T)</code>；此时<code>类型</code>作为 方法参数，所以用<code>? super</code>更安全。</li>
</ul>
<h2 id="？（无限定通配符）"><a class="header-anchor" href="#？（无限定通配符）">¶</a>？（无限定通配符）</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">? getFirst() &#123;...&#125; <span class="comment">// 访问器</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setFirst</span><span class="params">(?)</span> </span>&#123;...&#125; <span class="comment">// 更改器</span></span><br></pre></td></tr></table></figure>
<ul>
<li>访问器只能用<code>Object</code>接收</li>
<li>更改器不能用</li>
</ul>
<p>可以用任意<code>Object</code>对象调用原始 <code>Pair</code>类的<code>setObject</code>方法，说白了就是什么类型都能<br>
作为泛型方法的方法参数，但就是不能有返回值。</p>
<h2 id="通配符捕获"><a class="header-anchor" href="#通配符捕获">¶</a>通配符捕获</h2>
<p>由于通配符不能作为类型变量，所以必要时可以用一个辅助的泛型方法。</p>
<p>第一步：辅助泛型方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function"><span class="keyword">void</span> <span class="title">swapHelper</span><span class="params">(Pair&lt;T&gt; p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    T t = p.getFirst();</span><br><span class="line">    p.setFirst(p.getSecond());</span><br><span class="line">    p.setSecond(t);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第二步：通配符捕获</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(Pair&lt;?&gt; p)</span></span>&#123; swapHelper(p); &#125;</span><br></pre></td></tr></table></figure>
<h2 id="示例"><a class="header-anchor" href="#示例">¶</a>示例</h2>
<p>完整代码：<a href="https://github.com/Zouxxyy/java-learning/blob/master/corejava-learning/src/com/zouxxyy/corejava/chap8/pair3/PairTest3.java" title="github" target="_blank" rel="noopener">通配符示例</a></p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>corejava基础知识(3)-泛型</title>
    <url>/2019/03/05/java/corejava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86(3)-%E6%B3%9B%E5%9E%8B/</url>
    <content><![CDATA[<p>本文是对 java 泛型的总结</p>
<a id="more"></a>
<h1>泛型</h1>
<h2 id="泛型类与方法的一般格式"><a class="header-anchor" href="#泛型类与方法的一般格式">¶</a>泛型类与方法的一般格式</h2>
<p>泛型类：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>&lt;<span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> T first; </span><br><span class="line">    <span class="keyword">private</span> T second;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Pair</span><span class="params">()</span> </span>&#123; first = <span class="keyword">null</span> ; second = <span class="keyword">null</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Pair</span><span class="params">(T first, T second)</span> </span>&#123; <span class="keyword">this</span>.first = first; <span class="keyword">this</span>.second = second; &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> T <span class="title">getFirst</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> first; &#125; </span><br><span class="line">    <span class="function"><span class="keyword">public</span> T <span class="title">getSecond</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> second; &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFirst</span><span class="params">(T newValue)</span> </span>&#123; first = newValue; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSecond</span><span class="params">(T newValue)</span> </span>&#123; second = newValue; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>泛型方法:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArrayAlg</span></span></span><br><span class="line"><span class="class"></span>&#123; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">T <span class="title">getMiddle</span><span class="params">(T... a)</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> a[a.length / <span class="number">2</span>];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法是在普通类定义的泛型方法，调用时：</p>
<p><code>String middle = ArrayAlg.&lt;String&gt;getMiddle(&quot;]ohnM, &quot;Q.n, &quot;Public&quot;);</code></p>
<p>术语：</p>
<ul>
<li><code>ArrayList&lt;E&gt;</code> – 泛型类型</li>
<li><code>ArrayList</code> – 原始类型</li>
<li><code>E</code> – 类型参数(变量)</li>
<li><code>&lt;&gt;</code> – 读作”typeof”</li>
<li><code>ArrayList&lt;Integer&gt;</code> – 参数化的类型</li>
<li><code>Integer</code> – 实际类型参数</li>
</ul>
<h2 id="类型变量的限定"><a class="header-anchor" href="#类型变量的限定">¶</a>类型变量的限定</h2>
<p>我们可以对类型变量加以约束，毕竟我们很难适用每种泛型。可以通过对类型变量 T 设置限定(bound) 实现这一点，格式如下：</p>
<p><code>&lt;T extends BoundingType&gt;</code></p>
<p>BoundingType 可以是</p>
<ul>
<li><strong>类</strong>：T 是它的子类型(subtype)</li>
<li><strong>接口</strong>： T 是实现了该接口的类</li>
</ul>
<p>可以有多个接口，当然至多有一个类，用 “&amp;” 分隔。</p>
<p><strong>ps: 由于类型擦除机制，为了提高效率，应该将标签(tagging) 接口 (即没有方法的接口)放在边界列表的末尾。</strong></p>
<h2 id="泛型转化的内部"><a class="header-anchor" href="#泛型转化的内部">¶</a>泛型转化的内部</h2>
<ul>
<li>虚拟机中没有泛型， 只有普通的类和方法；所有的类型参数都用它们的限定类型替换。</li>
<li>编译器在调用泛型方法时会自动插入强制类型转换。</li>
<li>桥方法被合成来保持多态。</li>
</ul>
<h2 id="约束与局限性"><a class="header-anchor" href="#约束与局限性">¶</a>约束与局限性</h2>
<ul>
<li><strong>不能用基本类型实例化类型参数</strong></li>
</ul>
<p>即没有<code>Pair&lt;double&gt;</code>，只有<code>Pair&lt;Double&gt;</code>。原因是类型擦出后，Object不能存储double值，毕竟它不是对象</p>
<ul>
<li><strong>运行时类型查询只适用于原始类型</strong></li>
</ul>
<p>所有的类型查询只产生原始类型，例子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pair&lt;String&gt; stringPair = ...;</span><br><span class="line">Pair&lt;Employee&gt; employeePair = ...;</span><br><span class="line"><span class="keyword">if</span>(stringPir.getClass() == employeePair.getClass()) <span class="comment">// they are equal</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>不能创建参数化类型的数组</strong></li>
</ul>
<p><code>Pair&lt;String&gt;[] table</code>是错的；</p>
<p>只能使用AllayList： <code>ArrayList&lt;Pair&lt;String&gt;&gt; table</code></p>
<ul>
<li><strong>Varargs警告</strong></li>
</ul>
<p>向参数个数可变的方法传递一个泛型类型的实例时，为了调用这个方法，Java 虚拟机必须建立一个参数化类型的数组，可是这违反了前一个规则。</p>
<p>我们可以用 @SafeVarargs 直接标注该方法，这样就能正常运行。</p>
<ul>
<li><strong>不能实例化类型变量</strong></li>
</ul>
<p>例如：</p>
<p><code>public Pair() { first = new T(); second = new T(); } // Error</code></p>
<ul>
<li>
<p><strong>不能构造泛型数组</strong></p>
</li>
<li>
<p><strong>泛型类的静态上下文中类型变量无效</strong></p>
</li>
</ul>
<p>也就是不能在静态域或方法中引用类型变量。</p>
<ul>
<li><strong>不能抛出或捕获泛型类的实例</strong></li>
<li><strong>可以消除对受查异常的检查</strong></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>corejava基础知识(2)-接口和lambda表达式</title>
    <url>/2019/03/05/java/corejava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86(2)-%E6%8E%A5%E5%8F%A3%E5%92%8Clambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[<p>本文是对 java 接口和lambda表达式的总结</p>
<a id="more"></a>
<h1>接口</h1>
<h2 id="接口概念"><a class="header-anchor" href="#接口概念">¶</a>接口概念</h2>
<p>接口是对类的一组需求的描述，类遵从特定的描述，实现这项服务。<br>
例如：<br>
这是Camprable接口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Comparable</span>&lt;<span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(T other)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后在类中实现这一接口:</p>
<ol>
<li>将类声明为实现给定的接口</li>
<li>对接口中<strong>所有</strong>方法进行定义</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">implements</span> <span class="title">Comparable</span>&lt;<span class="title">Employee</span>&gt;</span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Employee other)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Double.compare(salary, other.salary);</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="接口特性"><a class="header-anchor" href="#接口特性">¶</a>接口特性</h2>
<ul>
<li>接口不是类，不能用new实例化一个接口，可是能声明接口变量，然后再引用实现了该接口的类对象</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Compararble x;</span><br><span class="line">x = <span class="keyword">new</span> Employee(...); <span class="comment">//ok</span></span><br></pre></td></tr></table></figure>
<p>解释：</p>
<p>可能会问这样的接口变量有啥用呢？？举个定时器的例子，Timer函数 需要接收 一个操作函数 ，但是我们不能直接传函数进去，所以我们把接口变量（引用实现了该接口的类）传进去。同时我们可以发现接口里的方法<strong>不是静态方法</strong>，因为我们需要先新建对象。</p>
<p>当然 javase8 中可以用 <strong>lambda表达式</strong> 传函数，见下文 lambda 表达式</p>
<ul>
<li>检查某对象是否实现某接口</li>
</ul>
<p><code>if(anObject instanceof Comparable) {...}</code></p>
<ul>
<li>接口也可以扩展</li>
</ul>
<p><code>public interface Powered extends Moveable</code></p>
<ul>
<li>接口中不含实例域，却可以包含常量（publice static final)</li>
<li>可以为接口提供默认实现</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Comparable</span>&lt;&lt;<span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">default</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(T other)</span></span>&#123;<span class="keyword">return</span> <span class="number">0</span>&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>一个类可以有多个接口</p>
</li>
<li>
<p><strong>继承时容易出现的问题：</strong></p>
</li>
</ul>
<p>例如子类继承compareTo方法时，必须要有<strong>子类与超类进行比较</strong>的思想，就像第五章equals方法一样，不然会有<strong>ClassCastException</strong>错误。</p>
<ul>
<li>只有一个抽象方法，并用lambda表达式替代，这种接口叫<strong>函数式接口</strong>；不含任何方法的接口叫<strong>标记接口</strong>，如 Cloneable。</li>
</ul>
<h2 id="解决接口冲突"><a class="header-anchor" href="#解决接口冲突">¶</a>解决接口冲突</h2>
<p>同名，且参数类型相同下：</p>
<p>1）超类优先</p>
<p>2）接口冲突：只要其中一个接口提供了默认方法，就会报错；如果都不提供，就是抽象类。</p>
<p>ps. 类优先的机制确保了与java SE 7 的兼容，因为那时没有默认方法。</p>
<h1>lambda表达式</h1>
<h2 id="为什么引入lambda表达式呢？"><a class="header-anchor" href="#为什么引入lambda表达式呢？">¶</a>为什么引入lambda表达式呢？</h2>
<p><code>Arrays.sort(strings, new LengthComparator());</code></p>
<p>这段代码用定制的比较器完成字符串排序，可以看出它传入了一个对象（接口），这个对象实现了Camparable接口，对象里面有compare方法。我们主要目的就是想传递这个方法，但是Java之前只能通过构造对象，来传递该方法。于是lambda表达式正式引入！</p>
<h2 id="lambda表达式语法"><a class="header-anchor" href="#lambda表达式语法">¶</a>lambda表达式语法</h2>
<p>例子：</p>
<p><code>(String first, String second) -&gt; {...}</code></p>
<p>把它替换到之前到代码中：</p>
<p><code>Array.sort(strings, (fisrt, second)-&gt;first.length() - second.length());</code></p>
<p>一些细节：</p>
<ul>
<li>如果可以推导出参数类型，那么类型可以不写；而且如果只有这一个参数，小括号也可以不写。</li>
<li>如果某些分支返回一个值，而另一些分支不返回值，这是不合法的。<br>
<code>(int) -&gt; {if(x &gt;= 0) return 1;} //不合法</code></li>
<li>对于只有一个抽象方法的接口，可以提供一个lambda，就像例子；这种接口称为函数式接口。常的用函数式接口有：Predicate, Consumer, Function, Supplier 等等。</li>
</ul>
<h2 id="方法引用与lambda"><a class="header-anchor" href="#方法引用与lambda">¶</a>方法引用与lambda</h2>
<p>假如想传递的 lambda 已经有了现成的方法，那么可以用方法引用来代替lambda。<br>
三种情况：</p>
<ol>
<li>
<p>静态方法</p>
<ul>
<li>Lambda: <code>(args) -&gt; ClassName.staticMethod(args)</code></li>
<li>方法引用：<code>ClassName::staticMethod</code></li>
</ul>
</li>
<li>
<p>现有对象的实例方法</p>
<ul>
<li>Lambda: <code>(args) -&gt; expr.instanceMethod(args)</code></li>
<li>方法引用：<code>expr::intanceMethod</code></li>
</ul>
</li>
<li>
<p>某类的实例方法</p>
<ul>
<li>Lambda: <code>(arg0, rest) -&gt; arg0.instanceMethod(rest)</code></li>
<li>方法引用：<code>ClassName::instanceMethod //arg0 是 ClassName 类型的</code></li>
</ul>
</li>
</ol>
<h2 id="变量作用域"><a class="header-anchor" href="#变量作用域">¶</a>变量作用域</h2>
<p>lambda 表达式可以访问外围方法或类中的变量，但需注意：</p>
<ul>
<li>该变量必须是最终变量，不可以重新赋值；如String</li>
<li>在 lambda 表达式中声明与一个局部变量同名的参数或局部变量是不合法的。</li>
<li>在一个 lambda 表达式中使用 this 关键字时， 是指创建这个 lambda 表达式的方法的 this 参数。 简单说就是没变化。</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>corejava基础知识(1)-继承</title>
    <url>/2019/03/05/java/corejava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86(1)-%E7%BB%A7%E6%89%BF/</url>
    <content><![CDATA[<a id="more"></a>
<p>占位</p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>hello hexo</title>
    <url>/2019/02/26/%E5%B7%A5%E5%85%B7/hello-hexo/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2020/01/10/5UKxWLad8w37kAi.png" alt="me.png"></p>
<a id="more"></a>
<h1>生成文章</h1>
<ol>
<li>使用命令生成初始文章，文件名之间有空格的话会自动加-中横线</li>
</ol>
<p><code>hexo n &quot;hello hexo&quot;  #双引号中填写要生成的文章名</code></p>
<ol start="2">
<li>在hexo/source/_posts中编辑生成出来的hello-hexo.md文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: hello hexo</span><br><span class="line">date: 2019-02-26 11:20:55</span><br><span class="line">tags:</span><br><span class="line">- hello</span><br><span class="line">categories: </span><br><span class="line">- hello hexo</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>本地预览文章内容，登录localhost:4000查看效果</li>
</ol>
<p><code>hexo s</code></p>
<ol start="4">
<li>内容发布到public文件夹数中，然后手动复制同步GitHub</li>
</ol>
<p><code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></p>
<hr>
<h1>Markdown语法</h1>
<h2 id="一、标题"><a class="header-anchor" href="#一、标题">¶</a>一、标题</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">### 这是三级标题</span><br><span class="line">#### 这是四级标题</span><br></pre></td></tr></table></figure>
<h3 id="这是三级标题"><a class="header-anchor" href="#这是三级标题">¶</a>这是三级标题</h3>
<h4 id="这是四级标题"><a class="header-anchor" href="#这是四级标题">¶</a>这是四级标题</h4>
<h2 id="二、字体"><a class="header-anchor" href="#二、字体">¶</a>二、字体</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**这是加粗的文字**</span><br><span class="line">*这是倾斜的文字*</span><br><span class="line">***这是斜体加粗的文字***</span><br><span class="line">~~这是加删除线的文字~~</span><br></pre></td></tr></table></figure>
<p><strong>这是加粗的文字</strong></p>
<p><em>这是倾斜的文字</em></p>
<p><em><strong>这是斜体加粗的文字</strong></em></p>
<p><s>这是加删除线的文字</s></p>
<h2 id="三、引用"><a class="header-anchor" href="#三、引用">¶</a>三、引用</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;这是引用的内容</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这是引用的内容</p>
</blockquote>
<h2 id="四、图片"><a class="header-anchor" href="#四、图片">¶</a>四、图片</h2>
<p><strong>语法：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![图片文字](图片地址 &quot;图片标题&quot;)</span><br><span class="line"></span><br><span class="line">图片标题就是显示在图片下面的文字</span><br><span class="line">图片文字可加可不加</span><br></pre></td></tr></table></figure>
<p><strong>示例：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![test](hello-hexo/test.jpg &quot;图片&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>效果：</strong></p>
<p><img src="https://i.loli.net/2020/01/10/2PaTmrRCe65lLgN.jpg" alt="test.jpg"></p>
<h2 id="五、超链接"><a class="header-anchor" href="#五、超链接">¶</a>五、超链接</h2>
<p><strong>语法：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[超链接名](超链接地址 &quot;超链接title&quot;)</span><br><span class="line">title可加可不加</span><br></pre></td></tr></table></figure>
<p><strong>示例：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[百度](http://baidu.com)</span><br></pre></td></tr></table></figure>
<p><strong>效果：</strong></p>
<p><a href="http://baidu.com" target="_blank" rel="noopener">百度</a></p>
<h2 id="六、列表"><a class="header-anchor" href="#六、列表">¶</a>六、列表</h2>
<p><strong>- 无序列表</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- 列表内容</span><br><span class="line">+ 列表内容</span><br><span class="line">* 列表内容</span><br></pre></td></tr></table></figure>
<ul>
<li>列表内容</li>
</ul>
<ul>
<li>列表内容</li>
</ul>
<ul>
<li>列表内容</li>
</ul>
<p><strong>- 有序列表</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.列表内容</span><br><span class="line">2.列表内容</span><br><span class="line">3.列表内容</span><br></pre></td></tr></table></figure>
<ol>
<li>列表内容</li>
<li>列表内容</li>
<li>列表内容</li>
</ol>
<p><strong>- 列表嵌套</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 一级列表</span><br><span class="line">   1. 二级列表</span><br><span class="line">2. 一级列表</span><br><span class="line">	2. 二级列表</span><br></pre></td></tr></table></figure>
<ol>
<li>一级列表
<ol>
<li>二级列表</li>
</ol>
</li>
<li>一级列表
<ol start="2">
<li>二级列表</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
</search>
