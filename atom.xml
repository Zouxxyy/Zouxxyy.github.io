<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZxysHexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-01-13T08:36:21.137Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zouxxyy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>spark源码-BlockManager</title>
    <link href="http://yoursite.com/2020/01/11/spark/spark%E6%BA%90%E7%A0%81-BlockManager/"/>
    <id>http://yoursite.com/2020/01/11/spark/spark源码-BlockManager/</id>
    <published>2020-01-11T13:41:08.000Z</published>
    <updated>2020-01-13T08:36:21.137Z</updated>
    
    <content type="html"><![CDATA[<p>每台节点（driver and executors）的block的总管理者，主要功能就是在本地或者远程的store中(堆内内存、磁盘、堆外内存) put、get、 block。</p><p>作为总管理者，BlockManager 依赖众多对象。</p><a id="more"></a><h1 id="重要成员"><a class="header-anchor" href="#重要成员"></a>重要成员</h1><p>从成员可以大体4类：RPC、传输、内存磁盘的 get or put（本文侧重）、shuffle</p><p><strong>RPC 相关</strong>（todo）</p><ul><li><p>master：BlockManagerMaster，RPC相关</p></li><li><p>rpcEnv</p></li><li><p>blockManagerId：该blockManager的Id，分布式系统</p></li><li><p>slaveEndpoint</p></li></ul><p><strong>块及其传输相关</strong>（todo）</p><ul><li><p>serializerManager：序列化管理者</p></li><li><p>mapOutputTracker：跟踪 shuffle write 的输出</p></li><li><p>blockTransferService：块传输服务（netty）</p></li><li><p>securityManager：块加密</p></li><li><p>remoteReadNioBufferConversion：<code>spark.network.remoteReadNioBufferConversion</code></p></li><li><p>futureExecutionContext</p></li><li><p>maxFailuresBeforeLocationRefresh</p></li><li><p>remoteBlockTempFileManager</p></li></ul><p><strong>内存磁盘相关</strong>：</p><ul><li><p><strong>memoryManager</strong></p></li><li><p><strong>memoryStore</strong></p></li><li><p><strong>diskBlockManager</strong></p></li><li><p><strong>diskStore</strong></p></li><li><p><strong>blockInfoManager</strong></p></li></ul><p><strong>shuffle</strong> 相关</p><ul><li><p>shuffleManager：使用的shuffleManager</p></li><li><p>externalShuffleServiceEnabled：<code>spark.shuffle.service.enabled</code>，是否使用外部排序(todo)，默认false</p></li><li><p>externalShuffleServicePort</p></li><li><p>shuffleServerId</p></li><li><p>shuffleClient：</p></li></ul><h1 id="重点功能"><a class="header-anchor" href="#重点功能"></a>重点功能</h1><h2 id="initialize"><a class="header-anchor" href="#initialize"></a>initialize</h2><p>只有掉用了initialize方法，该BlockManager才可用，看看就行，pass</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(appId: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  blockTransferService.init(<span class="keyword">this</span>)</span><br><span class="line">  shuffleClient.init(appId)</span><br><span class="line"></span><br><span class="line">  blockReplicationPolicy = &#123;</span><br><span class="line">    <span class="keyword">val</span> priorityClass = conf.get(</span><br><span class="line">      <span class="string">"spark.storage.replication.policy"</span>, classOf[<span class="type">RandomBlockReplicationPolicy</span>].getName)</span><br><span class="line">    <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(priorityClass)</span><br><span class="line">    <span class="keyword">val</span> ret = clazz.newInstance.asInstanceOf[<span class="type">BlockReplicationPolicy</span>]</span><br><span class="line">    logInfo(<span class="string">s"Using <span class="subst">$priorityClass</span> for block replication policy"</span>)</span><br><span class="line">    ret</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> id =</span><br><span class="line">    <span class="type">BlockManagerId</span>(executorId, blockTransferService.hostName, blockTransferService.port, <span class="type">None</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> idFromMaster = master.registerBlockManager(</span><br><span class="line">    id,</span><br><span class="line">    maxOnHeapMemory,</span><br><span class="line">    maxOffHeapMemory,</span><br><span class="line">    slaveEndpoint)</span><br><span class="line"></span><br><span class="line">  blockManagerId = <span class="keyword">if</span> (idFromMaster != <span class="literal">null</span>) idFromMaster <span class="keyword">else</span> id</span><br><span class="line"></span><br><span class="line">  shuffleServerId = <span class="keyword">if</span> (externalShuffleServiceEnabled) &#123;</span><br><span class="line">    logInfo(<span class="string">s"external shuffle service port = <span class="subst">$externalShuffleServicePort</span>"</span>)</span><br><span class="line">    <span class="type">BlockManagerId</span>(executorId, blockTransferService.hostName, externalShuffleServicePort)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    blockManagerId</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Register Executors' configuration with the local shuffle service, if one should exist.</span></span><br><span class="line">  <span class="keyword">if</span> (externalShuffleServiceEnabled &amp;&amp; !blockManagerId.isDriver) &#123;</span><br><span class="line">    registerWithExternalShuffleServer()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  logInfo(<span class="string">s"Initialized BlockManager: <span class="subst">$blockManagerId</span>"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Put"><a class="header-anchor" href="#Put"></a>Put</h2><p>block就是文件，所以只要有spark计算有写文件操作（内存和磁盘），那么就要用到 put。</p><p>内容较多，我的学习目标是：先<strong>思考清楚spark计算流程</strong>，至于BlockManager内部弄明白<strong>调用流程和store的选择</strong>即可。能力有限，下面的方式并不是全部。</p><h3 id="RDD-cache"><a class="header-anchor" href="#RDD-cache"></a>RDD cache</h3><p>RDD 的缓存 参见我写的spark cache：第一次执行时，自然是存。并且存完后也要读，因为该函数就是要get，这里就不提读步骤。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManager</span></span><br><span class="line"><span class="type">ShuffleMapTask</span>/<span class="type">ResultTask</span>.runTask  -&gt; <span class="type">RDD</span>.iterator -&gt; <span class="type">RDD</span>.getOrCompute -&gt; getOrElseUpdate</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager内部</span></span><br><span class="line">getOrElseUpdate -&gt; doPutIterator -&gt; doput</span><br><span class="line"></span><br><span class="line"><span class="comment">// 细看下doPutIterator的流程</span></span><br><span class="line">level.useMemory  -<span class="literal">true</span>--&gt; level.deserialized -<span class="literal">true</span>--&gt; memoryStore.putIteratorAsValues 注意</span><br><span class="line">                                             -<span class="literal">false</span>-&gt; memoryStore.putIteratorAsBytes  注意</span><br><span class="line">                 -<span class="literal">false</span>-&gt; diskStore.put</span><br><span class="line"><span class="comment">//注意： 当level为内存和磁盘时，会先存内存，内存不足再存磁盘</span></span><br></pre></td></tr></table></figure><h3 id="Broadcast"><a class="header-anchor" href="#Broadcast"></a>Broadcast</h3><p>广播变量的存储</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManager</span></span><br><span class="line"><span class="type">TorrentBroadcast</span>.writeBlocks -&gt; putSingle</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager内部</span></span><br><span class="line">putSingle -&gt; putIterator -&gt; doPutIterator -&gt; doput</span><br><span class="line"></span><br><span class="line"><span class="comment">// doPutIterator 同理</span></span><br></pre></td></tr></table></figure><h3 id="RDD-block-Replication"><a class="header-anchor" href="#RDD-block-Replication"></a>RDD block Replication</h3><blockquote><p>当RDD的storage level中的_replication大于1时，BlockManager需要将block数据发到另一个远程结点以备份，此时BlockManager会向远程结点发送UploadBlock消息，远程结点在收到该消息后会申请存储内存以存放收到的block数据。</p></blockquote><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManage，分2类</span></span><br><span class="line"><span class="type">NettyBlockRpcServer</span>.receive -&gt; <span class="keyword">match</span> uploadBlock -&gt; putBlockData</span><br><span class="line"><span class="type">NettyBlockRpcServer</span>.receiveStream -&gt; putBlockDataAsStream</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager内部</span></span><br><span class="line">putBlockData -&gt; putBytes -&gt; doPutBytes -&gt; doPut</span><br><span class="line">putBlockDataAsStream -&gt; 看不懂 -&gt; putBytes -&gt; doPutBytes -&gt; doPut</span><br><span class="line"></span><br><span class="line"><span class="comment">// doPutBytes 存储逻辑和 doPutIterator 基本一样</span></span><br><span class="line"><span class="comment">// 区别是 doPutIterator 输入是 Iterator[T]；doPutBytes 输入是 ChunkedByteBuffer，它需要先反序列化</span></span><br><span class="line"><span class="comment">// 分析：doPutIterator 是缓存步骤计算得到的自然是java对象； doPutBytes 是节点传自然是序列化的</span></span><br></pre></td></tr></table></figure><h3 id="Shuffle-相关"><a class="header-anchor" href="#Shuffle-相关"></a>Shuffle 相关</h3><p>Shuffle时有许多写磁盘的操作，它们都是使用了一个专门的直接将数据写入磁盘的类：</p><p>DiskWriter：<code>DiskBlockObjectWriter</code>，而它通过BlockManaged得到。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getDiskWriter</span></span>(</span><br><span class="line">  blockId: <span class="type">BlockId</span>,</span><br><span class="line">  file: <span class="type">File</span>,</span><br><span class="line">  serializerInstance: <span class="type">SerializerInstance</span>,</span><br><span class="line">  bufferSize: <span class="type">Int</span>,</span><br><span class="line">  writeMetrics: <span class="type">ShuffleWriteMetrics</span>): <span class="type">DiskBlockObjectWriter</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> syncWrites = conf.getBoolean(<span class="string">"spark.shuffle.sync"</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">DiskBlockObjectWriter</span>(file, serializerManager, serializerInstance, bufferSize,</span><br><span class="line">                            syncWrites, writeMetrics, blockId)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 通用流程</span></span><br><span class="line"><span class="comment">// 1. 得到 DiskWriter</span></span><br><span class="line"><span class="keyword">val</span> writer = blockManager.getDiskWriter(xxx)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 将数据写入DiskWriter</span></span><br><span class="line">writer.write(xxx)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 提交并返回偏移和大小</span></span><br><span class="line"><span class="keyword">val</span> fileSegment = writer.commitAndGet()</span><br></pre></td></tr></table></figure><p>主要有下列情况</p><ul><li><p><strong>ShuffleSpill</strong>：BlockId：<code>temp_shuffle_ + “randomUUID”</code></p></li><li><p><strong>BypassShuffle</strong><a href="https://zouxxyy.github.io/2019/11/30/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BBypassMergeSortShuffleWriter/" target="_blank" rel="noopener">前3步</a>：BlockId：<code>temp_shuffle_ + “randomUUID”</code></p></li><li><p><strong>ShuffleWrite</strong>生成的合并大文件：BlockId：<code>&quot;shuffle_&quot; + shuffleId + &quot;_&quot; + mapId + &quot;_&quot; + reduceId</code></p></li></ul><h2 id="get"><a class="header-anchor" href="#get"></a>get</h2><p>有写就有读，与写不同的是：<strong>写只能写本地，读可以读本地和远程</strong>，这个我想应该很好理解。</p><h3 id="RDD-cache-v2"><a class="header-anchor" href="#RDD-cache-v2"></a>RDD cache</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManager</span></span><br><span class="line"><span class="type">ShuffleMapTask</span>/<span class="type">ResultTask</span>.runTask  -&gt; <span class="type">RDD</span>.iterator -&gt; <span class="type">RDD</span>.getOrCompute -&gt; getOrElseUpdate</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager内部</span></span><br><span class="line">getOrElseUpdate -&gt; get -&gt; getLocalValues</span><br><span class="line">                       -&gt; getRemoteValues -&gt; getRemoteBytes -&gt; 反序列</span><br><span class="line"></span><br><span class="line"><span class="comment">// 细看下 getLocalValues 的流程</span></span><br><span class="line">level.useMemory  -<span class="literal">true</span>--&gt; level.deserialized -<span class="literal">true</span>--&gt; memoryStore.getValues </span><br><span class="line">                                             -<span class="literal">false</span>-&gt; memoryStore.getBytes  -&gt; 反序列</span><br><span class="line">                 -<span class="literal">false</span>-&gt; level.useDisk      -<span class="literal">true</span>--&gt; diskStore.getBytes    -&gt; 反序列</span><br></pre></td></tr></table></figure><h3 id="Broadcast-v2"><a class="header-anchor" href="#Broadcast-v2"></a>Broadcast</h3><p>广播变量的读取比较简单</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManager</span></span><br><span class="line"><span class="type">TorrentBroadcast</span>.readBroadcastBlock -&gt; getLocalValues</span><br></pre></td></tr></table></figure><h3 id="RDD-block-Replication-v2"><a class="header-anchor" href="#RDD-block-Replication-v2"></a>RDD block Replication</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 高层调用BlockManage，只有一类</span></span><br><span class="line"><span class="type">NettyBlockRpcServer</span>.receive -&gt; <span class="keyword">match</span> openBlocks -&gt; getBlockData</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager内部</span></span><br><span class="line">getBlockData -&gt; getLocalBytes</span><br></pre></td></tr></table></figure><h3 id="Shuffle-相关-v2"><a class="header-anchor" href="#Shuffle-相关-v2"></a>Shuffle 相关</h3><p>shuffle read 阶段：<code>BlockStoreShuffleReader</code> 核心是<code>ShuffleBlockFetcherIterator</code></p><p>现在可以理解它们名字的含义了，就是去fetch write阶段写下的block，</p><p>分本地<code>localBlocks</code> 和远程  <code>remoteBlocks</code></p><p>这部分内容较复杂，需结合rpc，先pass</p><h1 id="小结"><a class="header-anchor" href="#小结"></a>小结</h1><p>BlockManager 块管理者，使用它时，直接get、put即可；而之前对其底层结构的了解这一过程还是很有意思的，同时使我对spark有了更深的认识。</p><p>至此，spark文件系统告一段落，以后学了rpc再回来看文件系统的rpc。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;每台节点（driver and executors）的block的总管理者，主要功能就是在本地或者远程的store中(堆内内存、磁盘、堆外内存) put、get、 block。&lt;/p&gt;
&lt;p&gt;作为总管理者，BlockManager 依赖众多对象。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-cache和persist</title>
    <link href="http://yoursite.com/2020/01/11/spark/spark%E6%BA%90%E7%A0%81-cache%E5%92%8Cpersist/"/>
    <id>http://yoursite.com/2020/01/11/spark/spark源码-cache和persist/</id>
    <published>2020-01-11T13:40:44.000Z</published>
    <updated>2020-01-13T08:20:11.823Z</updated>
    
    <content type="html"><![CDATA[<p>介绍spark的缓存功能</p><a id="more"></a><h1 id="简介"><a class="header-anchor" href="#简介"></a>简介</h1><p>复杂的任务中，某个中间转换结果可能会被多次调用，此时可以使用 spark 的缓存功能，将计算的中间过程缓存在内存或者磁盘中，以便再次使用，减少不必要的计算。</p><h1 id="特点"><a class="header-anchor" href="#特点"></a>特点</h1><ul><li>懒加载，只有RDD触发action时才会进行计算并且缓存</li><li>5个参数控制存储级别：是否用内存缓存、 是否用磁盘缓存、是否用堆外内存缓存、是否序列化、缓存个数</li><li>cache() 是 persist() 也是 persist(StorageLevel.MEMORY_ONLY)</li><li>api:   <code>rdd.cache()</code> or <code>rdd.persist()</code>、<code>rdd.unpersist()</code>、<code>sc.getPersistentRDDs</code></li></ul><p>接着，以MEMORY_ONLY模式为例，从源码中验证这一切，同时加深对 <strong>spark内存管理</strong> 的理解</p><h1 id="流程"><a class="header-anchor" href="#流程"></a>流程</h1><h2 id="cache-or-persist-…"><a class="header-anchor" href="#cache-or-persist-…"></a>cache() or persist(…)</h2><p>解决一个疑问：<code>rdd1.cache()  rdd1 -&gt; rdd2 rdd2.cahce()</code> ，此时 rdd1 和 rdd2 都会被缓存</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// cache() 等同于 persist() 等同于 persist(StorageLevel.MEMORY_ONLY) ，也就是仅缓存于存储内存中。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 缓存级别，由5个参数组成</span></span><br><span class="line"><span class="keyword">new</span> <span class="type">StorageLevel</span>(useDisk, useMemory, useOffHeap, deserialized, replication))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  <span class="comment">// isLocallyCheckpointed 方法 判断该RDD是否已经标记为 checkpoint，注意不是cache</span></span><br><span class="line">  <span class="keyword">if</span> (isLocallyCheckpointed) &#123;</span><br><span class="line">    persist(<span class="type">LocalRDDCheckpointData</span>.transformStorageLevel(newLevel), allowOverride = <span class="literal">true</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    persist(newLevel, allowOverride = <span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>, allowOverride: <span class="type">Boolean</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  <span class="comment">// 可以发现当前版本并不支持改变已缓存 RDD 的 StorageLevel (注意RDD1转成RDD2后自然可以改变)</span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span> &amp;&amp; newLevel != storageLevel &amp;&amp; !allowOverride) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(</span><br><span class="line">      <span class="string">"Cannot change storage level of an RDD after it was already assigned a level"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 仅第一次缓存时触发 </span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">    sc.cleaner.foreach(_.registerRDDForCleanup(<span class="keyword">this</span>)) <span class="comment">// 用于 cleanups</span></span><br><span class="line">    <span class="comment">// 写入 persistentRdds，它是一个map（rdd.id, rdd），可调用 sc.getPersistentRDDs 得到</span></span><br><span class="line">    sc.persistRDD(<span class="keyword">this</span>) </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 设置 storageLevel，之前默认为StorageLevel.NONE</span></span><br><span class="line">  storageLevel = newLevel</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="getOrCompute-…"><a class="header-anchor" href="#getOrCompute-…"></a>getOrCompute(…)</h2><p>cache() 给 RDD埋了一个属性<code>storageLevel</code>，只有执行行动操作才会真正执行缓存</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// RDD的iterator，由于RDD本身懒加载，只要行动操作才会执行</span></span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="comment">// 计算前先检查 storageLevel 是否不为NONE</span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">    getOrCompute(split, context) <span class="comment">// 核心</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    computeOrReadCheckpoint(split, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">getOrCompute</span></span>(partition: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> blockId = <span class="type">RDDBlockId</span>(id, partition.index)</span><br><span class="line">  <span class="keyword">var</span> readCachedBlock = <span class="literal">true</span></span><br><span class="line">  <span class="comment">// 调用blockManager 的 getOrElseUpdate方法，取出或者生成该blockId对应的block数据，返回blockResult</span></span><br><span class="line">  <span class="type">SparkEnv</span>.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () =&gt; &#123;</span><br><span class="line">    <span class="comment">// 该函数变量仅生成block时执行</span></span><br><span class="line">    readCachedBlock = <span class="literal">false</span></span><br><span class="line">    computeOrReadCheckpoint(partition, context)</span><br><span class="line">  &#125;) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Left</span>(blockResult) =&gt;</span><br><span class="line">    <span class="comment">// 读 cache ：会计数</span></span><br><span class="line">    <span class="keyword">if</span> (readCachedBlock) &#123;</span><br><span class="line">      <span class="keyword">val</span> existingMetrics = context.taskMetrics().inputMetrics</span><br><span class="line">      existingMetrics.incBytesRead(blockResult.bytes)</span><br><span class="line">      <span class="comment">// blockResul.data 得到迭代器，封装成 InterruptibleIterator，结束！</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>[<span class="type">T</span>](context, blockResult.data.asInstanceOf[<span class="type">Iterator</span>[<span class="type">T</span>]]) &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">T</span> = &#123;</span><br><span class="line">          existingMetrics.incRecordsRead(<span class="number">1</span>)</span><br><span class="line">          delegate.next()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 生成 cache：compute 内有自己的计数，这里就不用处理</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, blockResult.data.asInstanceOf[<span class="type">Iterator</span>[<span class="type">T</span>]])</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Right</span>(iter) =&gt;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, iter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">T</span>]])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="getOrElseUpdate-…"><a class="header-anchor" href="#getOrElseUpdate-…"></a>getOrElseUpdate(…)</h2><p><code>getOrElseUpdate</code> 是<code>BlockManager </code>中的方法</p><p>函数参数<code>makeIterator</code>就是我们的<code>computeOrReadCheckpoint</code>方法</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOrElseUpdate</span></span>[<span class="type">T</span>](</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    level: <span class="type">StorageLevel</span>,</span><br><span class="line">    classTag: <span class="type">ClassTag</span>[<span class="type">T</span>],</span><br><span class="line">    makeIterator: () =&gt; <span class="type">Iterator</span>[<span class="type">T</span>]): <span class="type">Either</span>[<span class="type">BlockResult</span>, <span class="type">Iterator</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">  <span class="comment">// 调用 get[T](blockId) 方法，从各种 store 中找该 block</span></span><br><span class="line">  get[<span class="type">T</span>](blockId)(classTag) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="comment">// 找到了表明有缓存，直接返回找到的 BlockResult</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(block) =&gt;</span><br><span class="line">      <span class="keyword">return</span> <span class="type">Left</span>(block)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="comment">// 没缓存，继续执行下一步</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Initially we hold no locks on this block.</span></span><br><span class="line">  <span class="comment">// 调用 doPutIterator，将 makeIterator 计算的结果存入Block（逻辑概念）中（其实是存入各种store中）</span></span><br><span class="line">  <span class="comment">// 写数据时加 读锁</span></span><br><span class="line">  doPutIterator(blockId, makeIterator, level, classTag, keepReadLock = <span class="literal">true</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      <span class="comment">// doPut() 正常情况返回None，接着调用 getLocalValues 读取刚写的 block，返回 blockResult</span></span><br><span class="line">      <span class="keyword">val</span> blockResult = getLocalValues(blockId).getOrElse &#123;</span><br><span class="line">        releaseLock(blockId)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"get() failed for block <span class="subst">$blockId</span> even though we held a lock"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 放锁</span></span><br><span class="line">      releaseLock(blockId)</span><br><span class="line">      <span class="type">Left</span>(blockResult)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(iter) =&gt;</span><br><span class="line">     <span class="type">Right</span>(iter)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="补充"><a class="header-anchor" href="#补充"></a>补充</h1><h2 id="persistentRdds"><a class="header-anchor" href="#persistentRdds"></a>persistentRdds</h2><p>一个map ：key 为 <a href="http://rdd.id" target="_blank" rel="noopener">rdd.id</a> ，value 为 rdd</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">val</span> persistentRdds = &#123;</span><br><span class="line">    <span class="keyword">val</span> map: <span class="type">ConcurrentMap</span>[<span class="type">Int</span>, <span class="type">RDD</span>[_]] = <span class="keyword">new</span> <span class="type">MapMaker</span>().weakValues().makeMap[<span class="type">Int</span>, <span class="type">RDD</span>[_]]()</span><br><span class="line">    map.asScala</span><br><span class="line">  &#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPersistentRDDs</span></span>: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">RDD</span>[_]] = persistentRdds.toMap</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用户使用该方法得到它</span></span><br><span class="line"><span class="keyword">val</span> ds: collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">RDD</span>[_]] = sc.getPersistentRDDs</span><br></pre></td></tr></table></figure><h2 id="unpersist"><a class="header-anchor" href="#unpersist"></a>unpersist()</h2><p>取消缓存</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">unpersistRDD</span></span>(rddId: <span class="type">Int</span>, blocking: <span class="type">Boolean</span> = <span class="literal">true</span>) &#123;</span><br><span class="line">  <span class="comment">// 通知blockManager删掉属于该RDD的全部block</span></span><br><span class="line">  env.blockManager.master.removeRdd(rddId, blocking)</span><br><span class="line">  <span class="comment">// 从map中移掉它</span></span><br><span class="line">  persistentRdds.remove(rddId)</span><br><span class="line">  listenerBus.post(<span class="type">SparkListenerUnpersistRDD</span>(rddId))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="小结"><a class="header-anchor" href="#小结"></a>小结</h1><p>当你对spark的存储有一点理解时，本节相对简单。缓存就是将RDD的<code>storageLevel</code>属性改写，并把该RDD加入<code>persistentRdds</code>这个map中。当执行到<code>iterator</code>时触发，如果没有缓存过，则进行计算并写入BLock中，有缓存直接从BLock中提取即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍spark的缓存功能&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-block</title>
    <link href="http://yoursite.com/2020/01/11/spark/spark%E6%BA%90%E7%A0%81-block/"/>
    <id>http://yoursite.com/2020/01/11/spark/spark源码-block/</id>
    <published>2020-01-11T13:40:28.000Z</published>
    <updated>2020-01-13T02:08:02.512Z</updated>
    
    <content type="html"><![CDATA[<p><strong>首先明确spark有个自己文件系统，block就是里面的一个文件</strong>。如：缓存后<strong>的RDD的一个分区是一个block；计算产生的</strong>临时文件<strong>也是block</strong>。<strong>任何你要存的东西都是block</strong>。</p><p>因此既然它是文件，它就有<strong>文件名</strong>、<strong>元信息</strong>、<strong>锁</strong>、<strong>数据</strong>，理解它们，你就理解了block！</p><a id="more"></a><h1 id="BlockId"><a class="header-anchor" href="#BlockId"></a>BlockId</h1><p>每个Block都有个ID与之一一对应，这个<strong>BlockId也就是该文件的文件名</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">sealed</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BlockId</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// RDDBlockId、ShuffleBlockId什么的都是它的子类</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">asRDDId</span></span>: <span class="type">Option</span>[<span class="type">RDDBlockId</span>] = <span class="keyword">if</span> (isRDD) <span class="type">Some</span>(asInstanceOf[<span class="type">RDDBlockId</span>]) <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isRDD</span></span>: <span class="type">Boolean</span> = isInstanceOf[<span class="type">RDDBlockId</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isShuffle</span></span>: <span class="type">Boolean</span> = isInstanceOf[<span class="type">ShuffleBlockId</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isBroadcast</span></span>: <span class="type">Boolean</span> = isInstanceOf[<span class="type">BroadcastBlockId</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = name</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>理解了取名的规则，你就对spark的计算流程理解了。例如：</p><p>todo：理解剩余的 BlockId</p><p><strong>普通RDD</strong>：</p><ul><li>RDDBlockId <code>&quot;rdd_&quot; + rddId + &quot;_&quot; + splitIndex</code></li></ul><p>简单的按 rddId 和 分区ID 划分</p><p><strong>Shuffle过程</strong>：</p><ul><li>ShuffleBlockId <code>&quot;shuffle_&quot; + shuffleId + &quot;_&quot; + mapId + &quot;_&quot; + reduceId</code></li><li>ShuffleDataBlockId <code>&quot;shuffle_&quot; + shuffleId + &quot;_&quot; + mapId + &quot;_&quot; + reduceId + &quot;.data&quot;</code></li><li>ShuffleIndexBlockId <code>&quot;shuffle_&quot; + shuffleId + &quot;_&quot; + mapId + &quot;_&quot; + reduceId + &quot;.index&quot;</code></li></ul><p><strong>ShuffleBlockId 就是ShuffleWrite时单个task生成的那个大文件，ShuffleDataBlockId是这个大文件的临时叫法，ShuffleIndexBlockId是索引文件</strong>。而它们由<strong>shuffleId、mapId、reduceId</strong> 3者共同确定。</p><p>注意：<strong>reduceId是0（<code>NOOP_REDUCE_ID</code>）</strong>，因为<strong>SortShuffle最后生成的是1个大文件</strong>，所以这个reduceid没什么软用。理解清楚SortShuffleManager和被淘汰的HashShuffleManager的区别，骚年。</p><p><strong>临时文件</strong>：</p><ul><li><p>TempLocalBlockId <code>temp_local_ + “randomUUID”</code></p></li><li><p>TempShuffleBlockId <code>temp_shuffle_ + “randomUUID”</code></p></li></ul><p><strong>TempLocalBlockId是本地计算中间临时文件</strong></p><p><strong>TempShuffleBlockId是shuffle计算中间临时文件</strong>：1、BypassShuffle 单分区临时文件；2、shuffle排序时如果内存不够发生spill到磁盘的文件。</p><h1 id="BlockInfo"><a class="header-anchor" href="#BlockInfo"></a>BlockInfo</h1><p>block的元信息如下</p><p>构造参数：</p><ul><li><strong>StorageLevel</strong>：存储等级</li><li><strong>classTag</strong>：类名，用于序列化需求</li><li><strong>tellMaster</strong>：是否需要通知master block改变，大多数都要，但是broadcast blocks不用</li></ul><p>内部属性（可get、set）后面两参数用于实现锁机制：</p><ul><li><strong>size</strong>: block的大小（bytes）</li><li><strong>readerCount</strong>： 目前有多少个Task在读它</li><li><strong>writerTask</strong>：<strong>持有该块写锁的Task ID</strong>。默认值：BlockInfo.NO_WRITER，代表没有人写</li></ul><p>在set值后，会调用checkInvariants方法，检查值是否合格。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkInvariants</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 可多人读</span></span><br><span class="line">  assert(_readerCount &gt;= <span class="number">0</span>)</span><br><span class="line">  <span class="comment">// 读写互斥</span></span><br><span class="line">  assert(_readerCount == <span class="number">0</span> || _writerTask == <span class="type">BlockInfo</span>.<span class="type">NO_WRITER</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="BlockInfoManager"><a class="header-anchor" href="#BlockInfoManager"></a>BlockInfoManager</h1><p>它用于控制块的元信息，以<strong>实现块的锁机制</strong>，一个经典的文件读写锁机制。</p><p>需求：</p><ul><li>同一个文件可以多个人读，前提没人写；且读锁可重入</li><li>只能有一个人写文件，且没人读 ；且写锁不可重入</li><li>当拿不到锁，可以选择一直等待（实现方法：拿不到锁wait，放锁notify）</li></ul><h2 id="成员"><a class="header-anchor" href="#成员"></a>成员</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 换个别名</span></span><br><span class="line"><span class="comment">// 理解：一个task一个线程，锁的就它</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">type</span> <span class="title">TaskAttemptId</span> </span>= <span class="type">Long</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockId 和 BlockInfo的映射</span></span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> infos = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">BlockId</span>, <span class="type">BlockInfo</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 任务ID 和 它拥有的写锁</span></span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> writeLocksByTask =</span><br><span class="line"><span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TaskAttemptId</span>, mutable.<span class="type">Set</span>[<span class="type">BlockId</span>]]</span><br><span class="line"><span class="keyword">with</span> mutable.<span class="type">MultiMap</span>[<span class="type">TaskAttemptId</span>, <span class="type">BlockId</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 任务ID 和 它拥有的读锁</span></span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> readLocksByTask =</span><br><span class="line"><span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TaskAttemptId</span>, <span class="type">ConcurrentHashMultiset</span>[<span class="type">BlockId</span>]]</span><br></pre></td></tr></table></figure><p>writeLocksByTask的数据结构的写法真是<a href="https://www.scala-lang.org/api/current/scala/collection/mutable/MultiMap.html" target="_blank" rel="noopener">活久见</a>，使用<strong>有重复值的set</strong>实现<strong>可重入锁</strong>。</p><h2 id="难点方法"><a class="header-anchor" href="#难点方法"></a>难点方法</h2><h3 id="registerTask"><a class="header-anchor" href="#registerTask"></a>registerTask</h3><p>给<code>readLocksByTask</code>添加映射，值自然是空的</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">registerTask</span></span>(taskAttemptId: <span class="type">TaskAttemptId</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">  require(!readLocksByTask.contains(taskAttemptId),</span><br><span class="line">          <span class="string">s"Task attempt <span class="subst">$taskAttemptId</span> is already registered"</span>)</span><br><span class="line">  readLocksByTask(taskAttemptId) = <span class="type">ConcurrentHashMultiset</span>.create()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意一个特殊的TaskId: <code>NON_TASK_WRITER</code>，它在<strong>BlockInfoManager初始化时被注册</strong>，非任务线程，如driver线程。</p><h3 id="lockNewBlockForWriting"><a class="header-anchor" href="#lockNewBlockForWriting"></a>lockNewBlockForWriting</h3><p>创建新的block时，自然需要<strong>加写锁</strong>，它用到了<code>lockForReading</code>和<code>lockForWriting</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lockNewBlockForWriting</span></span>(</span><br><span class="line">  blockId: <span class="type">BlockId</span>,</span><br><span class="line">  newBlockInfo: <span class="type">BlockInfo</span>): <span class="type">Boolean</span> = synchronized &#123;</span><br><span class="line">  logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> trying to put <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  <span class="comment">// 由于有多线程的存在不能直接执行操作，而是先判断读锁，防止别的兄弟先一步已经lockNewBlockForWriting了</span></span><br><span class="line">  lockForReading(blockId) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(info) =&gt;</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">    <span class="comment">// 给 infos 加映射</span></span><br><span class="line">    infos(blockId) = newBlockInfo</span><br><span class="line">    <span class="comment">// 上写锁</span></span><br><span class="line">    lockForWriting(blockId)</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="lockForWriting"><a class="header-anchor" href="#lockForWriting"></a>lockForWriting</h3><p>加写锁</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lockForWriting</span></span>(</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    blocking: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">Option</span>[<span class="type">BlockInfo</span>] = synchronized &#123;</span><br><span class="line">  logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> trying to acquire write lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  do &#123;</span><br><span class="line">    infos.get(blockId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">return</span> <span class="type">None</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(info) =&gt;</span><br><span class="line">      <span class="comment">// 当该block没有人读，且没有人写时才能锁它，并返回</span></span><br><span class="line">        <span class="keyword">if</span> (info.writerTask == <span class="type">BlockInfo</span>.<span class="type">NO_WRITER</span> &amp;&amp; info.readerCount == <span class="number">0</span>) &#123;</span><br><span class="line">          info.writerTask = currentTaskAttemptId</span><br><span class="line">          writeLocksByTask.addBinding(currentTaskAttemptId, blockId)</span><br><span class="line">          logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> acquired write lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">          <span class="keyword">return</span> <span class="type">Some</span>(info)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果之前没拿到锁，就wait()，blocking控制是否等待</span></span><br><span class="line">    <span class="keyword">if</span> (blocking) &#123;</span><br><span class="line">      wait()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">while</span> (blocking)</span><br><span class="line">  <span class="type">None</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="lockForReading"><a class="header-anchor" href="#lockForReading"></a>lockForReading</h3><p>加读锁</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lockForReading</span></span>(</span><br><span class="line">  blockId: <span class="type">BlockId</span>,</span><br><span class="line">  blocking: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">Option</span>[<span class="type">BlockInfo</span>] = synchronized &#123;</span><br><span class="line">  logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> trying to acquire read lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  do &#123;</span><br><span class="line">    infos.get(blockId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">return</span> <span class="type">None</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(info) =&gt;</span><br><span class="line">      <span class="comment">// 没有人写，即可加读锁，并返回</span></span><br><span class="line">      <span class="keyword">if</span> (info.writerTask == <span class="type">BlockInfo</span>.<span class="type">NO_WRITER</span>) &#123;</span><br><span class="line">        info.readerCount += <span class="number">1</span></span><br><span class="line">        readLocksByTask(currentTaskAttemptId).add(blockId)</span><br><span class="line">        logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> acquired read lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="type">Some</span>(info)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 同上</span></span><br><span class="line">    <span class="keyword">if</span> (blocking) &#123;</span><br><span class="line">      wait()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">while</span> (blocking)</span><br><span class="line">  <span class="type">None</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="unlock"><a class="header-anchor" href="#unlock"></a>unlock</h3><p>放锁</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unlock</span></span>(blockId: <span class="type">BlockId</span>, taskAttemptId: <span class="type">Option</span>[<span class="type">TaskAttemptId</span>] = <span class="type">None</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">  <span class="keyword">val</span> taskId = taskAttemptId.getOrElse(currentTaskAttemptId)</span><br><span class="line">  logTrace(<span class="string">s"Task <span class="subst">$taskId</span> releasing lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  <span class="keyword">val</span> info = get(blockId).getOrElse &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Block <span class="subst">$blockId</span> not found"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 放写锁</span></span><br><span class="line">  <span class="keyword">if</span> (info.writerTask != <span class="type">BlockInfo</span>.<span class="type">NO_WRITER</span>) &#123;</span><br><span class="line">    info.writerTask = <span class="type">BlockInfo</span>.<span class="type">NO_WRITER</span></span><br><span class="line">    writeLocksByTask.removeBinding(taskId, blockId)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    assert(info.readerCount &gt; <span class="number">0</span>, <span class="string">s"Block <span class="subst">$blockId</span> is not locked for reading"</span>)</span><br><span class="line">    <span class="comment">// 放读锁</span></span><br><span class="line">    info.readerCount -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">val</span> countsForTask = readLocksByTask(taskId)</span><br><span class="line">    <span class="comment">// remove(blockId, n) 删除重复set中指定元素 n 个 。此处自然只删一个</span></span><br><span class="line">    <span class="keyword">val</span> newPinCountForTask: <span class="type">Int</span> = countsForTask.remove(blockId, <span class="number">1</span>) - <span class="number">1</span></span><br><span class="line">    assert(newPinCountForTask &gt;= <span class="number">0</span>,</span><br><span class="line">      <span class="string">s"Task <span class="subst">$taskId</span> release lock on block <span class="subst">$blockId</span> more times than it acquired it"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  notifyAll()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="downgradeLock"><a class="header-anchor" href="#downgradeLock"></a>downgradeLock</h3><p>由写锁降为读锁。todo：了解这玩意有啥用</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">downgradeLock</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">  logTrace(<span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> downgrading write lock for <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  <span class="keyword">val</span> info = get(blockId).get</span><br><span class="line">  require(info.writerTask == currentTaskAttemptId,</span><br><span class="line">    <span class="string">s"Task <span class="subst">$currentTaskAttemptId</span> tried to downgrade a write lock that it does not hold on"</span> +</span><br><span class="line">      <span class="string">s" block <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  <span class="comment">// 放锁</span></span><br><span class="line">  unlock(blockId)</span><br><span class="line">  <span class="comment">// 加读锁，注意blocking为false</span></span><br><span class="line">  <span class="keyword">val</span> lockOutcome = lockForReading(blockId, blocking = <span class="literal">false</span>)</span><br><span class="line">  assert(lockOutcome.isDefined)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="BlockResult"><a class="header-anchor" href="#BlockResult"></a>BlockResult</h1><p>调用 <code>getLocalValues(blockId)</code> 或者 <code>getRemoteValues(blockId)</code> 时，以java对象的形式返回block。</p><p>注意，<strong>数据是存在store中的</strong>，如<code>memoryStore</code>，调它的API取出数据封装成BlockResult。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">BlockResult</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val data: <span class="type">Iterator</span>[<span class="type">Any</span>],    // 数据，从memoryStore中取出</span></span></span><br><span class="line"><span class="class"><span class="params">    val readMethod: <span class="type">DataReadMethod</span>.<span class="type">Value</span>, // <span class="type">Memory</span>, <span class="type">Disk</span>, <span class="type">Hadoop</span>, <span class="type">Network</span></span></span></span><br><span class="line"><span class="class"><span class="params">    val bytes: <span class="type">Long</span></span>)            <span class="title">//</span> <span class="title">大小，从info中取出</span></span></span><br></pre></td></tr></table></figure><h1 id="BlockData"><a class="header-anchor" href="#BlockData"></a>BlockData</h1><p>调用 <code>getLocalBytes(blockId)</code> 或者 <code>getRemoteBytes(blockId)</code> 时，以序列化字节的形式返回block。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">BlockData</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toInputStream</span></span>(): <span class="type">InputStream</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toNetty</span></span>(): <span class="type">Object</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toChunkedByteBuffer</span></span>(allocator: <span class="type">Int</span> =&gt; <span class="type">ByteBuffer</span>): <span class="type">ChunkedByteBuffer</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toByteBuffer</span></span>(): <span class="type">ByteBuffer</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">size</span></span>: <span class="type">Long</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(): <span class="type">Unit</span>   <span class="comment">// 多了一个销毁</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它有3个实现类，<code>ByteBufferBlockData</code>、<code>DiskBlockData</code>，以及<code>EncryptedBlockData</code>。</p><p>以<code>ByteBufferBlockData</code>为例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ByteBufferBlockData</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val buffer: <span class="type">ChunkedByteBuffer</span>,     // 数据，从memoryStore中取出</span></span></span><br><span class="line"><span class="class"><span class="params">    val shouldDispose: <span class="type">Boolean</span></span>) <span class="keyword">extends</span> <span class="title">BlockData</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 下面4个是选择以什么形式提取</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toInputStream</span></span>(): <span class="type">InputStream</span> = buffer.toInputStream(dispose = <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toNetty</span></span>(): <span class="type">Object</span> = buffer.toNetty</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toChunkedByteBuffer</span></span>(allocator: <span class="type">Int</span> =&gt; <span class="type">ByteBuffer</span>): <span class="type">ChunkedByteBuffer</span> = &#123;</span><br><span class="line">    buffer.copy(allocator)&#125;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toByteBuffer</span></span>(): <span class="type">ByteBuffer</span> = buffer.toByteBuffer</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">size</span></span>: <span class="type">Long</span> = buffer.size</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (shouldDispose) &#123;</span><br><span class="line">      buffer.dispose()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="小结"><a class="header-anchor" href="#小结"></a>小结</h1><p>我觉得对Block的认识<strong>非常非常重要</strong>，它是spark内部文件系统的基石。</p><p>对知识梳理一波：</p><p>Store：真正存数据，如MemoryStore、DiskStore</p><p>MemoryManager: 划分内存</p><p>Block：文件，有<strong>文件名</strong>（blockId）、<strong>元信息</strong>(blockInfo)、<strong>锁</strong>(blockInfoManager)、<strong>数据</strong>（从Store中取）</p><p>BlockManager：对Block进行管理（下一节）</p><p>RPC：各种master、manager（以后再学）</p><p>这当真是一个完整的<strong>分布式多人文件系统</strong>！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;首先明确spark有个自己文件系统，block就是里面的一个文件&lt;/strong&gt;。如：缓存后&lt;strong&gt;的RDD的一个分区是一个block；计算产生的&lt;/strong&gt;临时文件&lt;strong&gt;也是block&lt;/strong&gt;。&lt;strong&gt;任何你要存的东西都是block&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;因此既然它是文件，它就有&lt;strong&gt;文件名&lt;/strong&gt;、&lt;strong&gt;元信息&lt;/strong&gt;、&lt;strong&gt;锁&lt;/strong&gt;、&lt;strong&gt;数据&lt;/strong&gt;，理解它们，你就理解了block！&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-DiskBlockManager和DiskStore</title>
    <link href="http://yoursite.com/2020/01/11/spark/spark%E6%BA%90%E7%A0%81-DiskBlockManager%E5%92%8CDiskStore/"/>
    <id>http://yoursite.com/2020/01/11/spark/spark源码-DiskBlockManager和DiskStore/</id>
    <published>2020-01-11T13:40:04.000Z</published>
    <updated>2020-01-13T02:04:19.550Z</updated>
    
    <content type="html"><![CDATA[<p>本文将磁盘文件的存储和管理，注意设计两个类 DiskBlockManager 和 DiskStore</p><a id="more"></a><h1 id="DiskBlockManager"><a class="header-anchor" href="#DiskBlockManager"></a>DiskBlockManager</h1><p>建立BlockID和磁盘中真实文件的一一映射，同时负责创建文件夹与删除文件夹</p><h2 id="成员"><a class="header-anchor" href="#成员"></a>成员</h2><ul><li><p>subDirsPerLocalDir：每个LOCAL_DIRS下目录的最大个数，<code>spark.diskStore.subDirectories</code>（默认64）</p></li><li><p>localDirs：本地目录数组，由<code>createLocalDirs()</code>创建。</p></li><li><p>subDirs：2维数组<code>[localDirs个数][subDirsPerLocalDir 64]</code>，相当于人为把上面的目录按hash散列开，2级目录用于防止出现顶层inodes个数过多。（block文件就在该目录下）</p></li></ul><p>下面以我的spark on yarn集群为例：</p><p>localDirs ：<code>root + &quot;blockmgr&quot; + &quot;-&quot; + UUID.randomUUID</code>，其中 root 由<code>spark.local.dir</code>指定（可多个），默认是 <code>/tmp</code>。总算知道这玩意是干嘛的了，很明显该目录对应的<strong>磁盘对性能要求高！</strong></p><p>subDirs：可以看到散列了5个子文件夹</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[xxx tmp]$ tree ./blockmgr*</span><br><span class="line">./blockmgr-37c233af-359c-4bcd-8dcc-5f216e55ce7d</span><br><span class="line">├── 0c</span><br><span class="line">├── 0d</span><br><span class="line">├── 0e</span><br><span class="line">├── 11</span><br><span class="line">└── 13</span><br></pre></td></tr></table></figure><ul><li>shutdownHook：给<code>ShutdownHookManager</code>添加一个钩子，调用doStop，递归删除localDirs下的文件</li></ul><h2 id="难点方法"><a class="header-anchor" href="#难点方法"></a>难点方法</h2><h3 id="getFile"><a class="header-anchor" href="#getFile"></a>getFile</h3><p>根据名字或者blockID返回磁盘中的File(subDir, filename)，如果没有就创建</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getFile</span></span>(filename: <span class="type">String</span>): <span class="type">File</span> = &#123;</span><br><span class="line">  <span class="comment">// 先hash取余 dirId， 再hash取余 subDirId</span></span><br><span class="line">  <span class="keyword">val</span> hash = <span class="type">Utils</span>.nonNegativeHash(filename)</span><br><span class="line">  <span class="keyword">val</span> dirId = hash % localDirs.length</span><br><span class="line">  <span class="keyword">val</span> subDirId = (hash / localDirs.length) % subDirsPerLocalDir</span><br><span class="line">  <span class="comment">// 如果</span></span><br><span class="line">  <span class="keyword">val</span> subDir = subDirs(dirId).synchronized &#123;</span><br><span class="line">    <span class="keyword">val</span> old = subDirs(dirId)(subDirId)</span><br><span class="line">    <span class="keyword">if</span> (old != <span class="literal">null</span>) &#123;</span><br><span class="line">      old</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 子目录格式：16进制2位符</span></span><br><span class="line">      <span class="keyword">val</span> newDir = <span class="keyword">new</span> <span class="type">File</span>(localDirs(dirId), <span class="string">"%02x"</span>.format(subDirId))</span><br><span class="line">      <span class="keyword">if</span> (!newDir.exists() &amp;&amp; !newDir.mkdir()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">s"Failed to create local dir in <span class="subst">$newDir</span>."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      subDirs(dirId)(subDirId) = newDir</span><br><span class="line">      newDir</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">File</span>(subDir, filename)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getFile</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">File</span> = getFile(blockId.name)</span><br></pre></td></tr></table></figure><h3 id="createTempXXXBlock"><a class="header-anchor" href="#createTempXXXBlock"></a>createTempXXXBlock</h3><ul><li><p><code>createTempLocalBlock</code>：创建存放本地计算中间临时文件的Block</p><p>blockId =  temp_local_ + “randomUUID”</p></li><li><p><code>createTempShuffleBlock</code>：创建存放shuffle计算中间临时文件（如shuffle排序内存不够发生spill）的Block，blockId =  temp_shuffle_ + “randomUUID”</p></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTempLocalBlock</span></span>(): (<span class="type">TempLocalBlockId</span>, <span class="type">File</span>) = &#123;</span><br><span class="line">  <span class="keyword">var</span> blockId = <span class="keyword">new</span> <span class="type">TempLocalBlockId</span>(<span class="type">UUID</span>.randomUUID())</span><br><span class="line">  <span class="keyword">while</span> (getFile(blockId).exists()) &#123;</span><br><span class="line">    blockId = <span class="keyword">new</span> <span class="type">TempLocalBlockId</span>(<span class="type">UUID</span>.randomUUID())</span><br><span class="line">  &#125;</span><br><span class="line">  (blockId, getFile(blockId)) <span class="comment">// getFile创建目录</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTempShuffleBlock</span></span>(): (<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>) = &#123;</span><br><span class="line">  <span class="keyword">var</span> blockId = <span class="keyword">new</span> <span class="type">TempShuffleBlockId</span>(<span class="type">UUID</span>.randomUUID())</span><br><span class="line">  <span class="keyword">while</span> (getFile(blockId).exists()) &#123;</span><br><span class="line">    blockId = <span class="keyword">new</span> <span class="type">TempShuffleBlockId</span>(<span class="type">UUID</span>.randomUUID())</span><br><span class="line">  &#125;</span><br><span class="line">  (blockId, getFile(blockId)) <span class="comment">// getFile创建目录</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="stop"><a class="header-anchor" href="#stop"></a>stop</h3><p>调用<code>stop()</code>，删除文件</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>() &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="type">ShutdownHookManager</span>.removeShutdownHook(shutdownHook)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">      logError(<span class="string">s"Exception while removing shutdown hook."</span>, e)</span><br><span class="line">  &#125;</span><br><span class="line">  doStop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// deleteFilesOnStop：DiskBlockManager 的 构造参数，决定是否递归删除localDirs下的文件</span></span><br><span class="line">  <span class="comment">// 一般只有使用外部shuffle服务时，它才为false</span></span><br><span class="line">  <span class="keyword">if</span> (deleteFilesOnStop) &#123;</span><br><span class="line">    localDirs.foreach &#123; localDir =&gt;</span><br><span class="line">      <span class="keyword">if</span> (localDir.isDirectory() &amp;&amp; localDir.exists()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (!<span class="type">ShutdownHookManager</span>.hasRootAsShutdownDeleteDir(localDir)) &#123;</span><br><span class="line">            <span class="type">Utils</span>.deleteRecursively(localDir)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">            logError(<span class="string">s"Exception while deleting local spark dir: <span class="subst">$localDir</span>"</span>, e)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="DiskStore"><a class="header-anchor" href="#DiskStore"></a>DiskStore</h1><p>负责将blocks写入磁盘及删改查操作</p><h2 id="成员-v2"><a class="header-anchor" href="#成员-v2"></a>成员</h2><ul><li>minMemoryMapBytes:    使用内存映射（java nio 功能）的最小值<code>spark.storage.memoryMapThreshold</code>默认2M</li><li>maxMemoryMapBytes：使用内存映射的最大值</li><li>blockSizes：一个ConcurrentHashMap，BlockId  -&gt; blockSize</li></ul><h2 id="难点方法-v2"><a class="header-anchor" href="#难点方法-v2"></a>难点方法</h2><h3 id="put"><a class="header-anchor" href="#put"></a>put</h3><p>增</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">put</span></span>(blockId: <span class="type">BlockId</span>)(writeFunc: <span class="type">WritableByteChannel</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (contains(blockId)) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Block <span class="subst">$blockId</span> is already present in the disk store"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  logDebug(<span class="string">s"Attempting to put block <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  <span class="keyword">val</span> startTime = <span class="type">System</span>.currentTimeMillis</span><br><span class="line">  <span class="keyword">val</span> file = diskManager.getFile(blockId) <span class="comment">// 创建文件夹，并返回file</span></span><br><span class="line">  <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">CountingWritableChannel</span>(openForWrite(file)) <span class="comment">// 开channel</span></span><br><span class="line">  <span class="keyword">var</span> threwException: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    writeFunc(out) <span class="comment">// 写入文件夹中</span></span><br><span class="line">    blockSizes.put(blockId, out.getCount) <span class="comment">// 计数size，加入blockSizes map中</span></span><br><span class="line">    threwException = <span class="literal">false</span></span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      out.close()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> ioe: <span class="type">IOException</span> =&gt;</span><br><span class="line">        <span class="keyword">if</span> (!threwException) &#123;</span><br><span class="line">          threwException = <span class="literal">true</span></span><br><span class="line">          <span class="keyword">throw</span> ioe</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">       <span class="keyword">if</span> (threwException) &#123;</span><br><span class="line">        remove(blockId)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> finishTime = <span class="type">System</span>.currentTimeMillis</span><br><span class="line">  logDebug(<span class="string">"Block %s stored as %s file on disk in %d ms"</span>.format(</span><br><span class="line">    file.getName,</span><br><span class="line">    <span class="type">Utils</span>.bytesToString(file.length()),</span><br><span class="line">    finishTime - startTime))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以下是一种调用它的方式</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">putBytes</span></span>(blockId: <span class="type">BlockId</span>, bytes: <span class="type">ChunkedByteBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  put(blockId) &#123; channel =&gt;</span><br><span class="line">    bytes.writeFully(channel)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="getBytes"><a class="header-anchor" href="#getBytes"></a>getBytes</h3><p>查，根据blockId，返回BlockData。</p><p>有两种：<code>DiskBlockData</code>（普通） 以 及<code>EncryptedBlockData</code> （加密）</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getBytes</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">BlockData</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> file = diskManager.getFile(blockId.name)</span><br><span class="line">  <span class="keyword">val</span> blockSize = getSize(blockId)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 分两种，一种是加密的一种是非加密的</span></span><br><span class="line">  securityManager.getIOEncryptionKey() <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(key) =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">EncryptedBlockData</span>(file, blockSize, conf, key)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">DiskBlockData</span>(minMemoryMapBytes, maxMemoryMapBytes, file, blockSize)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="remove"><a class="header-anchor" href="#remove"></a>remove</h3><p>删</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  blockSizes.remove(blockId)</span><br><span class="line">  <span class="keyword">val</span> file = diskManager.getFile(blockId.name)</span><br><span class="line">  <span class="keyword">if</span> (file.exists()) &#123;</span><br><span class="line">    <span class="keyword">val</span> ret = file.delete()</span><br><span class="line">    <span class="keyword">if</span> (!ret) &#123;</span><br><span class="line">      logWarning(<span class="string">s"Error deleting <span class="subst">$&#123;file.getPath()&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    ret</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="小结"><a class="header-anchor" href="#小结"></a>小结</h1><p>DiskBlockManager 用于建立BlockID和磁盘中真实文件的一一映射，同时负责创建文件夹与删除文件夹，学习时可以找到那个文件夹进入看看。</p><p>DiskStore 负责将数据写入磁盘以及后续的读取与删除，相比MemoryStore它更简单。因为MemoryStore是真正的仓库，它需要定义数据结构；而DiskStore的数据是在磁盘的。</p><p>todo: 未来学习下java nio，再学习<code>BlockData</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将磁盘文件的存储和管理，注意设计两个类 DiskBlockManager 和 DiskStore&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-MemoryManager和MemoryStore</title>
    <link href="http://yoursite.com/2020/01/11/spark/spark%E6%BA%90%E7%A0%81-MemoryManager%E5%92%8CMemoryStore/"/>
    <id>http://yoursite.com/2020/01/11/spark/spark源码-MemoryManager和MemoryStore/</id>
    <published>2020-01-11T13:39:34.000Z</published>
    <updated>2020-01-13T01:50:02.542Z</updated>
    
    <content type="html"><![CDATA[<p>它俩的对象和方法较多，需要耐心反复看。</p><a id="more"></a><h1 id="MemoryManager"><a class="header-anchor" href="#MemoryManager"></a>MemoryManager</h1><p>限制存储内存（<code>storage</code>）和执行内存（<code>execution</code>）的大小的管理器，有两种实现：<code>StaticMemoryManager</code> 和 <code>UnifiedMemoryManager</code>。</p><img src="https://i.loli.net/2020/01/11/blD2MIJj1AkOQGv.png" alt="image001.png" style="zoom:67%;"><ul><li><p>一个Executor，一个JVM，一个MemoryManager，多个Task</p></li><li><p>同一Executor的Task可共享堆内堆外内存，而同一个节点的不同Executor的Task只能共享堆外内存</p></li><li><p>只能共享存储内存，计算内存肯定无法共享</p></li></ul><h2 id="核心依赖对象——MemoryPool"><a class="header-anchor" href="#核心依赖对象——MemoryPool"></a>核心依赖对象——MemoryPool</h2><p>它用于统计<strong>每个Executor</strong>内存使用情况的类（它并不是去查内存，只是记录，比如要用内存时就把数字增加，减时就把数字减少），具体实现分两种：<code>StorageMemoryPool</code> 和 <code>ExecutionMemoryPool</code>，它们每个又分堆内和堆外内存。基本方法有：<code>poolSize</code>、<code>memoryUsed</code>、<code>memoryFree(_poolSize - memoryUsed)</code>，看名知意。</p><ul><li><p><code>StorageMemoryPool</code>：除了有统计功能，并且还会在需要的内存不够时，通知<code>MemoryStore</code>调用<code>evictBlocksToFreeSpace</code>驱逐内存（发现不够，会先向执行内存借，如果借不到再驱逐。这部分代码在MemoryManager中）。</p></li><li><p><code>ExecutionMemoryPool</code>：相对内存池复杂点，由于存储内存对于同一个Executor的多个Task来说是<strong>共享</strong>的，而执行内存是<strong>不共享</strong>的。因此，其内部需要<strong>维护一个<code>HashMap</code></strong>，存储每个Task使用的执行内存。</p><p>细节1:  内部的<code>acquireMemory</code> 和 <code>releaseMemory</code> 之间采用 <code>wait()</code> 和 <code>notifyAll()</code>机制 ：在内存不够时<code>acquireMemory</code>会触发 wait，而当<code>releaseMemory</code>时，notifyall。</p><p>细节2:  每个Task<strong>最少到分执行内存的 1 / 2N</strong>，当超过1 / N时，不再允许申请。</p><p>细节3:  <code>acquireMemory</code> 内部有个回调函数<code>maybeGrowPool</code>：当执行内存不足时，从存储内存中借它剩余的，或者要它归还从执行内存中借走的。</p></li></ul><p>从这里也可以发现，执行内存可以要回被借走的，而存储内存不可以，这就是后面要说的<strong>动态占用</strong>规则之一。</p><p>注意：<code>MemoryPool</code>里的操作，都<strong>加了object lock</strong>(<code>MemoryManager</code>)</p><h2 id="UnifiedMemoryManager"><a class="header-anchor" href="#UnifiedMemoryManager"></a>UnifiedMemoryManager</h2><p>spark 1.6 后统一使用<code>UnifiedMemoryManager</code>，它的进步是<strong>存储内存和执行内存可以动态占用</strong>。</p><h3 id="内存划分"><a class="header-anchor" href="#内存划分"></a>内存划分</h3><p><img src="https://i.loli.net/2020/01/11/sBvxNCqUHnKa61c.png" alt="屏幕快照 2020-01-03 下午8.47.52.png"></p><p><strong>堆内内存</strong>（<code>spark.executor.memory</code>  JVM内存）</p><ul><li>存储（storage）内存：（堆内内存 - 预留） * 0.6 <code>spark.memory.fraction</code> * 0.5 <code>spark.memory.storageFraction</code>，用于 <strong>cache、broadcast</strong> 等</li><li>执行（executor）内存：（堆内内存 - 预留） * 0.6 <code>spark.memory.fraction</code> * (1 - 0.5 <code>spark.memory.storageFraction</code>，用于<strong>计算，如shuffles、join、sorts and aggregations</strong></li><li>其它内存：（堆内内存 - 预留） * （1 - 0.6 <code>spark.memory.fraction</code>）</li><li>预留内存：<code>RESERVED_SYSTEM_MEMORY_BYTES</code> 规定 300M</li></ul><p><strong>堆外内存</strong>（<code>spark.memory.offHeap.size</code>）</p><ul><li>存储内存：堆外内存 * 0.5 <code>spark.memory.storageFraction</code></li><li>执行内存：堆外内存 * （1 - 0.5 <code>spark.memory.storageFraction</code>）</li></ul><h3 id="动态占用"><a class="header-anchor" href="#动态占用"></a>动态占用</h3><p><img src="https://i.loli.net/2020/01/11/FHQD3Y1LohgG8Sf.png" alt="image006.png"></p><p>存储内存可以尽可能多地借用执行内存中的free内存，但是当执行池需要这部分内存时，<strong>会把该部分内存池中的对象从内存中驱逐出</strong>，直到满足执行池的内存需求。</p><p>执行内存也可以尽可能多地借用存储内存中的free内存，不同的是，<strong>执行内存不会被存储池驱逐出内存</strong>。</p><p>也就是说，缓存block时可能会因为执行池占用了大量的内存池，不能释放导致<strong>缓存block失败</strong>，在这种情况下，新的block会<strong>根据StorageLevel做相应处理</strong>（如 spill 磁盘或干脆丢弃以前的）。</p><p>注意 <code>MemoryManager </code>里的<code>acquireMemory</code>操作都加了<code>synchronized</code></p><h1 id="MemoryStore"><a class="header-anchor" href="#MemoryStore"></a>MemoryStore</h1><p>负责将blocks写入内存及删改查操作，写入结构为反序列化的Java对象数组，或者序列化的ByteBuffers</p><h2 id="核心依赖对象——MemoryEntry"><a class="header-anchor" href="#核心依赖对象——MemoryEntry"></a>核心依赖对象——MemoryEntry</h2><p>块在内存中的抽象表示，也就是数据在存储内存中存储方式，有两种实现：<code>DeserializedMemoryEntry</code>和<code>SerializedMemoryEntry</code></p><p><code>DeserializedMemoryEntry</code>：存储的数据结构是Array[T]，只适用于堆内</p><p><code>SerializedMemoryEntry</code>：存储的数据结构是<code>ChunkedByteBuffer</code>（物理上存储为多个块而不是单个块上的连续数组），适用于堆内和堆外</p><p>采用数组的数据结构很好的<strong>解决存储内存碎片</strong>问题：通常RDD 的 record 是 在 other内存中的是不连续空间，当要缓存时（也就是存到存储内存中），会将其由不连续的存储空间转换为连续的存储空间（数组）。</p><h2 id="核心成员"><a class="header-anchor" href="#核心成员"></a>核心成员</h2><p><strong>entries</strong>：一个 LinkedHashMap （BlockId  -&gt;  MemoryEntry），blocks 存入 MemoryEntry，同时建立 BlockId 和 MemoryEntry 的映射关系。注意LinkedHashMap本身不是线程安全的，因此<strong>对其并发访问都要加锁</strong>。<strong>MemoryStore的核心其实就是对entries的增删改查</strong>。</p><p><strong>onHeapUnrollMemoryMap 和 offHeapUnrollMemoryMap</strong>：（BlockId  -&gt;  使用的 UnrollMemory 的大小）</p><h2 id="核心方法："><a class="header-anchor" href="#核心方法："></a><strong>核心方法</strong>：</h2><h3 id="putBytes"><a class="header-anchor" href="#putBytes"></a><strong>putBytes</strong></h3><p>序列化ByteBuffer的写入：在写入前，先调用<strong>MemoryManager.acquireStorageMemory()<strong>申请所需的内存，再将</strong>ChunkedByteBuffer</strong>封装进<strong>SerializedMemoryEntry</strong>，最后将该MemoryEntry放入entries映射。</p><h3 id="putIterator-T"><a class="header-anchor" href="#putIterator-T"></a><strong>putIterator[T]</strong></h3><p>迭代器对象的写入，根据入参 <strong>valuesHolder</strong>的不同，决定写成对象还是字节</p><p>开始时，先初始化（申请）一部分内存（spark.storage.unrollMemoryThreshold 默认1M），将迭代器对象逐渐存入<strong>valuesHolder</strong>，并且每添加<code>memoryCheckPeriod</code>个元素就<strong>检查一次valuesHolder中数据占用的内存</strong>，如果超过了申请的内存就继续申请<code>UnrollMemory</code>（由<code>memoryGrowthFactor</code>参数控制申请大小，这种递增的方式避免OOM）。将全部元素存入valuesHolder后，使用它的<strong>build方法</strong>生成<code>MemoryEntry</code>（如果是堆内模式内部就是将SizeTrackingVector转成数组），接着release掉申请的unroll内存，最后申请整体大小的Storage内存。化零为整，这个零它就是UnrollMemory，现在知道UnrollMemory是啥了吧，它就是个工具人。</p><p>该函数，由下面两个函数调用。</p><h3 id="putIteratorAsValues"><a class="header-anchor" href="#putIteratorAsValues"></a>putIteratorAsValues</h3><p>将迭代器对象 写成对象，使用的<strong>valuesHolder</strong>是 <code>DeserializedValuesHolder</code></p><p>该方法的重点把握DeserializedValuesHolder，其内部有两个数据结构：SizeTrackingVector 和 Array；SizeTrackingVector 实现了SizeTracker接口，通过采样估计的方式得到其占用大小（<strong>估计值</strong>）。数据存入DeserializedValuesHolder其实是写入到SizeTrackingVector中，等待全部写完，再使用<strong>getBuilder</strong>方法，将SizeTrackingVector转成Array，再转成DeserializedMemoryEntry。</p><h3 id="putIteratorAsBytes"><a class="header-anchor" href="#putIteratorAsBytes"></a>putIteratorAsBytes</h3><p>将迭代器对象 写成字节，使用的<strong>valuesHolder</strong>是 <code>SerializedValuesHolder</code></p><p>SerializedValuesHolder 内部是各种流，且用到了<strong>serializerManager</strong>，暂时不想了解过深，只要知道每次write时流内部会记录size，因此得到的总size是精确的。其内部的build方法，将数据封装成SerializedMemoryEntry。</p><h3 id="getBytes、getValues、remove"><a class="header-anchor" href="#getBytes、getValues、remove"></a>getBytes、getValues、remove</h3><p>读删操作就很简单了，直接根据blockId 对<strong>entries</strong>中的Map进行读删即可</p><p>getBytes 的返回值是 ChunkedByteBuffer ，getValues 的返回值是 Iterator[T]，分别对应entry的2种数据结构</p><h3 id="evictBlocksToFreeSpace"><a class="header-anchor" href="#evictBlocksToFreeSpace"></a>evictBlocksToFreeSpace</h3><p>是不是很熟悉这名字，驱除存储内存时用的就是它。</p><p><strong>调用时机</strong></p><ul><li>存储内存不足（自己的不够并且借执行内存的也不够 或者 被执行内存占用很多）</li><li>执行内存不足时，需要回收被存储内存占用的</li></ul><p><strong>回收限制</strong></p><ul><li>不能是同一个 RDD的 block，避免循环淘汰</li><li>memoryMode一致，即同属堆内或者堆外</li><li>Block  不能处于被读状态</li><li>LRU（最近最少使用）：这是 entries 也就是 LinkedHashMap自带的功能</li></ul><h1 id="小结"><a class="header-anchor" href="#小结"></a>小结</h1><p>MemoryManager 用于对内存划分，同时实现执行内存和存储内存的动态占用。这一切都是逻辑上的，它其实就是计数员，MemoryPool 就是它的账本。当你需要使用执行内存或者存储内存时，你要向它汇报，它会对下帐本告诉你可不可分到，接着对账本修改。</p><p>MemoryStore 负责将数据写入<strong>存储内存</strong>以及后续的读取与删除，数据结构就是一个LinkedHashMap。说白了它就是个仓库，存缓存，存广播等等。</p><p>你可能会有个疑问，MemoryManager不是划了执行内存和存储内存嘛，都是关于存储内存的，那么执行内存的操作去哪啦？<strong>它与TaskMemoryManager和MemConsumer有关</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;它俩的对象和方法较多，需要耐心反复看。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>我的2019</title>
    <link href="http://yoursite.com/2019/12/31/%E9%9A%8F%E7%AC%94/%E6%88%91%E7%9A%842019/"/>
    <id>http://yoursite.com/2019/12/31/随笔/我的2019/</id>
    <published>2019-12-31T08:59:34.000Z</published>
    <updated>2020-01-02T01:06:30.500Z</updated>
    
    <content type="html"><![CDATA[<p>2019小结，多图慎点</p><a id="more"></a><h1 id="github"><a class="header-anchor" href="#github"></a>github</h1><p>一度执着commit，一度觉得太水，一度又执着，一度又… <strong>希望明年加油，争取贡献PR</strong></p><p><img src="https://i.loli.net/2019/12/31/UtnVwE4IDSa1cZe.png" alt></p><h1 id="Hexo"><a class="header-anchor" href="#Hexo"></a>Hexo</h1><p>2019年，在小破站写(shui)了<strong>49篇文(shui)章(wen)</strong>。学习上，打60分吧，勉强及格，明年要更加努力！</p><p><img src="https://i.loli.net/2019/12/31/ZuN1XwA7nH8ylsf.png" alt></p><h1 id="读书"><a class="header-anchor" href="#读书"></a>读书</h1><p>欣赏来自豆瓣的蜜汁黑色散点图，一次更新后变成了这样，实在无力吐槽</p><p><img src="https://i.loli.net/2019/12/31/PLiUoA8uN4nfsBb.png" alt></p><p>微信读书的2019成绩单，<strong>勉强超过平均水平?!</strong> 今年相比去年热情降低了不少，感觉很难遇到让人眼前一亮的书了。</p><p>今年看过最好看的3本是：<strong>《平凡的世界》 《霍乱时期的爱情》 《追风筝的人》</strong>。经典不愧是经典～</p><p>夸夸<strong>良心</strong>的微信读书：开局一条狗，<strong>无线卡全靠白嫖</strong>。倘若收费，我必奉陪。</p><p><img src="https://i.loli.net/2019/12/31/IKZmn3JNlhv8BT7.jpg" alt></p><h1 id="billbill"><a class="header-anchor" href="#billbill"></a>billbill</h1><p>9月开始玩的，王者时刻生成后再自己剪一下，投了几篇，自娱自乐加朋友圈形成完美闭环。奈何画(A)质(V)不忍直视，真想放弃，希望<strong>王者的自动剪辑画质能提升点</strong>！</p><p>也夸夸B站，在B站上学习了很多免费学习资源，而且用户体验也挺不错的，果然程(si)序(fei)员(zai)懂二次元，奥利给！至于大会员嘛，下次一定，ヾﾉ≧∀≦)o</p><p><img src="https://i.loli.net/2019/12/31/EzojgVJNAnXhrCY.jpg" alt></p><h1 id="音乐"><a class="header-anchor" href="#音乐"></a>音乐</h1><p>2020年，也要<strong>将非主流贯彻到底</strong>！年度听的最多竟然是嵩哥的，是不是计算错了？？</p><p><img src="https://i.loli.net/2019/12/31/Oq9WUB1KQ3gMj7s.jpg" alt></p><h1 id="王者荣耀"><a class="header-anchor" href="#王者荣耀"></a>王者荣耀</h1><p>人老了反应跟不上了，混混王者就行，以后玩的时间越来越少了。话说，我的<strong>李白荣耀典藏又跳票了</strong>，<strong>韩信的鼠年传说被伽罗抢了</strong>。🐶 策划，我要转战隔壁lol手游！</p><p>貌似没找到王者的2019年报，也太偷懒了吧？？</p><h1 id="2020-加油！"><a class="header-anchor" href="#2020-加油！"></a>2020 加油！</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2019小结，多图慎点&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式版本控制系统-Git</title>
    <link href="http://yoursite.com/2019/12/31/%E5%B7%A5%E5%85%B7/%E5%88%86%E5%B8%83%E5%BC%8F%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F-Git/"/>
    <id>http://yoursite.com/2019/12/31/工具/分布式版本控制系统-Git/</id>
    <published>2019-12-31T01:10:52.000Z</published>
    <updated>2019-12-31T01:14:45.901Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要包括Git的用法加底层</p><h1 id="使用"><a class="header-anchor" href="#使用"></a>使用</h1><h2 id="初始化相关"><a class="header-anchor" href="#初始化相关"></a>初始化相关</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># --system 操作系统级别； --global 用户级别； 不加： 项目级别 </span></span><br><span class="line">$ git config --global user.name <span class="string">"zouxxyy"</span></span><br><span class="line">$ git config --global user.email <span class="string">"xxxxxxxxxxxxxx"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看配置</span></span><br><span class="line">$ git config --list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建git对象，会生成 .git 目录</span></span><br><span class="line">$ git init</span><br><span class="line">$ ls</span><br><span class="line">HEAD                 指示当前分支的文件</span><br><span class="line">config               配置文件</span><br><span class="line">hooks.  （目录）      存放钩子脚本，比如提交代码前或者后的自动执行操作</span><br><span class="line">objects （目录）      存放所有历史记录的</span><br><span class="line">branches             指示目前被检测出的分支</span><br><span class="line">description          仓库描述性信息的文件</span><br><span class="line">info    （目录）      存放全局性的排除文件。比如 mac 进去查看就有.DS_Store</span><br><span class="line">refs    （目录）      存放分支与tags的提交对象的指针</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="文件相关"><a class="header-anchor" href="#文件相关"></a>文件相关</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加新文件（由未跟踪到跟踪状态，同时也是暂存状态）或者添加修改文件（由修改状态到暂存状态）（粗略这样说，其实底层做了更多事）</span></span><br><span class="line">$ git add test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交文件（由暂存状态到提交状态）；可加参数 -a 跳过 add 步骤（未跟踪的不能提交）</span></span><br><span class="line">$ git commit -m <span class="string">"commit message"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件的状态</span></span><br><span class="line">$ git status</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看未被 add 的修改</span></span><br><span class="line">$ git diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看已 add 但未被 commit 的修改</span></span><br><span class="line">$ git diff --staged</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件 （同时让文件由跟踪变成未跟踪状态）（注意它 和手动删除文件再执行 git add 效果一样）</span></span><br><span class="line">$ git rm test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重命名</span></span><br><span class="line">$ git mv test.txt new.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 commit log；可加 --oneline 参数简写</span></span><br><span class="line">$ git <span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 完整 commit log（只要改动了HEAD）</span></span><br><span class="line">$ git reflog</span><br></pre></td></tr></table></figure><h2 id="分支相关"><a class="header-anchor" href="#分支相关"></a>分支相关</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建分支</span></span><br><span class="line">$ git branch branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换分支 (切换前保证 status 干净)</span></span><br><span class="line">$ git checkout branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并切换</span></span><br><span class="line">$ git checkout -b branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示所有分支 可加参数 -v 查看它们的最后一次提交</span></span><br><span class="line">$ git branch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除分支（需先切到另一个分支）可加参数 -D 强制删除</span></span><br><span class="line">$ git branch -d branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建分支，并指向指定的提交对象 (时光机)</span></span><br><span class="line">$ git branch branchname commitHash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分支合并 （合并前切回主分支，再合并需要合并的分支）</span></span><br><span class="line"><span class="comment"># 合并可能会产生冲突，需要手动修改冲突的文件，再提交</span></span><br><span class="line">$ git merge otherBranch</span><br></pre></td></tr></table></figure><h2 id="stash相关"><a class="header-anchor" href="#stash相关"></a>stash相关</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 有时想切分支，但又不想提交，可以用到 stash 功能，是一种暂存的栈。这个栈是针对分支的～</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前分支的 stash</span></span><br><span class="line">$ git stash list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将未提交的改动 入栈（接着可以安全执行切分支操作）</span></span><br><span class="line">$ git stash</span><br><span class="line"></span><br><span class="line"><span class="comment"># （切回分支后）弹出 改动</span></span><br><span class="line">$ git pop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后面两个一般不用，我们栈里只放一个元素，git stash 入栈，git pop 出栈</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只取指定改动，不删除</span></span><br><span class="line">$ git stash apply stashName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定的栈里的东西</span></span><br><span class="line">$ git stash drop stashName</span><br></pre></td></tr></table></figure><h2 id="回退相关"><a class="header-anchor" href="#回退相关"></a>回退相关</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 撤回工作目录的修改（未add -&gt; 未修改）</span></span><br><span class="line">$ git checkout -- fileName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回暂存区的修改 （未commit -&gt; 未add）</span></span><br><span class="line">$ git reset [HEAD] fileName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可用于修改刚刚提交的message（相当于后退一步，再重新提交）</span></span><br><span class="line">$ git commit --amend</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意下面3个命令都可以撤回到指定提交对象（把 HEAD~ 改为指定的 commitHash 即可）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回上一次提交 （commit -&gt; 未commit）</span></span><br><span class="line">$ git reset --soft HEAD~</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回上一次提交同时撤回暂存区的修改 （commit -&gt; 未add）</span></span><br><span class="line">$ git reset [--mix] HEAD~</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回上一次提交同时撤回暂存区的修改同时撤回工作目录的修改 （commit -&gt; 未修改，危险！）</span></span><br><span class="line">$ git reset --hard HEAD~</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回少用 reset --hard，最好使用 branch </span></span><br><span class="line">$ git branch recoverBranchName commitHash</span><br></pre></td></tr></table></figure><h2 id="标签相关"><a class="header-anchor" href="#标签相关"></a>标签相关</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 列出所有tag</span></span><br><span class="line">$ git tag</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打tag</span></span><br><span class="line">$ git tag tagName commitHash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定tag</span></span><br><span class="line">$ git tag -d tagName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切到指定tag，然后创新分支，防止头部分离</span></span><br><span class="line">$ git checkout tagName</span><br><span class="line">$ git checkout -b branchName</span><br></pre></td></tr></table></figure><h2 id="远程相关"><a class="header-anchor" href="#远程相关"></a>远程相关</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看全部远程分支</span></span><br><span class="line">$ git remote -v</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加远程分支别名(注意：orgin 只个别名)</span></span><br><span class="line">$ git remote add orgin https://xxxx.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 克隆远程仓库到本地（此时自动创建全部远程跟踪分支，同时生成一个关联远程master的本地分支（注意））</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://xxxx.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由本地分支创建远程跟踪分支，再推到远程分支，但并不关联（注意）</span></span><br><span class="line"><span class="comment"># 推荐仅当创建一个新分支，并想把它推到远程时，才使用它；并且接着执行关联操作，也就下一行，之后仅用 git push 即可</span></span><br><span class="line">$ git push orgin branchName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本地分支（已创建）关联远程跟踪分支</span></span><br><span class="line">$ git branch -u orgin/branchName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取远程分支（全部分支）到远程跟踪分支中</span></span><br><span class="line"><span class="comment"># 推荐仅当远程有了新分支时，才使用它（注意）</span></span><br><span class="line">$ git fetch orgin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个本地分支，同时关联远程跟踪分支</span></span><br><span class="line">$ git branch -b branchName orgin/branchName</span><br><span class="line">$ git branch --track orgin/branchName (效果一样，快捷写法)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉去远程分支到远程跟踪分支和本地分支（本地分支已经关联远程跟踪分支），相当于执行 git fetch + git merge</span></span><br><span class="line">$ git pull</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除远程分支</span></span><br><span class="line">$ git push orgin --delete branchName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出没用远程跟踪分支，并删除</span></span><br><span class="line">$ git remote prune orgin --dry-run</span><br><span class="line">$ git remote orgin</span><br></pre></td></tr></table></figure><p>看起来比较复杂，但只要记住一个逻辑链就好，也就是按以下3步走：</p><ol><li><p><strong>建立远程跟踪分支</strong></p><ul><li><code>git clone https://xxxx.git</code></li><li><code>git push orgin branchName</code> 推<strong>新分支</strong>到远程</li><li><code>git fetch orgin</code> 拉远程的<strong>新分支</strong></li></ul></li><li><p><strong>本地分支关联远程跟踪分支</strong></p><ul><li><code>git checkout branchName</code> 创建本地新分支（和远程分支同名） 并关联 (还是它简单)</li><li><code>git branch -u orgin/branchName</code> 当前分支 关联</li><li><code>git branch --track orgin/branchName</code> 创建本地新分支 关联</li></ul></li><li><p><strong>愉快使用</strong></p><ul><li><code>git pull</code> 拉到本地</li><li><code>git push</code> 推到远程（先 pull，解决冲突再 push）</li></ul></li></ol><p>上面是自己团队的项目（<strong>有权限</strong>），当想为开源项目做贡献（<strong>没权限</strong>）时，使用 <strong>pull request</strong>， 大致流程（用到再研究）为：</p><ol><li><p><strong>folk 一份</strong></p></li><li><p><strong>在 folk 的库中，作出改动并提交，注意提交前先拉远程库，保持最新。</strong></p></li><li><p><strong>进网站（如github），点 pullRequest</strong></p></li></ol><h1 id="底层"><a class="header-anchor" href="#底层"></a>底层</h1><p>git 的底层数据结构是一种在 <code>object</code> 目录中存放的 k-v 类型的数据。key 是 hash值 (前两位是子文件夹名，后面是文件名)，value 是 数据内容。</p><p>数据内容 分为 <strong>git 对象</strong>（<code>blob</code>）、<strong>树对象</strong>（<code>tree</code>）、<strong>提交对象</strong>（<code>commit</code>）</p><h2 id="git-对象"><a class="header-anchor" href="#git-对象"></a>git 对象</h2><p><strong>代表文件，它是真正存数据的</strong>。它是一对一的（注意：<strong>一个文件的历史文件都算一个单独的文件</strong>）。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 向数据库中写数据，并返回hash键值。存的是一个blob对象</span></span><br><span class="line">$ git <span class="built_in">hash</span>-object -w test.txt</span><br><span class="line">915c628f360b2d8c3edbe1ac65cf575b69029b61</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据是经过压缩的，可以用cat-file查看该数据</span></span><br><span class="line"><span class="comment"># -p 显示内容； -t 查看类型</span></span><br><span class="line">$ git cat-file -p 915c628f360b2d8c3edbe1ac65cf575b69029b61</span><br><span class="line">hello git</span><br></pre></td></tr></table></figure><h2 id="树对象"><a class="header-anchor" href="#树对象"></a>树对象</h2><p><strong>代表版本</strong>，可以理解成一个树对象指向多个 git 对象，形成一个版本。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 暂存区 写入 test.txt 的首个版本（指针信息）</span></span><br><span class="line"><span class="comment"># --add 文件首次添加；--cacheinfo 表示添加的文件位于git数据库；100644 文件类型； test.txt 文件名</span></span><br><span class="line">$ git update-index --add --cacheinfo 100644 915c628f360b2d8c3edbe1ac65cf575b69029b61 test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看暂存区</span></span><br><span class="line">$ git ls-files -s</span><br><span class="line">100644 915c628f360b2d8c3edbe1ac65cf575b69029b61 0 test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成树对象。暂存区未清空</span></span><br><span class="line">$ git write-tree</span><br><span class="line">72203871fa4668ad777833634034dcd3426879db</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看树对象内容</span></span><br><span class="line">$ git cat-file -p 72203871fa4668ad777833634034dcd3426879db</span><br><span class="line">100644 blob 915c628f360b2d8c3edbe1ac65cf575b69029b61 test.txt</span><br></pre></td></tr></table></figure><h2 id="提交对象"><a class="header-anchor" href="#提交对象"></a>提交对象</h2><p><strong>相当于对版本添加描述信息</strong>。一个提交对象封装一个树对象，而且它是<strong>链式</strong>的，里面指向上一个提交对象。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成提交对象，后接树对象hash值</span></span><br><span class="line"><span class="comment"># 这里是第一次提交。以后可以加 -p 指定父提交对象</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">'first commit'</span> | git commit-tree 72203871fa4668ad777833634034dcd3426879db</span><br><span class="line">5402d3560f35d66f7f1e4e4665dd63bf3d786495</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看提交对象的内容</span></span><br><span class="line">$ git cat-file -p 5402d3560f35d66f7f1e4e4665dd63bf3d786495</span><br><span class="line">tree 72203871fa4668ad777833634034dcd3426879db</span><br><span class="line">author zouxxyy &lt;xxxxxxxxx@qq.com&gt; 1575450654 +0800</span><br><span class="line">committer zouxxyy &lt;xxxxxxxxx@qq.com&gt; 1575450654 +0800</span><br><span class="line"></span><br><span class="line">first commit</span><br></pre></td></tr></table></figure><h2 id="文件相关-v2"><a class="header-anchor" href="#文件相关-v2"></a>文件相关</h2><ul><li><p><code>git add test.txt</code> 相当于先将文件做成<code>git对象</code>存入<code>object</code> ，再将它（指针信息）放入暂存区，等待提交。该操作是<strong>绝对安全</strong>的，因为数据已经写入文件中。所以可以理解为啥只有 <code>git add</code>，而没有什么<code>git change</code>之类的，因为修改也会加<code>git对象</code>。</p></li><li><p><code>git commit -m &quot;xx&quot;</code> 由暂存区生成树对象，再生成提交对象。<strong>注意暂存区的东西不删除</strong></p></li><li><p><code>git rm test.txt</code> <strong>相当于将该文件对应的（指针信息）从暂存区中删除，同时删除工作目录中的文件</strong></p></li></ul><h2 id="分支相关-v2"><a class="header-anchor" href="#分支相关-v2"></a>分支相关</h2><p><strong>分支切换会切换工作目录</strong>，所以切换前，先把当前分支该提交的提交了，<strong>保证工作目录干净</strong>。</p><ul><li><p>2个重要的东西：</p><ul><li><code>refs</code> 存放各个分支（和tags）的提交对象的指针（hashkey）</li><li><code>HEAD</code> 指明当前分支的指针位置</li></ul></li><li><p><code>git branch branchname</code> 相当于在 <code>refs/heads</code> 目录中新建以<strong>分支名</strong>命名的当前分支的提交对象的指针</p></li><li><p><code>git checkout branchname</code> 更改<code>HEAD</code>文件；<strong>切换工作目录</strong>；<strong>影响暂存区</strong></p></li><li><p><code>git commit -m &quot;commit message&quot;</code> 更新 <code>refs</code> 中对应分支的提交对象</p></li></ul><h2 id="回退相关-v2"><a class="header-anchor" href="#回退相关-v2"></a>回退相关</h2><p><code>git checkout branchName</code> 和 <code>git reset --hard commitHash</code> 的区别 ：</p><ul><li><p><code>checkout</code> 只改HEAD ；<code>reset</code> HEAD 和 分支指针的一起改</p></li><li><p><code>checkout</code> 对工作目录是安全的；<code>reset --hard</code> 会<strong>完全强制覆盖工作目录</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要包括Git的用法加底层&lt;/p&gt;
&lt;h1 id=&quot;使用&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#使用&quot;&gt;&lt;/a&gt;使用&lt;/h1&gt;
&lt;h2 id=&quot;初始化相关&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#初始化相关&quot;&gt;&lt;/a&gt;初始化相关&lt;/h2&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# --system 操作系统级别； --global 用户级别； 不加： 项目级别 &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ git config --global user.name &lt;span class=&quot;string&quot;&gt;&quot;zouxxyy&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ git config --global user.email &lt;span class=&quot;string&quot;&gt;&quot;xxxxxxxxxxxxxx&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 查看配置&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ git config --list&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 创建git对象，会生成 .git 目录&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ git init&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ ls&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;HEAD                 指示当前分支的文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;config               配置文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hooks.  （目录）      存放钩子脚本，比如提交代码前或者后的自动执行操作&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;objects （目录）      存放所有历史记录的&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;branches             指示目前被检测出的分支&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;description          仓库描述性信息的文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;info    （目录）      存放全局性的排除文件。比如 mac 进去查看就有.DS_Store&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;refs    （目录）      存放分支与tags的提交对象的指针&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Tools" scheme="http://yoursite.com/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>spark源码-shuffle之BlockStoreShuffleReader</title>
    <link href="http://yoursite.com/2019/12/30/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BBlockStoreShuffleReader/"/>
    <id>http://yoursite.com/2019/12/30/spark/spark源码-shuffle之BlockStoreShuffleReader/</id>
    <published>2019-12-30T01:33:22.000Z</published>
    <updated>2020-01-10T09:30:57.239Z</updated>
    
    <content type="html"><![CDATA[<p>spark的唯一 ShuffleReader ：BlockStoreShuffleReader</p><a id="more"></a><h1 id="引入"><a class="header-anchor" href="#引入"></a>引入</h1><p><a href="https://zouxxyy.github.io/2019/11/29/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BShuffleManager/" target="_blank" rel="noopener">之前</a>提过，<code>getShuffleWrite</code> 只在 <code>ShuffleMapTask</code> 中出现。</p><p>那么<code>getShuffleRead</code>呢？由于不管哪个 Task 都需要读数据，于是就把该步骤封装在RDD的<code>computer</code>方法中。以下是<code>ShuffleRDD</code>中的<code>computer</code>方法。通过调用 <code>shuffleManager</code> 的 <code>getReader</code> 就获取了本文的主角<code>BlockStoreShuffleReader</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">  <span class="keyword">val</span> dep = dependencies.head.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]]</span><br><span class="line">  <span class="comment">// 是不是觉得 split.index, split.index + 1 很怪，后面会发现这是怎么回事</span></span><br><span class="line">  <span class="type">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class="number">1</span>, context)</span><br><span class="line">    .read()</span><br><span class="line">    .asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="大致流程"><a class="header-anchor" href="#大致流程"></a>大致流程</h1><ol><li><p><strong>获取初始迭代器<code>ShuffleBlockFetcherIterator</code></strong>（<code>Iterator[(BlockId, InputStream)</code>）</p></li><li><p><strong>反序列化出实例，并生成 k,v 迭代器</strong>（<code>Iterator[(key, value)]</code>）</p></li><li><p><strong>添加readMetrics，再封装成<code>InterruptibleIterator</code></strong></p></li><li><p><strong>进行聚合操作</strong>。分有无聚合，当有聚合时，又分是否执行过map聚合</p></li><li><p><strong>进行排序操作</strong>。分有无排序。</p></li></ol><p><code>BlockStoreShuffleReader </code> 要干的事其实很容易理解，就是把<strong>不同 block 中同一分区的record</strong>，拉到指定的 reducer 中，再对它进行<strong>聚合和排序</strong>即可。一个 reducer 处理一个分区。</p><h1 id="源码"><a class="header-anchor" href="#源码"></a>源码</h1><h2 id="获取初始迭代器ShuffleBlockFetcherIterator"><a class="header-anchor" href="#获取初始迭代器ShuffleBlockFetcherIterator"></a>获取初始迭代器<code>ShuffleBlockFetcherIterator</code></h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取初始迭代器 ShuffleBlockFetcherIterator</span></span><br><span class="line"><span class="keyword">val</span> wrappedStreams = <span class="keyword">new</span> <span class="type">ShuffleBlockFetcherIterator</span>(</span><br><span class="line">  context,</span><br><span class="line">  blockManager.shuffleClient, <span class="comment">// 默认是 NettyBlockTransferService，如果使用外部shuffle系统则使用 ExternalShuffleClient</span></span><br><span class="line">  blockManager,</span><br><span class="line">  <span class="comment">// 通过它得到 2元tuple : (BlockManagerId, Seq[(BlockId, BlockSize)])</span></span><br><span class="line">  mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),</span><br><span class="line">  serializerManager.wrapStream,</span><br><span class="line">  <span class="comment">// Note: we use getSizeAsMb when no suffix is provided for backwards compatibility</span></span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.getSizeAsMb(<span class="string">"spark.reducer.maxSizeInFlight"</span>, <span class="string">"48m"</span>) * <span class="number">1024</span> * <span class="number">1024</span>,</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.getInt(<span class="string">"spark.reducer.maxReqsInFlight"</span>, <span class="type">Int</span>.<span class="type">MaxValue</span>),</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.get(config.<span class="type">REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS</span>),</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.get(config.<span class="type">MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM</span>),</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.getBoolean(<span class="string">"spark.shuffle.detectCorrupt"</span>, <span class="literal">true</span>))</span><br></pre></td></tr></table></figure><p>这是 <code>mapOutputTracker.getMapSizesByExecutorId</code> 里的关键步骤</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convertMapStatuses</span></span>(</span><br><span class="line">    shuffleId: <span class="type">Int</span>,</span><br><span class="line">    startPartition: <span class="type">Int</span>,</span><br><span class="line">    endPartition: <span class="type">Int</span>,</span><br><span class="line">    statuses: <span class="type">Array</span>[<span class="type">MapStatus</span>]): <span class="type">Iterator</span>[(<span class="type">BlockManagerId</span>, <span class="type">Seq</span>[(<span class="type">BlockId</span>, <span class="type">Long</span>)])] = &#123;</span><br><span class="line">  assert (statuses != <span class="literal">null</span>)</span><br><span class="line">  <span class="keyword">val</span> splitsByAddress = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">BlockManagerId</span>, <span class="type">ListBuffer</span>[(<span class="type">BlockId</span>, <span class="type">Long</span>)]]</span><br><span class="line">  <span class="comment">// mapTask 的个数决定了 Seq[(BlockId, BlockSize)] 内元素的个数，很好理解</span></span><br><span class="line">  <span class="keyword">for</span> ((status, mapId) &lt;- statuses.iterator.zipWithIndex) &#123;</span><br><span class="line">    <span class="keyword">if</span> (status == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> errorMessage = <span class="string">s"Missing an output location for shuffle <span class="subst">$shuffleId</span>"</span></span><br><span class="line">      logError(errorMessage)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">MetadataFetchFailedException</span>(shuffleId, startPartition, errorMessage)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 左闭右开，因此前面的 （split.index, split.index + 1）中的 split.index + 1，并没有什么软用  </span></span><br><span class="line">      <span class="keyword">for</span> (part &lt;- startPartition until endPartition) &#123;</span><br><span class="line">        <span class="keyword">val</span> size = status.getSizeForBlock(part)</span><br><span class="line">        <span class="keyword">if</span> (size != <span class="number">0</span>) &#123;</span><br><span class="line">          splitsByAddress.getOrElseUpdate(status.location, <span class="type">ListBuffer</span>()) +=</span><br><span class="line">              ((<span class="type">ShuffleBlockId</span>(shuffleId, mapId, part), size))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  splitsByAddress.iterator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>ShuffleBlockFetcherIterator  </code>这个迭代器的具体实现比较复杂，简单介绍下：</p><ul><li><p>分本地数据块 和 远程数据块</p></li><li><p>本地数据块直接调用 <code>BlockManager.getBlockData</code></p></li><li><p>远程数据块采用Netty通过网络获取</p></li></ul><p>下面是 <code>IndexShuffleBlockResolver</code> 中的 <code>getBlockData</code>。我们的索引文件（indexFile）终于派上用场</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getBlockData</span></span>(blockId: <span class="type">ShuffleBlockId</span>): <span class="type">ManagedBuffer</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> indexFile = getIndexFile(blockId.shuffleId, blockId.mapId)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> channel = <span class="type">Files</span>.newByteChannel(indexFile.toPath)</span><br><span class="line">  <span class="comment">// 根据reduceId选择索引</span></span><br><span class="line">  channel.position(blockId.reduceId * <span class="number">8</span>L)</span><br><span class="line">  <span class="keyword">val</span> in = <span class="keyword">new</span> <span class="type">DataInputStream</span>(<span class="type">Channels</span>.newInputStream(channel))</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> offset = in.readLong()</span><br><span class="line">    <span class="keyword">val</span> nextOffset = in.readLong()</span><br><span class="line">    <span class="keyword">val</span> actualPosition = channel.position()</span><br><span class="line">    <span class="keyword">val</span> expectedPosition = blockId.reduceId * <span class="number">8</span>L + <span class="number">16</span></span><br><span class="line">    <span class="keyword">if</span> (actualPosition != expectedPosition) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">s"SPARK-22982: Incorrect channel position after index file reads: "</span> +</span><br><span class="line">        <span class="string">s"expected <span class="subst">$expectedPosition</span> but actual position was <span class="subst">$actualPosition</span>."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FileSegmentManagedBuffer</span>(</span><br><span class="line">      transportConf,</span><br><span class="line">      getDataFile(blockId.shuffleId, blockId.mapId),</span><br><span class="line">      offset,</span><br><span class="line">      nextOffset - offset)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    in.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="反序列化出实例，并生成-k-v-迭代器"><a class="header-anchor" href="#反序列化出实例，并生成-k-v-迭代器"></a>反序列化出实例，并生成 k,v 迭代器</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> serializerInstance = dep.serializer.newInstance()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a key/value iterator for each stream</span></span><br><span class="line"><span class="keyword">val</span> recordIter = wrappedStreams.flatMap &#123; <span class="keyword">case</span> (blockId, wrappedStream) =&gt;</span><br><span class="line">  serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="添加readMetrics，再封装成InterruptibleIterator"><a class="header-anchor" href="#添加readMetrics，再封装成InterruptibleIterator"></a>添加readMetrics，再封装成<code>InterruptibleIterator</code></h2><p>readMetrics 里都是些记录数据，用于监控展示</p><p><code>InterruptibleIterator</code> 封装了任务中断功能</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Update the context task metrics for each record read.</span></span><br><span class="line"><span class="keyword">val</span> readMetrics = context.taskMetrics.createTempShuffleReadMetrics()</span><br><span class="line"><span class="keyword">val</span> metricIter = <span class="type">CompletionIterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>), <span class="type">Iterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>)]](</span><br><span class="line">  recordIter.map &#123; record =&gt;</span><br><span class="line">    readMetrics.incRecordsRead(<span class="number">1</span>) <span class="comment">// sparkUI 里的 record 数</span></span><br><span class="line">    record</span><br><span class="line">  &#125;,</span><br><span class="line">  context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> interruptibleIter = <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>)](context, metricIter)</span><br></pre></td></tr></table></figure><h2 id="进行聚合操作"><a class="header-anchor" href="#进行聚合操作"></a>进行聚合操作</h2><p>分有无聚合，当有聚合时，又分是否在 mapTask 时执行过map聚合</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> aggregatedIter: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = <span class="keyword">if</span> (dep.aggregator.isDefined) &#123;</span><br><span class="line">  <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="comment">// We are reading values that are already combined</span></span><br><span class="line">    <span class="comment">// 有 mapSideCombine，就是聚合(K, C)</span></span><br><span class="line">    <span class="keyword">val</span> combinedKeyValuesIterator = interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">    dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 无 mapSideCombine，就是聚合(K, V)</span></span><br><span class="line">    <span class="keyword">val</span> keyValuesIterator = interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">Nothing</span>)]]</span><br><span class="line">    dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">// 不聚合</span></span><br><span class="line">  interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="进行排序操作"><a class="header-anchor" href="#进行排序操作"></a>进行排序操作</h2><p>分是否需要排序。如果需要，就使用<code>ExternalSorter</code>在分区内部进行排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> resultIter = dep.keyOrdering <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Some</span>(keyOrd: <span class="type">Ordering</span>[<span class="type">K</span>]) =&gt;</span><br><span class="line">    <span class="comment">// Create an ExternalSorter to sort the data.</span></span><br><span class="line">    <span class="keyword">val</span> sorter =</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">C</span>, <span class="type">C</span>](context, ordering = <span class="type">Some</span>(keyOrd), serializer = dep.serializer)</span><br><span class="line">    sorter.insertAll(aggregatedIter)</span><br><span class="line">    context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class="line">    context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class="line">    context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class="line">    <span class="comment">// Use completion callback to stop sorter if task was finished/cancelled.</span></span><br><span class="line">    context.addTaskCompletionListener[<span class="type">Unit</span>](_ =&gt; &#123;</span><br><span class="line">      sorter.stop()</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="type">CompletionIterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>], <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]](sorter.iterator, sorter.stop())</span><br><span class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">    aggregatedIter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark的唯一 ShuffleReader ：BlockStoreShuffleReader&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark杂谈-指定文件在HDFS中的写入节点</title>
    <link href="http://yoursite.com/2019/12/28/spark/spark%E6%9D%82%E8%B0%88-%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6%E5%9C%A8HDFS%E4%B8%AD%E7%9A%84%E5%86%99%E5%85%A5%E8%8A%82%E7%82%B9/"/>
    <id>http://yoursite.com/2019/12/28/spark/spark杂谈-指定文件在HDFS中的写入节点/</id>
    <published>2019-12-28T06:52:51.000Z</published>
    <updated>2019-12-28T06:56:53.632Z</updated>
    
    <content type="html"><![CDATA[<p>项目中需要指定文件写到HDFS的具体哪个节点，通过查找API，找到一种解决办法。以 <code>TextOutputFormat</code> 为例测试</p><a id="more"></a><h1 id="write"><a class="header-anchor" href="#write"></a>write</h1><p>观察 <code>TextOutputFormat</code> 里的 <code>write</code> 方法，它使用了<code>out.write</code>将 record 写入 HDFS。找到 <code>out</code> ，修改它</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> nullKey = key == <span class="keyword">null</span> || key <span class="keyword">instanceof</span> NullWritable;</span><br><span class="line">  <span class="keyword">boolean</span> nullValue = value == <span class="keyword">null</span> || value <span class="keyword">instanceof</span> NullWritable;</span><br><span class="line">  <span class="keyword">if</span> (nullKey &amp;&amp; nullValue) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!nullKey) &#123;</span><br><span class="line">    writeObject(key);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!(nullKey || nullValue)) &#123;</span><br><span class="line">    out.write(keyValueSeparator);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!nullValue) &#123;</span><br><span class="line">    writeObject(value);</span><br><span class="line">  &#125;</span><br><span class="line">  out.write(newline);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="getRecordWriter"><a class="header-anchor" href="#getRecordWriter"></a>getRecordWriter</h1><p><code>out</code> 是一个<code>DataOutputStream</code>，它通过 <code>getRecordWriter</code>得到的。</p><p>默认是由 <code>FSDataOutputStream fileOut = fs.create(file, progress;</code> 生成，但它无法指定节点。</p><p>作出修改：</p><p><strong>关键是把它替换成 <code>DistributedFileSystem</code> 里的 <code>create</code> 方法，其中有个参数是 <code>favoredNodes </code>，顾名思义数据将优先存到它指定节点</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRecordWriter</span></span>(ignored: <span class="type">FileSystem</span>, job: <span class="type">JobConf</span>, name: <span class="type">String</span>, progress: <span class="type">Progressable</span>): <span class="type">RecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> keyValueSeparator = job.get(<span class="string">"mapreduce.output.textoutputformat.separator"</span>, <span class="string">"\t"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> file = <span class="type">FileOutputFormat</span>.getTaskOutputPath(job, name)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将 OutputStream 转成 DistributedFileSystem</span></span><br><span class="line">  <span class="keyword">val</span> fs = file.getFileSystem(job).asInstanceOf[<span class="type">DistributedFileSystem</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> favoredNodes = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">InetSocketAddress</span>](<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这里自己指定</span></span><br><span class="line">  favoredNodes(<span class="number">0</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.12"</span>, <span class="number">9866</span>) <span class="comment">// 这个port是该节点dataNode的port</span></span><br><span class="line">  favoredNodes(<span class="number">1</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.13"</span>, <span class="number">9866</span>)</span><br><span class="line">  favoredNodes(<span class="number">2</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.17"</span>, <span class="number">9866</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fileOut: <span class="type">FSDataOutputStream</span> = fs.create(file,</span><br><span class="line">    <span class="type">FsPermission</span>.getFileDefault.applyUMask(<span class="type">FsPermission</span>.getUMask(fs.getConf)),</span><br><span class="line">    <span class="literal">true</span>,</span><br><span class="line">    fs.getConf.getInt(<span class="type">IO_FILE_BUFFER_SIZE_KEY</span>, <span class="type">IO_FILE_BUFFER_SIZE_DEFAULT</span>),</span><br><span class="line">    fs.getDefaultReplication(file),</span><br><span class="line">    fs.getDefaultBlockSize(file),</span><br><span class="line">    <span class="literal">null</span>,</span><br><span class="line">    favoredNodes)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">TextOutputFormat</span>.<span class="type">LineRecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>](fileOut, keyValueSeparator)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="完整测试代码"><a class="header-anchor" href="#完整测试代码"></a>完整测试代码</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.examples.zxyTest</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.<span class="type">InetSocketAddress</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.<span class="type">CommonConfigurationKeysPublic</span>.&#123;<span class="type">IO_FILE_BUFFER_SIZE_DEFAULT</span>, <span class="type">IO_FILE_BUFFER_SIZE_KEY</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.permission.<span class="type">FsPermission</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FSDataOutputStream</span>, <span class="type">FileSystem</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hdfs.<span class="type">DistributedFileSystem</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.&#123;<span class="type">IntWritable</span>, <span class="type">Text</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.&#123;<span class="type">FileOutputFormat</span>, <span class="type">JobConf</span>, <span class="type">RecordWriter</span>, <span class="type">TextOutputFormat</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.<span class="type">Progressable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 测试 指定 HDFS 存储节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FavoredNodesTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder</span><br><span class="line">      .appName(<span class="string">"FavoredNodesTest"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .config(<span class="string">"spark.driver.memory"</span>, <span class="string">"2g"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>((<span class="string">"A"</span>, <span class="number">1</span>), (<span class="string">"B"</span>, <span class="number">2</span>), (<span class="string">"C"</span>, <span class="number">3</span>), (<span class="string">"D"</span>, <span class="number">4</span>)), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    rdd.saveAsHadoopFile(<span class="string">"hdfs://dell-r720/zxyTest/output"</span>,</span><br><span class="line">      classOf[<span class="type">Text</span>],</span><br><span class="line">      classOf[<span class="type">IntWritable</span>],</span><br><span class="line">      classOf[<span class="type">MyTextOutputFormat</span>[<span class="type">Text</span>, <span class="type">IntWritable</span>]])</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"Result has been saved"</span>)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTextOutputFormat</span>[<span class="type">K</span>, <span class="type">V</span>] <span class="keyword">extends</span> <span class="title">TextOutputFormat</span>[<span class="type">K</span>, <span class="type">V</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRecordWriter</span></span>(ignored: <span class="type">FileSystem</span>, job: <span class="type">JobConf</span>, name: <span class="type">String</span>, progress: <span class="type">Progressable</span>): <span class="type">RecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> keyValueSeparator = job.get(<span class="string">"mapreduce.output.textoutputformat.separator"</span>, <span class="string">"\t"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> file = <span class="type">FileOutputFormat</span>.getTaskOutputPath(job, name)</span><br><span class="line">    <span class="keyword">val</span> fs = file.getFileSystem(job).asInstanceOf[<span class="type">DistributedFileSystem</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3 副本位置</span></span><br><span class="line">    <span class="keyword">val</span> favoredNodes = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">InetSocketAddress</span>](<span class="number">3</span>)</span><br><span class="line">    favoredNodes(<span class="number">0</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.12"</span>, <span class="number">9866</span>) <span class="comment">// 9866是该节点dataNode的port</span></span><br><span class="line">    favoredNodes(<span class="number">1</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.13"</span>, <span class="number">9866</span>)</span><br><span class="line">    favoredNodes(<span class="number">2</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.17"</span>, <span class="number">9866</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileOut: <span class="type">FSDataOutputStream</span> = fs.create(file,</span><br><span class="line">      <span class="type">FsPermission</span>.getFileDefault.applyUMask(<span class="type">FsPermission</span>.getUMask(fs.getConf)),</span><br><span class="line">      <span class="literal">true</span>,</span><br><span class="line">      fs.getConf.getInt(<span class="type">IO_FILE_BUFFER_SIZE_KEY</span>, <span class="type">IO_FILE_BUFFER_SIZE_DEFAULT</span>),</span><br><span class="line">      fs.getDefaultReplication(file),</span><br><span class="line">      fs.getDefaultBlockSize(file),</span><br><span class="line">      <span class="literal">null</span>,</span><br><span class="line">      favoredNodes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">TextOutputFormat</span>.<span class="type">LineRecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>](fileOut, keyValueSeparator)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;项目中需要指定文件写到HDFS的具体哪个节点，通过查找API，找到一种解决办法。以 &lt;code&gt;TextOutputFormat&lt;/code&gt; 为例测试&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="HDFS" scheme="http://yoursite.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>自动化运维工具-Ansible</title>
    <link href="http://yoursite.com/2019/12/24/%E5%B7%A5%E5%85%B7/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7-Ansible/"/>
    <id>http://yoursite.com/2019/12/24/工具/自动化运维工具-Ansible/</id>
    <published>2019-12-24T08:11:22.000Z</published>
    <updated>2019-12-24T08:34:27.351Z</updated>
    
    <content type="html"><![CDATA[<p>Ansible 是一个开源的基于 OpenSSH 的自动化配置管理工具。可以用它来配置系统、部署软件和编排更高级的 IT 任务，比如持续部署或零停机更新。管理员可以通过 Ansible 在成百上千台计算机上同时执行指令(任务)，这是<a href="https://ansible-tran.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">官方文档</a>。</p><a id="more"></a><h1 id="配置组"><a class="header-anchor" href="#配置组"></a>配置组</h1><p>Ansible 可同时操作属于一个组的多台主机，在 <code>/etc/ansible/hosts</code> 中配置组</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim /etc/ansible/hosts</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用法：[组名] + hostnames</span></span><br><span class="line">[spark]</span><br><span class="line">dell-r730-[1:4]</span><br><span class="line"></span><br><span class="line">[hdfs]</span><br><span class="line">dell-r730-[1:4]</span><br><span class="line">dell-r730-7</span><br></pre></td></tr></table></figure><p>使用时可分为 <strong>ad-hoc</strong> 和 <strong>playbook</strong></p><h1 id="ad-hoc"><a class="header-anchor" href="#ad-hoc"></a>ad-hoc</h1><p>在命令行敲入的shell命令，去执行些简单的任务：</p><p><code>ansible 组名 -m 模块名 -a 具体操作 [-u 用户名] [--sudo] [-f 10]</code></p><ul><li><code>-m</code> 模块名</li><li><code>-a</code> 具体操作，<strong>一般是k v结构</strong>。</li><li><code>-u</code> 默认以当前用户的身份执行命令，也可手动指定</li><li><code>--sudo</code> 通过 sudo 去执行命令（ passwordless 模式 ）</li><li><code>-f</code> 并发量</li></ul><p>下面介绍些常用模块</p><h2 id="commond-和-shell"><a class="header-anchor" href="#commond-和-shell"></a>commond 和 shell</h2><p><code>commond </code>是默认的模块</p><p><code>commond</code> 和 <code>shell</code> 都是直接敲命令的。<code>command</code> 更安全，但<strong>不支持 shell 变量，也不支持管道等 shell 相关的东西</strong></p><p>shell使用变量时也存在限制，所以尽量<strong>不要敲太复杂的命令</strong>。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打印当前时间</span></span><br><span class="line">$ ansible zxy -a <span class="string">"date"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单管道命令</span></span><br><span class="line">$ ansible zxy -m shell -a <span class="string">"cat /etc/hosts &gt; test.txt"</span></span><br></pre></td></tr></table></figure><h2 id="file-和-copy"><a class="header-anchor" href="#file-和-copy"></a>file 和 copy</h2><p><code>file</code> 用于创建（删除）文件或文件夹，也可更改所有者权限<br><code>copy</code> 用与复制文件<br>（我觉得有点麻烦，没有commnd来的快点）</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建文件（如果存在，更新时间戳）</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testfile state=touch"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建目录（如果存在，不进行任何操作）</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testdir state=directory"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件或目录</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testdir state=absent"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可加 owner 定义所有者，mode 定义 权限，recurse 对目录递归。</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testdir state=directory owner=cluster mode=0777 recurse=true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制文件</span></span><br><span class="line"><span class="comment"># force=no 不强制覆盖，backup=no 不备份旧文件</span></span><br><span class="line">$ ansible zxy -m copy -a <span class="string">"src=/home/cluster/testfile dest=/home/cluster/testfile2 owner=cluster mode=0777"</span></span><br></pre></td></tr></table></figure><h2 id="synchronize"><a class="header-anchor" href="#synchronize"></a>synchronize</h2><p>文件或文件夹同步，有 push（默认）和 pull 模式。</p><p>默认启用了<code>archive</code>参数，该参数默认开启了recursive, links, perms, times, owner，group和-D参数。</p><p>可加 <code>--exclude=xxx</code> 忽略指定文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ansible zxy -m synchronize -a <span class="string">"src=/home/cluster/testfile dest=/home/cluster/testfile"</span></span><br></pre></td></tr></table></figure><h2 id="ping"><a class="header-anchor" href="#ping"></a>ping</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ansible zxy -m ping</span><br></pre></td></tr></table></figure><h1 id="playbook"><a class="header-anchor" href="#playbook"></a>playbook</h1><p>将一系列有序任务保存成yml文件，方便多次使用和有序的执行指定的任务。</p><ul><li>执行</li></ul><p><code>ansible-playbook playbook.yml</code></p><ul><li>格式</li></ul><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">- hosts:</span> <span class="string">组名</span></span><br><span class="line"><span class="attr">  vars:</span></span><br><span class="line">    <span class="string">变量名:</span> <span class="string">值</span></span><br><span class="line"><span class="attr">  remote_user:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">  tasks:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">描述任务（自定义）</span></span><br><span class="line">    <span class="string">模块名:</span> <span class="string">具体操作</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">xxxx</span></span><br><span class="line">    <span class="string">模块名:</span> <span class="string">xxxxx</span></span><br><span class="line"><span class="attr">    notify:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">钩子名(</span> <span class="string">task</span> <span class="string">结束且该</span> <span class="string">task</span> <span class="string">有意义（改变了东西）时被触发，只执行一次)</span></span><br><span class="line"><span class="attr">  handlers:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">钩子名</span></span><br><span class="line">      <span class="string">模块名:</span> <span class="string">具体操作</span></span><br></pre></td></tr></table></figure><ul><li>例子</li></ul><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">- hosts:</span> <span class="string">webservers</span></span><br><span class="line"><span class="attr">  vars:</span></span><br><span class="line"><span class="attr">    http_port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    max_clients:</span> <span class="number">200</span></span><br><span class="line"><span class="attr">  remote_user:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">  tasks:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ensure</span> <span class="string">apache</span> <span class="string">is</span> <span class="string">at</span> <span class="string">the</span> <span class="string">latest</span> <span class="string">version</span></span><br><span class="line"><span class="attr">    yum:</span> <span class="string">pkg=httpd</span> <span class="string">state=latest</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">write</span> <span class="string">the</span> <span class="string">apache</span> <span class="string">config</span> <span class="string">file</span></span><br><span class="line"><span class="attr">    template:</span> <span class="string">src=/srv/httpd.j2</span> <span class="string">dest=/etc/httpd.conf</span></span><br><span class="line"><span class="attr">    notify:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">restart</span> <span class="string">apache</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ensure</span> <span class="string">apache</span> <span class="string">is</span> <span class="string">running</span></span><br><span class="line"><span class="attr">    service:</span> <span class="string">name=httpd</span> <span class="string">state=started</span></span><br><span class="line"><span class="attr">  handlers:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">restart</span> <span class="string">apache</span></span><br><span class="line"><span class="attr">      service:</span> <span class="string">name=httpd</span> <span class="string">state=restarted</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ansible 是一个开源的基于 OpenSSH 的自动化配置管理工具。可以用它来配置系统、部署软件和编排更高级的 IT 任务，比如持续部署或零停机更新。管理员可以通过 Ansible 在成百上千台计算机上同时执行指令(任务)，这是&lt;a href=&quot;https://ansible-tran.readthedocs.io/en/latest/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方文档&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Tools" scheme="http://yoursite.com/categories/Tools/"/>
    
    
      <category term="运维" scheme="http://yoursite.com/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>大数据bug记录</title>
    <link href="http://yoursite.com/2019/12/20/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AEbug%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2019/12/20/大数据/大数据bug记录/</id>
    <published>2019-12-20T03:26:05.000Z</published>
    <updated>2019-12-20T06:38:10.753Z</updated>
    
    <content type="html"><![CDATA[<p>记录遇到的简单bug</p><a id="more"></a><h1 id="201911-201912"><a class="header-anchor" href="#201911-201912"></a>201911-201912</h1><h2 id="集群时间不同步"><a class="header-anchor" href="#集群时间不同步"></a>集群时间不同步</h2><p><strong>错误详情</strong></p><p>使用 spark 时，yarn 启动节点时报错</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Application application_1576029485466_0001 failed 2 times due to Error launching </span><br><span class="line">appattempt_1576029485466_0001_000002. Got exception: org.apache.hadoop.yarn.exceptions.YarnException: </span><br><span class="line">Unauthorized request to start container.</span><br><span class="line">This token is expired. current time is 1576059076270 found 1576030868681</span><br></pre></td></tr></table></figure><p><strong>解决办法</strong></p><p>让时间同步</p><h2 id="unable-to-create-new-native-thread"><a class="header-anchor" href="#unable-to-create-new-native-thread"></a>unable to create new native thread</h2><p><strong>错误详情</strong></p><p>使用 spark 时，程序报错</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-12-11 11:34:58,366 INFO scheduler.DAGScheduler: ResultStage 0 (collect at SparkSharder.java:430) failed in 80.679 s due to Job aborted due to stage failure: Task 5428 in stage 0.0 failed 4 times, most recent failure: Lost task 5428.3 in stage 0.0 (TID 5440, dell-r730-4, executor 3): java.io.IOException: DestHost:destPort dell-r720:8020 , LocalHost:localPort dell-r730-4/10.0.0.14:0. Failed on local exception: java.io.IOException: Couldn&apos;t set up IO streams: java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)</span><br><span class="line">at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:808)</span><br><span class="line">at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)</span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1491)</span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1388)</span><br><span class="line">at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)</span><br><span class="line">at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)</span><br><span class="line">at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)</span><br><span class="line">at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:907)</span><br><span class="line">at sun.reflect.GeneratedMethodAccessor307.invoke(Unknown Source)</span><br></pre></td></tr></table></figure><p><strong>解决办法</strong></p><p>原因是超过了<code>unlimt -u</code>设定的最大线程数，把它增大即可</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改 limits.conf</span></span><br><span class="line">$ vim /etc/security/limits.conf</span><br><span class="line">...</span><br><span class="line">*       soft    nofile  65536      <span class="comment"># 文件打开数（以前改的）</span></span><br><span class="line">*       hard    nofile  65536</span><br><span class="line"></span><br><span class="line">*       hard    nproc     65536    <span class="comment"># 线程数（加上这两行）</span></span><br><span class="line">*       soft    nproc     65536</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果没有效果，则继续修改 20-nproc.conf（它限制了线程最大值）</span></span><br><span class="line">$ vim /etc/security/limits.d/20-nproc.conf</span><br><span class="line"><span class="comment"># *       soft    nproc     65536  # 修改它</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录遇到的简单bug&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="bug" scheme="http://yoursite.com/tags/bug/"/>
    
  </entry>
  
  <entry>
    <title>spark调试-常用代码</title>
    <link href="http://yoursite.com/2019/12/20/spark/spark%E8%B0%83%E8%AF%95-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2019/12/20/spark/spark调试-常用代码/</id>
    <published>2019-12-20T03:13:34.000Z</published>
    <updated>2019-12-20T03:16:15.473Z</updated>
    
    <content type="html"><![CDATA[<p>一些我用到的调试代码</p><a id="more"></a><h1 id="统计每个分区的record个数"><a class="header-anchor" href="#统计每个分区的record个数"></a>统计每个分区的record个数</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; numOfRecordPerPartition = javaRDD.mapPartitionsWithIndex(</span><br><span class="line">        ((Function2&lt;Integer, Iterator&lt;T&gt;, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt;) (Index, iterator) -&gt; &#123;</span><br><span class="line">            List&lt;Tuple2&lt;Integer, Integer&gt;&gt; numOfRecord = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">int</span> totalElement = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">                iterator.next();</span><br><span class="line">                totalElement++;</span><br><span class="line">            &#125;</span><br><span class="line">            numOfRecord.add(<span class="keyword">new</span> Tuple2&lt;&gt;(Index, totalElement));</span><br><span class="line">            <span class="keyword">return</span> numOfRecord.iterator();</span><br><span class="line">        &#125;), <span class="keyword">true</span>).collect();</span><br></pre></td></tr></table></figure><h1 id="收集每个分区的第一个record"><a class="header-anchor" href="#收集每个分区的第一个record"></a>收集每个分区的第一个record</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Tuple2&lt;Integer, T&gt;&gt; firstRecordPerPartition = javaRDD.mapPartitionsWithIndex(</span><br><span class="line">        ((Function2&lt;Integer, Iterator&lt;T&gt;, Iterator&lt;Tuple2&lt;Integer, T&gt;&gt;&gt;) (Index, iterator) -&gt; &#123;</span><br><span class="line">            List&lt;Tuple2&lt;Integer, T&gt;&gt; record = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">if</span> (iterator.hasNext())</span><br><span class="line">                record.add(<span class="keyword">new</span> Tuple2&lt;&gt;(Index, iterator.next()));</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                record.add(<span class="keyword">null</span>);</span><br><span class="line">            <span class="keyword">return</span> record.iterator();</span><br><span class="line">        &#125;), <span class="keyword">true</span>).collect();</span><br></pre></td></tr></table></figure><h1 id="查看当前RDD的PreferredLocations"><a class="header-anchor" href="#查看当前RDD的PreferredLocations"></a>查看当前RDD的PreferredLocations</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Partition[] partitions = javaRDD.rdd().getPartitions();</span><br><span class="line">List&lt;Seq&lt;String&gt;&gt; preferredLocationsList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (Partition partition : partitions) &#123;</span><br><span class="line">    preferredLocationsList.add(javaRDD.rdd().getPreferredLocations(partition));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一些我用到的调试代码&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>linux-常用命令</title>
    <link href="http://yoursite.com/2019/12/20/linux/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>http://yoursite.com/2019/12/20/linux/linux常用命令/</id>
    <published>2019-12-20T02:16:07.000Z</published>
    <updated>2020-01-07T13:24:34.682Z</updated>
    
    <content type="html"><![CDATA[<p>一些用过的linux命令</p><a id="more"></a><h1 id="文件"><a class="header-anchor" href="#文件"></a>文件</h1><ol><li><strong>查看目录内子目录所使用的空间</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ du -h  --max-depth=1</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>改变文件（夹）拥有者</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ chown (-R) zxy filename</span><br></pre></td></tr></table></figure><ol start="3"><li><strong>改变文件权限</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ chmod 755 filename</span><br></pre></td></tr></table></figure><ol start="4"><li><strong>压缩与解压</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar 压缩，-p 保留权限，-z 使用 gzip</span></span><br><span class="line">$ tar cvpfz /mnt/data/homeback.tgz /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># tar 解压</span></span><br><span class="line">$ tar xvpfz /mnt/data/homeback.tgz -C /</span><br></pre></td></tr></table></figure><ol start="5"><li><strong>查看磁盘使用情况</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ df -h</span><br></pre></td></tr></table></figure><ol start="6"><li><strong>创建软连接</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ln -s ./hadoop-2.7.7 ./hadoop</span><br></pre></td></tr></table></figure><h1 id="系统"><a class="header-anchor" href="#系统"></a>系统</h1><ol><li>端口使用</li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示 tcp，udp 的端口和进程等相关情况</span></span><br><span class="line">$ netstat -tunlp</span><br></pre></td></tr></table></figure><ol start="2"><li>批量删除进程</li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">kill</span> -9 `ps -ef |grep xxx|awk <span class="string">'&#123;print $2&#125;'</span>`</span><br></pre></td></tr></table></figure><h1 id="vim"><a class="header-anchor" href="#vim"></a>vim</h1><table><thead><tr><th>命令</th><th>功能</th></tr></thead><tbody><tr><td>gg</td><td>跳转到文件头</td></tr><tr><td>Shift+g</td><td>跳转到文件末尾</td></tr><tr><td>/字符串</td><td>从开头查找，n下一个，N上一个</td></tr><tr><td>?字符串</td><td>从底部查找</td></tr></tbody></table><h1 id="服务器"><a class="header-anchor" href="#服务器"></a>服务器</h1><ol><li><strong>添加用户</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ adduser username</span><br><span class="line">$ passwd username</span><br><span class="line"><span class="comment"># 赋予root权限</span></span><br><span class="line">$ vim /etc/sudoers</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>免密登陆</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub zxy@10.0.0.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line">$ ssh zxy@10.0.0.1 <span class="string">"cat &gt;&gt; ~/.ssh/authorized_keys"</span> &lt; ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><ol start="3"><li><strong>传输文件</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 源文件 -&gt; 目的地</span></span><br><span class="line">scp -r zxy@10.0.0.1:/home/zxy/filename  /Users/zxy/Desktop/</span><br></pre></td></tr></table></figure><ol start="4"><li><strong>配置网卡ip地址</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># (1) 修改对应网卡配置文件（centos）</span></span><br><span class="line">$ sudo vim /etc/sysconfig/network-scripts/ifcfg-em1</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2) 重启生效</span></span><br><span class="line">$ sudo service network restart</span><br><span class="line"></span><br><span class="line"><span class="comment"># (3) 查看效果</span></span><br><span class="line">$ ifconfig</span><br></pre></td></tr></table></figure><ol start="5"><li><strong>使用 nfs 服务器</strong></li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解释：</span></span><br><span class="line"><span class="comment"># /mnt/storage2/data                     用于共享的目录</span></span><br><span class="line"><span class="comment"># *                                      客户端：所有主机</span></span><br><span class="line"><span class="comment"># rw                                     可读可写</span></span><br><span class="line"><span class="comment"># sync                                   数据同步，效率低，但可以保证数据的一致性</span></span><br><span class="line"><span class="comment"># no_root_squash                         让root保持权限，也就是让客户端的root相当于服务端的root</span></span><br><span class="line"><span class="comment"># all_squash,anonuid=1001,anongid=1001   客户端写数据时，普通用户名强转成指定名字（1001）</span></span><br><span class="line"><span class="comment"># no_all_squash 默认                      客户端写数据时，保持用户名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (1) 修改 exports </span></span><br><span class="line">$ sudo vim /etc/exports </span><br><span class="line"></span><br><span class="line"><span class="comment"># /mnt/storage2/data  该目录统一用户 1001（cluster集群使用）</span></span><br><span class="line">/mnt/storage2/data *(rw,sync,no_root_squash,all_squash,anonuid=1001,anongid=1001)</span><br><span class="line"><span class="comment"># /mnt/storage2/users 该目录用于共享，保持个人用户名</span></span><br><span class="line">/mnt/storage2/users *(rw,sync,no_root_squash)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2) 服务端 挂载（更新）</span></span><br><span class="line">$ exportfs -arv</span><br><span class="line"></span><br><span class="line"><span class="comment"># (3) 客户端 挂载</span></span><br><span class="line">$ mount -t nfs storage-2:/mnt/storage2/data /home/cluster/Storage2</span><br><span class="line">$ mount -t nfs storage-2:/mnt/storage2/users /mnt/users</span><br><span class="line"></span><br><span class="line"><span class="comment"># (4) 客户端 卸载</span></span><br><span class="line">$ umount /mnt/users</span><br><span class="line"></span><br><span class="line"><span class="comment"># (5) 服务端 卸载</span></span><br><span class="line">$ exportfs -auv</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一些用过的linux命令&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="shell" scheme="http://yoursite.com/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-shuffle之SortShuffleWriter</title>
    <link href="http://yoursite.com/2019/12/04/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BSortShuffleWriter/"/>
    <id>http://yoursite.com/2019/12/04/spark/spark源码-shuffle之SortShuffleWriter/</id>
    <published>2019-12-04T06:36:32.000Z</published>
    <updated>2020-01-10T09:31:11.570Z</updated>
    
    <content type="html"><![CDATA[<p>spark 三大ShuffleWriter 之 SortShuffleWriter</p><a id="more"></a><h1 id="特点"><a class="header-anchor" href="#特点"></a>特点</h1><ul><li><p>最大特点就是<strong>支持 map-side aggregation</strong></p></li><li><p>最基础的 <code>ShuffleWriter</code>。当另外两种用不了时才选它～</p></li></ul><h1 id="大致流程"><a class="header-anchor" href="#大致流程"></a>大致流程</h1><p>首先记住有3种情况：<strong>含 aggregator 和 ordering</strong>；<strong>含 aggregator 但不含 ordering</strong>； <strong>前两种都不含</strong>。</p><p>为啥没有只含ordering的情况呢？因为不含aggregator就不做排序，永远记住<strong>ShuffleWriter阶段的排序只是为了使聚合更舒服</strong>！</p><ol><li><p><strong>选择 <code>sorter</code></strong>：情况1和2 选同一种<code>sorter</code>，情况3选另一种。我把它们称为 分支1 和 分支2</p></li><li><p><strong>读取数据</strong>：分支1把数据读进<code>PartitionedAppendOnlyMap</code>（把同一分区key相同的聚合），分支2把数据读进<code>PartitionedPairBuffer</code>（简单放入）</p></li><li><p><strong>数据数量达到阈值发生spill</strong>：这个spill文件整体是按<strong>分区顺序</strong>堆叠的。不同点是分区内部数据情况：情况1按 ordering 排序；情况2按 key 的 hash值 排序（这个排序只是为了方便聚合）；情况3 不排序</p></li><li><p><strong>合并spill文件和内存中未spill的文件，并返回分区长度数组</strong>：情况1先归并排序再聚合；情况2只聚合；情况3啥都不干</p></li><li><p><strong>根据分区长度数组生成索引文件</strong></p></li><li><p><strong>封装信息到<code>MapStatus</code>返回</strong></p></li></ol><p>总的来说就是一个 <strong>task</strong> 生成一个<strong>由 spill 文件合并形成的且聚合了的大文件</strong>和一个<strong>索引文件</strong>。</p><p>由于情况2更复杂点，以情况2为示例：<br><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/shuffle/SortShuffleWriter.jpg" alt></p><h1 id="源码"><a class="header-anchor" href="#源码"></a>源码</h1><h2 id="write-…"><a class="header-anchor" href="#write-…"></a>write(…)</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 流程1：根据是否需要 mapSideCombine 选择不同的 sorter</span></span><br><span class="line">  sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// In this case we pass neither an aggregator nor an ordering to the sorter, because we don't</span></span><br><span class="line">    <span class="comment">// care whether the keys get sorted in each partition; that will be done on the reduce side</span></span><br><span class="line">    <span class="comment">// if the operation being run is sortByKey.</span></span><br><span class="line">    <span class="comment">// 注意 ordering = None，官方解释的很清楚了，对吧</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](</span><br><span class="line">      context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 流程2-3：读取数据 与 spill</span></span><br><span class="line">  sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">  <span class="comment">// 文件名 "shuffle_" + shuffleId + "_" + mapId + "_" + reduceId + ".data"</span></span><br><span class="line">  <span class="keyword">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line">  <span class="keyword">val</span> tmp = <span class="type">Utils</span>.tempFileWith(output)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 流程4: 合并 spill文件 和 内存中未spill的文件，并返回分区长度数组</span></span><br><span class="line">    <span class="keyword">val</span> blockId = <span class="type">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class="type">IndexShuffleBlockResolver</span>.<span class="type">NOOP_REDUCE_ID</span>)</span><br><span class="line">    <span class="keyword">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class="line">    <span class="comment">// 流程5: 根据分区长度数组生成索引文件。这个步骤都是一样的，参考 BypassMergeSortShuffleWriter 篇</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class="line">    <span class="comment">// 流程6: 封装信息到 MapStatus 返回</span></span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logError(<span class="string">s"Error while deleting temp file <span class="subst">$&#123;tmp.getAbsolutePath&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="insertAll-…"><a class="header-anchor" href="#insertAll-…"></a>insertAll(…)</h2><p>流程2-3：读取数据 与 spill</p><p>主要关注 <code>PartitionedAppendOnlyMap </code> 和 <code>PartitionedPairBuffer </code></p><ul><li><p>相同点：都实现<code>WritablePartitionedPairCollection</code> trait。它们内部都是用 <code>Array</code> （key0, value0, key1, value1, key2, value2…）实现 Map 逻辑。<strong>key 是 （分区ID，原key）</strong></p></li><li><p>不同点：<code>PartitionedAppendOnlyMap </code><strong>支持添加于更新</strong> value：它使用<code>map.changeValue((getPartition(kv._1), kv._1), update)</code> 完成数据添加或者更新（聚合）。而<code>PartitionedPairBuffer </code><strong>仅支持添加</strong>。</p></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertAll</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> stop combining if we find that the reduction factor isn't high</span></span><br><span class="line">  <span class="keyword">val</span> shouldCombine = aggregator.isDefined</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 选择是否 combine</span></span><br><span class="line">  <span class="keyword">if</span> (shouldCombine) &#123;</span><br><span class="line">    <span class="comment">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class="line">    <span class="keyword">val</span> mergeValue = aggregator.get.mergeValue</span><br><span class="line">    <span class="keyword">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class="line">    <span class="keyword">var</span> kv: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="comment">// 从 aggregator 中取出 createCombiner 和 mergeValue，制作成update函数</span></span><br><span class="line">    <span class="keyword">val</span> update = (hadValue: <span class="type">Boolean</span>, oldValue: <span class="type">C</span>) =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class="keyword">else</span> createCombiner(kv._2)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;</span><br><span class="line">      <span class="comment">// 计数 + 1</span></span><br><span class="line">      addElementsRead()</span><br><span class="line">      kv = records.next()</span><br><span class="line">      <span class="comment">// combine 模式使用 PartitionedAppendOnlyMap</span></span><br><span class="line">      map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class="line">      <span class="comment">// 流程3: spill</span></span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Stick values into our buffer</span></span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;</span><br><span class="line">      addElementsRead()</span><br><span class="line">      <span class="keyword">val</span> kv = records.next()</span><br><span class="line">      <span class="comment">// 非 combine 模式使用 PartitionedPairBuffer</span></span><br><span class="line">      buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class="type">C</span>])</span><br><span class="line">      <span class="comment">// 流程3: spill</span></span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="maybeSpill-…"><a class="header-anchor" href="#maybeSpill-…"></a>maybeSpill(…)</h2><p>spill 的条件：内存申请没成功 或者 达到设定的阈值<code>numElementsForceSpillThreshold</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpill</span></span>(collection: <span class="type">C</span>, currentMemory: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> shouldSpill = <span class="literal">false</span></span><br><span class="line">  <span class="comment">// 元素个数是32的整数倍 且 大于 myMemoryThreshold</span></span><br><span class="line">  <span class="keyword">if</span> (elementsRead % <span class="number">32</span> == <span class="number">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class="line">    <span class="comment">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class="line">    <span class="keyword">val</span> amountToRequest = <span class="number">2</span> * currentMemory - myMemoryThreshold</span><br><span class="line">    <span class="comment">// 申请内存</span></span><br><span class="line">    <span class="keyword">val</span> granted = acquireMemory(amountToRequest)</span><br><span class="line">    myMemoryThreshold += granted</span><br><span class="line">    <span class="comment">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class="line">    <span class="comment">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class="line">    shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// spill条件：上面的申请没成功 或者  达到阈值</span></span><br><span class="line">  shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class="line">  <span class="comment">// Actually spill</span></span><br><span class="line">  <span class="keyword">if</span> (shouldSpill) &#123;</span><br><span class="line">    _spillCount += <span class="number">1</span></span><br><span class="line">    logSpillage(currentMemory)</span><br><span class="line">    <span class="comment">// spill 在这里发生</span></span><br><span class="line">    spill(collection)</span><br><span class="line">    _elementsRead = <span class="number">0</span></span><br><span class="line">    _memoryBytesSpilled += currentMemory</span><br><span class="line">    releaseMemory()</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="spill-…"><a class="header-anchor" href="#spill-…"></a>spill(…)</h2><p>先排序，后spill。排序方面，由于分支不同，有<strong>两个排序逻辑</strong>。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">protected</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">spill</span></span>(collection: <span class="type">WritablePartitionedPairCollection</span>[<span class="type">K</span>, <span class="type">C</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 排序</span></span><br><span class="line">  <span class="keyword">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class="line">  <span class="comment">// spill</span></span><br><span class="line">  <span class="keyword">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</span><br><span class="line">  spills += spillFile</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>PartitionedAppendOnlyMap </code>的排序逻辑：<strong>2重排序</strong>，先按分区ID排，再对分区内的数据排序（优先按 ordering 排序，否则hash）</p><p>注意这个hash排序，学过java中 == 和 equals 的区别的兄弟应该知道，hashcode 相等 是 两个对象 equals 的<strong>必要条件</strong>。这里只能保证 hashcode 相同的数据在一起，后续聚合时，<strong>还需经过比较后才能聚合</strong>（先打个预防针，后面源码会读到它）。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionKeyComparator</span></span>[<span class="type">K</span>](keyComparator: <span class="type">Comparator</span>[<span class="type">K</span>]): <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(a: (<span class="type">Int</span>, <span class="type">K</span>), b: (<span class="type">Int</span>, <span class="type">K</span>)): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> partitionDiff = a._1 - b._1</span><br><span class="line">      <span class="keyword">if</span> (partitionDiff != <span class="number">0</span>) &#123;</span><br><span class="line">        partitionDiff</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        keyComparator.compare(a._2, b._2)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// keyComparator：有 ordering 用 ordering，否则按 hash 排序。</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">val</span> keyComparator: <span class="type">Comparator</span>[<span class="type">K</span>] = ordering.getOrElse(<span class="keyword">new</span> <span class="type">Comparator</span>[<span class="type">K</span>] &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(a: <span class="type">K</span>, b: <span class="type">K</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> h1 = <span class="keyword">if</span> (a == <span class="literal">null</span>) <span class="number">0</span> <span class="keyword">else</span> a.hashCode()</span><br><span class="line">    <span class="keyword">val</span> h2 = <span class="keyword">if</span> (b == <span class="literal">null</span>) <span class="number">0</span> <span class="keyword">else</span> b.hashCode()</span><br><span class="line">    <span class="keyword">if</span> (h1 &lt; h2) <span class="number">-1</span> <span class="keyword">else</span> <span class="keyword">if</span> (h1 == h2) <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p><code>PartitionedPairBuffer</code>的排序逻辑：仅<strong>比较分区ID</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">partitionedDestructiveSortedIterator</span></span>(keyComparator: <span class="type">Option</span>[<span class="type">Comparator</span>[<span class="type">K</span>]])</span><br><span class="line">  : <span class="type">Iterator</span>[((<span class="type">Int</span>, <span class="type">K</span>), <span class="type">V</span>)] = &#123;</span><br><span class="line">  <span class="keyword">val</span> comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Sorter</span>(<span class="keyword">new</span> <span class="type">KVArraySortDataFormat</span>[(<span class="type">Int</span>, <span class="type">K</span>), <span class="type">AnyRef</span>]).sort(data, <span class="number">0</span>, curSize, comparator)</span><br><span class="line">  iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A comparator for (Int, K) pairs that orders them by only their partition ID.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionComparator</span></span>[<span class="type">K</span>]: <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] = <span class="keyword">new</span> <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(a: (<span class="type">Int</span>, <span class="type">K</span>), b: (<span class="type">Int</span>, <span class="type">K</span>)): <span class="type">Int</span> = &#123;</span><br><span class="line">    a._1 - b._1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="writePartitionedFile-…"><a class="header-anchor" href="#writePartitionedFile-…"></a>writePartitionedFile(…)</h2><p>流程4: 合并spill文件和内存中未spill的文件，并返回分区长度数组</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writePartitionedFile</span></span>(</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    outputFile: <span class="type">File</span>): <span class="type">Array</span>[<span class="type">Long</span>] = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Track location of each range in the output file</span></span><br><span class="line">  <span class="keyword">val</span> lengths = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Long</span>](numPartitions)</span><br><span class="line">  <span class="keyword">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class="line">    context.taskMetrics().shuffleWriteMetrics)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 首先知道：collection.destructiveSortedWritablePartitionedIterator(comparator) 用这玩意获取内存中的数据（未被spill），后面多次用到它</span></span><br><span class="line">  <span class="keyword">if</span> (spills.isEmpty) &#123;</span><br><span class="line">    <span class="comment">// Case where we only have in-memory data</span></span><br><span class="line">    <span class="comment">// 只有内存文件，刷进内存即可（当然排序什么的还是要的，和上一步一样的规则）</span></span><br><span class="line">    <span class="keyword">val</span> collection = <span class="keyword">if</span> (aggregator.isDefined) map <span class="keyword">else</span> buffer</span><br><span class="line">    <span class="keyword">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class="line">    <span class="keyword">while</span> (it.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> partitionId = it.nextPartition()</span><br><span class="line">      <span class="keyword">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class="line">        it.writeNext(writer)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">      <span class="comment">// 记录分区长度</span></span><br><span class="line">      lengths(partitionId) = segment.length</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 合并操作在这里：this.partitionedIterator，内部调用 merge()</span></span><br><span class="line">    <span class="keyword">for</span> ((id, elements) &lt;- <span class="keyword">this</span>.partitionedIterator) &#123;</span><br><span class="line">      <span class="keyword">if</span> (elements.hasNext) &#123;</span><br><span class="line">        <span class="keyword">for</span> (elem &lt;- elements) &#123;</span><br><span class="line">          writer.write(elem._1, elem._2)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">        lengths(id) = segment.length</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  writer.close()</span><br><span class="line">  context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class="line">  context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class="line">  context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class="line"></span><br><span class="line">  lengths</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="merge-…"><a class="header-anchor" href="#merge-…"></a>merge(…)</h2><p>把 spill文件 和 内存文件 同分区的数据放到一起计算（3种计算情况）</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(spills: <span class="type">Seq</span>[<span class="type">SpilledFile</span>], inMemory: <span class="type">Iterator</span>[((<span class="type">Int</span>, <span class="type">K</span>), <span class="type">C</span>)])</span><br><span class="line">    : <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]])] = &#123;</span><br><span class="line">  <span class="keyword">val</span> readers = spills.map(<span class="keyword">new</span> <span class="type">SpillReader</span>(_))</span><br><span class="line">  <span class="keyword">val</span> inMemBuffered = inMemory.buffered</span><br><span class="line">  (<span class="number">0</span> until numPartitions).iterator.map &#123; p =&gt;</span><br><span class="line">    <span class="comment">// 很明显这兄弟函数式编程写的很6</span></span><br><span class="line">    <span class="comment">// 主要就是把 spill文件 和 内存文件 同分区的数据放到一起计算（3种计算情况）。其实和以前的2重循环是一个意思</span></span><br><span class="line">    <span class="keyword">val</span> inMemIterator = <span class="keyword">new</span> <span class="type">IteratorForPartition</span>(p, inMemBuffered)</span><br><span class="line">    <span class="keyword">val</span> iterators = readers.map(_.readNextPartition()) ++ <span class="type">Seq</span>(inMemIterator)</span><br><span class="line">    <span class="keyword">if</span> (aggregator.isDefined) &#123;</span><br><span class="line">      <span class="comment">// Perform partial aggregation across partitions</span></span><br><span class="line">      <span class="comment">// 聚合：（分有无 ordering 两种情况）</span></span><br><span class="line">      (p, mergeWithAggregation(</span><br><span class="line">        iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ordering.isDefined) &#123;</span><br><span class="line">      <span class="comment">// No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);</span></span><br><span class="line">      <span class="comment">// sort the elements without trying to merge them</span></span><br><span class="line">      <span class="comment">// 只排序：对它们进行归并排序。</span></span><br><span class="line">      <span class="comment">// 说实话，我不觉得它会进这一分支。因为我多次强调过没有 aggregator 就必定没有 ordering</span></span><br><span class="line">      (p, mergeSort(iterators, ordering.get))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 啥都不要，直接把同分区文件 flatten</span></span><br><span class="line">      (p, iterators.iterator.flatten)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="mergeWithAggregation-…"><a class="header-anchor" href="#mergeWithAggregation-…"></a>mergeWithAggregation(…)</h2><p>聚合：（分有无 ordering 两种情况）</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">mergeWithAggregation</span></span>(</span><br><span class="line">    iterators: <span class="type">Seq</span>[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]],</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    comparator: <span class="type">Comparator</span>[<span class="type">K</span>],</span><br><span class="line">    totalOrder: <span class="type">Boolean</span>)</span><br><span class="line">    : <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] =</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span> (!totalOrder) &#123;</span><br><span class="line">    <span class="comment">// We only have a partial ordering, e.g. comparing the keys by hash code, which means that</span></span><br><span class="line">    <span class="comment">// multiple distinct keys might be treated as equal by the ordering. To deal with this, we</span></span><br><span class="line">    <span class="comment">// need to read all keys considered equal by the ordering at once and compare them.</span></span><br><span class="line">    <span class="comment">// 无 ordering ，comparator 是 hash比较器。hash值相同的是key相同的必要条件</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]] &#123;</span><br><span class="line">      <span class="keyword">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Buffers reused across elements to decrease memory allocation</span></span><br><span class="line">      <span class="keyword">val</span> keys = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">K</span>]</span><br><span class="line">      <span class="keyword">val</span> combiners = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">C</span>]</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = sorted.hasNext</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = &#123;</span><br><span class="line">        <span class="keyword">if</span> (!hasNext) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoSuchElementException</span></span><br><span class="line">        &#125;</span><br><span class="line">        keys.clear()</span><br><span class="line">        combiners.clear()</span><br><span class="line">        <span class="keyword">val</span> firstPair = sorted.next()</span><br><span class="line">        keys += firstPair._1</span><br><span class="line">        combiners += firstPair._2</span><br><span class="line">        <span class="keyword">val</span> key = firstPair._1</span><br><span class="line">        <span class="comment">// hash值相等</span></span><br><span class="line">        <span class="keyword">while</span> (sorted.hasNext &amp;&amp; comparator.compare(sorted.head._1, key) == <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">val</span> pair = sorted.next()</span><br><span class="line">          <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">          <span class="keyword">var</span> foundKey = <span class="literal">false</span></span><br><span class="line">          <span class="keyword">while</span> (i &lt; keys.size &amp;&amp; !foundKey) &#123;</span><br><span class="line">            <span class="comment">// 注意 == 。 scala 就是用 == 比较对象相等的 </span></span><br><span class="line">            <span class="keyword">if</span> (keys(i) == pair._1) &#123;</span><br><span class="line">              <span class="comment">// key 相等 就合并</span></span><br><span class="line">              combiners(i) = mergeCombiners(combiners(i), pair._2)</span><br><span class="line">              foundKey = <span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (!foundKey) &#123;</span><br><span class="line">            keys += pair._1</span><br><span class="line">            combiners += pair._2</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Note that we return an iterator of elements since we could've had many keys marked</span></span><br><span class="line">        <span class="comment">// equal by the partial order; we flatten this below to get a flat iterator of (K, C).</span></span><br><span class="line">        keys.iterator.zip(combiners.iterator)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.flatMap(i =&gt; i)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// We have a total ordering, so the objects with the same key are sequential.</span></span><br><span class="line">    <span class="comment">// 有 ordering：先归并排序，再把有相同的key的元素聚合就行了</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] &#123;</span><br><span class="line">      <span class="comment">// 归并排序</span></span><br><span class="line">      <span class="keyword">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = sorted.hasNext</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">        <span class="keyword">if</span> (!hasNext) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoSuchElementException</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> elem = sorted.next()</span><br><span class="line">        <span class="keyword">val</span> k = elem._1</span><br><span class="line">        <span class="keyword">var</span> c = elem._2</span><br><span class="line">        <span class="comment">// key 相等 就合并</span></span><br><span class="line">        <span class="keyword">while</span> (sorted.hasNext &amp;&amp; sorted.head._1 == k) &#123;</span><br><span class="line">          <span class="keyword">val</span> pair = sorted.next()</span><br><span class="line">          c = mergeCombiners(c, pair._2)</span><br><span class="line">        &#125;</span><br><span class="line">        (k, c)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark 三大ShuffleWriter 之 SortShuffleWriter&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-shuffle之UnsafeShuffleWriter</title>
    <link href="http://yoursite.com/2019/12/02/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BUnsafeShuffleWriter/"/>
    <id>http://yoursite.com/2019/12/02/spark/spark源码-shuffle之UnsafeShuffleWriter/</id>
    <published>2019-12-02T01:25:06.000Z</published>
    <updated>2020-01-11T08:48:16.767Z</updated>
    
    <content type="html"><![CDATA[<p>spark 三大ShuffleWriter 之 UnsafeShuffleWriter</p><a id="more"></a><h1 id="特点"><a class="header-anchor" href="#特点"></a>特点</h1><ul><li><p>它<strong>只适用不需要 map-side aggregation</strong> 的Shuffle操作</p></li><li><p>它使用UnSafe API操作序列化数据，而不是Java对象，减少了内存占用及因此导致的GC耗时(参考Spark 内存管理之Tungsten)，因此使用它时需要<strong>Serializer支持relocation</strong>。</p></li><li><p><strong>reduce端的分区数目小于等于 2^24</strong> (因为排序过程中需要使用是数据指针，它记录了数据的地址和分区ID，其中分区ID占24位)</p></li><li><p>溢写 &amp; 合并时使用UnSafe API直接操作序列化数据，<strong>合并时不需要反序列化数据</strong>。</p></li><li><p>溢写 &amp; 合并时<strong>可以使用fastMerge提升效率</strong>(调用NIO的transferTo方法)</p></li></ul><h1 id="大致流程"><a class="header-anchor" href="#大致流程"></a>大致流程</h1><ol><li><p>遍历数据，插入到<code>ShuffleExternalSorter</code>中。该过程完成许多事：<strong>将数据读入内存</strong> 同时 <strong>将数据指针不断写到指针数组</strong>中，当达到spill阈值，使用<code>writeSortedFile()</code>排序（排序排的是指针数组）并spill到磁盘。</p></li><li><p><code>sorter.closeAndGetSpills()</code>：spill 内存中剩余的文件，返回所有 spill 文件的元信息（重点是每个分区的长度）</p></li><li><p>合并所有 spill 文件成一个大文件（有3种合并工具选择），并返回分区长度数组</p></li><li><p>根据分区长度数组生成索引文件</p></li><li><p>封装信息到<code>MapStatus</code>返回</p></li></ol><p>总的来说就是一个 <strong>task</strong> 生成一个<strong>由 spill 文件合并形成的大文件（这玩意只是按分区排好，内部是无序的）<strong>和一个</strong>索引文件</strong>（上面流程是主要流程，重命名什么的就不写了）。<br><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/shuffle/UnsafeShuffleWriter.jpg" alt></p><h1 id="源码"><a class="header-anchor" href="#源码"></a>源码</h1><h2 id="write-…"><a class="header-anchor" href="#write-…"></a>write(…)</h2><p>这个<code>write(...)</code>方法就比较清爽了，因为步骤都封在方法里了</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public void write(scala.collection.<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  boolean success = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">      <span class="comment">// 对应流程1</span></span><br><span class="line">      insertRecordIntoSorter(records.next());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 剩余流程</span></span><br><span class="line">    closeAndWriteOutput();</span><br><span class="line">    success = <span class="literal">true</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (sorter != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        sorter.cleanupResources();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (<span class="type">Exception</span> e) &#123;</span><br><span class="line">        <span class="comment">// Only throw this error if we won't be masking another</span></span><br><span class="line">        <span class="comment">// error.</span></span><br><span class="line">        <span class="keyword">if</span> (success) &#123;</span><br><span class="line">          <span class="keyword">throw</span> e;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          logger.error(<span class="string">"In addition to a failure during writing, we failed during "</span> +</span><br><span class="line">                       <span class="string">"cleanup."</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="insertRecordIntoSorter-…"><a class="header-anchor" href="#insertRecordIntoSorter-…"></a>insertRecordIntoSorter(…)</h2><p>其核心方法是<code>sorter.insertRecord()</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">void insertRecordIntoSorter(<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert(sorter != <span class="literal">null</span>);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">K</span> key = record._1();</span><br><span class="line">  <span class="keyword">final</span> int partitionId = partitioner.getPartition(key);</span><br><span class="line">  serBuffer.reset();</span><br><span class="line">  <span class="comment">// serOutputStream: 用于序列化对象的写入的流</span></span><br><span class="line">  serOutputStream.writeKey(key, <span class="type">OBJECT_CLASS_TAG</span>);</span><br><span class="line">  serOutputStream.writeValue(record._2(), <span class="type">OBJECT_CLASS_TAG</span>);</span><br><span class="line">  serOutputStream.flush();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> int serializedRecordSize = serBuffer.size();</span><br><span class="line">  assert (serializedRecordSize &gt; <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 都是它干的</span></span><br><span class="line">  sorter.insertRecord(</span><br><span class="line">    serBuffer.getBuf(), <span class="type">Platform</span>.<span class="type">BYTE_ARRAY_OFFSET</span>, serializedRecordSize, partitionId);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="sorter-insertRecord-…"><a class="header-anchor" href="#sorter-insertRecord-…"></a>sorter.insertRecord(…)</h2><p><strong>将数据读入内存</strong> 同时 <strong>将数据指针不断写到指针数组</strong>中，<strong>当达到spill阈值，发生 spill</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public void insertRecord(<span class="type">Object</span> recordBase, long recordOffset, int length, int partitionId)</span><br><span class="line">  <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// for tests</span></span><br><span class="line">  assert(inMemSorter != <span class="literal">null</span>);</span><br><span class="line">  <span class="comment">// 如果 Sorter 内的数据超过阈值，就发生 spill</span></span><br><span class="line">  <span class="keyword">if</span> (inMemSorter.numRecords() &gt;= numElementsForSpillThreshold) &#123;</span><br><span class="line">    logger.info(<span class="string">"Spilling data because number of spilledRecords crossed the threshold "</span> +</span><br><span class="line">      numElementsForSpillThreshold);</span><br><span class="line">    spill();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 检查 inMemSorter中 的数组是否满了，如果满了就扩容</span></span><br><span class="line">  growPointerArrayIfNecessary();</span><br><span class="line">  <span class="keyword">final</span> int uaoSize = <span class="type">UnsafeAlignedOffset</span>.getUaoSize();</span><br><span class="line">  <span class="comment">// Need 4 or 8 bytes to store the record length.</span></span><br><span class="line">  <span class="keyword">final</span> int required = length + uaoSize;</span><br><span class="line">  acquireNewPageIfNecessary(required);</span><br><span class="line"></span><br><span class="line">  assert(currentPage != <span class="literal">null</span>);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">Object</span> base = currentPage.getBaseObject();</span><br><span class="line">  <span class="keyword">final</span> long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);</span><br><span class="line">  <span class="type">UnsafeAlignedOffset</span>.putSize(base, pageCursor, length);</span><br><span class="line">  pageCursor += uaoSize;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 将数据写入 MemoryBlock 中</span></span><br><span class="line">  <span class="type">Platform</span>.copyMemory(recordBase, recordOffset, base, pageCursor, length);</span><br><span class="line">  pageCursor += length;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 将数据指针信息写入inMemSorter的数组中</span></span><br><span class="line">  inMemSorter.insertRecord(recordAddress, partitionId);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="spill"><a class="header-anchor" href="#spill"></a>spill()</h2><p><code>spill()</code> 的核心是<code> writeSortedFile(false)</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public long spill(long size, <span class="type">MemoryConsumer</span> trigger) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (trigger != <span class="keyword">this</span> || inMemSorter == <span class="literal">null</span> || inMemSorter.numRecords() == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>L;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  logger.info(<span class="string">"Thread &#123;&#125; spilling sort data of &#123;&#125; to disk (&#123;&#125; &#123;&#125; so far)"</span>,</span><br><span class="line">    <span class="type">Thread</span>.currentThread().getId(),</span><br><span class="line">    <span class="type">Utils</span>.bytesToString(getMemoryUsage()),</span><br><span class="line">    spills.size(),</span><br><span class="line">    spills.size() &gt; <span class="number">1</span> ? <span class="string">" times"</span> : <span class="string">" time"</span>);</span><br><span class="line"></span><br><span class="line">  writeSortedFile(<span class="literal">false</span>);</span><br><span class="line">  <span class="keyword">final</span> long spillSize = freeMemory();</span><br><span class="line">  <span class="comment">// spill 完整。重置 inMemSorter</span></span><br><span class="line">  inMemSorter.reset();</span><br><span class="line">  <span class="comment">// Reset the in-memory sorter's pointer array only after freeing up the memory pages holding the</span></span><br><span class="line">  <span class="comment">// records. Otherwise, if the task is over allocated memory, then without freeing the memory</span></span><br><span class="line">  <span class="comment">// pages, we might not be able to get memory for the pointer array.</span></span><br><span class="line">  taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);</span><br><span class="line">  <span class="keyword">return</span> spillSize;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="writeSortedFile"><a class="header-anchor" href="#writeSortedFile"></a>writeSortedFile()</h2><p>对内存中的数据进行排序（排序排的是指针数组）并 写到磁盘</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> void writeSortedFile(boolean isLastFile) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">ShuffleWriteMetrics</span> writeMetricsToUse;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (isLastFile) &#123;</span><br><span class="line">    writeMetricsToUse = writeMetrics;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    writeMetricsToUse = <span class="keyword">new</span> <span class="type">ShuffleWriteMetrics</span>();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// This call performs the actual sort.</span></span><br><span class="line">  <span class="comment">// 迭代器：包含分区有序的数据指针（排序在这里进行，有两种排序手段）</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">ShuffleInMemorySorter</span>.<span class="type">ShuffleSorterIterator</span> sortedRecords =</span><br><span class="line">    inMemSorter.getSortedIterator();</span><br><span class="line">  <span class="keyword">final</span> byte[] writeBuffer = <span class="keyword">new</span> byte[diskWriteBufferSize];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 文件名：temp_shuffle_ + UUID</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">Tuple2</span>&lt;<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>&gt; spilledFileInfo =</span><br><span class="line">    blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> file = spilledFileInfo._2();</span><br><span class="line">  <span class="keyword">final</span> <span class="type">TempShuffleBlockId</span> blockId = spilledFileInfo._1();</span><br><span class="line">  <span class="keyword">final</span> <span class="type">SpillInfo</span> spillInfo = <span class="keyword">new</span> <span class="type">SpillInfo</span>(numPartitions, file, blockId);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">SerializerInstance</span> ser = <span class="type">DummySerializerInstance</span>.<span class="type">INSTANCE</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">DiskBlockObjectWriter</span> writer =</span><br><span class="line">    blockManager.getDiskWriter(blockId, file, ser, fileBufferSizeBytes, writeMetricsToUse);</span><br><span class="line"></span><br><span class="line">  int currentPartition = <span class="number">-1</span>;</span><br><span class="line">  <span class="keyword">final</span> int uaoSize = <span class="type">UnsafeAlignedOffset</span>.getUaoSize();</span><br><span class="line">  <span class="keyword">while</span> (sortedRecords.hasNext()) &#123;</span><br><span class="line">    sortedRecords.loadNext();</span><br><span class="line">    <span class="keyword">final</span> int partition = sortedRecords.packedRecordPointer.getPartitionId();</span><br><span class="line">    assert (partition &gt;= currentPartition);</span><br><span class="line">    <span class="keyword">if</span> (partition != currentPartition) &#123;</span><br><span class="line">      <span class="comment">// Switch to the new partition</span></span><br><span class="line">      <span class="keyword">if</span> (currentPartition != <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileSegment</span> fileSegment = writer.commitAndGet();</span><br><span class="line">        <span class="comment">// 记录每个分区的长度</span></span><br><span class="line">        spillInfo.partitionLengths[currentPartition] = fileSegment.length();</span><br><span class="line">      &#125;</span><br><span class="line">      currentPartition = partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Object</span> recordPage = taskMemoryManager.getPage(recordPointer);</span><br><span class="line">    <span class="keyword">final</span> long recordOffsetInPage = taskMemoryManager.getOffsetInPage(recordPointer);</span><br><span class="line">    int dataRemaining = <span class="type">UnsafeAlignedOffset</span>.getSize(recordPage, recordOffsetInPage);</span><br><span class="line">    long recordReadPosition = recordOffsetInPage + uaoSize; <span class="comment">// skip over record length</span></span><br><span class="line">    <span class="keyword">while</span> (dataRemaining &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">final</span> int toTransfer = <span class="type">Math</span>.min(diskWriteBufferSize, dataRemaining);</span><br><span class="line">      <span class="type">Platform</span>.copyMemory(</span><br><span class="line">        recordPage, recordReadPosition, writeBuffer, <span class="type">Platform</span>.<span class="type">BYTE_ARRAY_OFFSET</span>, toTransfer);</span><br><span class="line">      writer.write(writeBuffer, <span class="number">0</span>, toTransfer);</span><br><span class="line">      recordReadPosition += toTransfer;</span><br><span class="line">      dataRemaining -= toTransfer;</span><br><span class="line">    &#125;</span><br><span class="line">    writer.recordWritten();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">FileSegment</span> committedSegment = writer.commitAndGet();</span><br><span class="line">  writer.close();</span><br><span class="line">  <span class="keyword">if</span> (currentPartition != <span class="number">-1</span>) &#123;</span><br><span class="line">    spillInfo.partitionLengths[currentPartition] = committedSegment.length();</span><br><span class="line">    spills.add(spillInfo);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!isLastFile) &#123;  <span class="comment">// i.e. this is a spill file</span></span><br><span class="line">    writeMetrics.incRecordsWritten(writeMetricsToUse.recordsWritten());</span><br><span class="line">    taskContext.taskMetrics().incDiskBytesSpilled(writeMetricsToUse.bytesWritten());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="closeAndWriteOutput"><a class="header-anchor" href="#closeAndWriteOutput"></a>closeAndWriteOutput();</h2><p>终于完成了流程1: <code>insertRecordIntoSorter</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">void closeAndWriteOutput() <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert(sorter != <span class="literal">null</span>);</span><br><span class="line">  updatePeakMemoryUsed();</span><br><span class="line">  serBuffer = <span class="literal">null</span>;</span><br><span class="line">  serOutputStream = <span class="literal">null</span>;</span><br><span class="line">  <span class="comment">// 流程2: spill 内存中剩余的文件，返回所有 spill 文件的元信息（重点是每个分区的长度）</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">SpillInfo</span>[] spills = sorter.closeAndGetSpills();</span><br><span class="line">  sorter = <span class="literal">null</span>;</span><br><span class="line">  <span class="keyword">final</span> long[] partitionLengths;</span><br><span class="line">  <span class="comment">// 文件名："shuffle_" + shuffleId + "_" + mapId + "_" + reduceId + ".data"</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> tmp = <span class="type">Utils</span>.tempFileWith(output);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 流程3: 合并 spill 文件，并返回 spill 文件的大小，用于计算索引文件</span></span><br><span class="line">      partitionLengths = mergeSpills(spills, tmp);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">SpillInfo</span> spill : spills) &#123;</span><br><span class="line">        <span class="keyword">if</span> (spill.file.exists() &amp;&amp; ! spill.file.delete()) &#123;</span><br><span class="line">          logger.error(<span class="string">"Error while deleting spill file &#123;&#125;"</span>, spill.file.getPath());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 流程4: 生成索引文件。这个步骤都是一样的，参考 BypassMergeSortShuffleWriter</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logger.error(<span class="string">"Error while deleting temp file &#123;&#125;"</span>, tmp.getAbsolutePath());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  流程<span class="number">5</span>: 封装信息到`<span class="type">MapStatus</span>`返回</span><br><span class="line">  mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="mergeSpills-…"><a class="header-anchor" href="#mergeSpills-…"></a>mergeSpills(…)</h3><p>合并 spill 文件，有3种合并手段：</p><ul><li><p>快合并：<strong>不使用压缩，或者特定的支持拼接的压缩格式</strong>：Snappy、LZF、LZ4、ZStd</p><ol><li><p>当使用nio的<code>transferTo</code>传输 且 不需要加密时，使用 <code>mergeSpillsWithTransferTo(spills, outputFile)</code></p></li><li><p>否则使用<code>mergeSpillsWithFileStream(spills, outputFile, null)</code></p></li></ol></li><li><p>慢合并：</p><ol><li>使用<code>mergeSpillsWithFileStream(spills, outputFile, compressionCodec)</code></li></ol></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> long[] mergeSpills(<span class="type">SpillInfo</span>[] spills, <span class="type">File</span> outputFile) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  <span class="comment">// 压缩，以及压缩格式</span></span><br><span class="line">  <span class="keyword">final</span> boolean compressionEnabled = sparkConf.getBoolean(<span class="string">"spark.shuffle.compress"</span>, <span class="literal">true</span>);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">CompressionCodec</span> compressionCodec = <span class="type">CompressionCodec</span>$.<span class="type">MODULE</span>$.createCodec(sparkConf);</span><br><span class="line">  <span class="keyword">final</span> boolean fastMergeEnabled =</span><br><span class="line">    sparkConf.getBoolean(<span class="string">"spark.shuffle.unsafe.fastMergeEnabled"</span>, <span class="literal">true</span>);</span><br><span class="line">  <span class="comment">// 支持快速合并的情况：不使用压缩，或者特定的支持拼接的压缩格式：Snappy、LZF、LZ4、ZStd</span></span><br><span class="line">  <span class="keyword">final</span> boolean fastMergeIsSupported = !compressionEnabled ||</span><br><span class="line">    <span class="type">CompressionCodec</span>$.<span class="type">MODULE</span>$.supportsConcatenationOfSerializedStreams(compressionCodec);</span><br><span class="line">  <span class="comment">// 是否加密</span></span><br><span class="line">  <span class="keyword">final</span> boolean encryptionEnabled = blockManager.serializerManager().encryptionEnabled();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (spills.length == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">FileOutputStream</span>(outputFile).close(); <span class="comment">// Create an empty file</span></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> long[partitioner.numPartitions()];</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (spills.length == <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="type">Files</span>.move(spills[<span class="number">0</span>].file, outputFile);</span><br><span class="line">      <span class="keyword">return</span> spills[<span class="number">0</span>].partitionLengths;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">final</span> long[] partitionLengths;</span><br><span class="line">      <span class="keyword">if</span> (fastMergeEnabled &amp;&amp; fastMergeIsSupported) &#123;</span><br><span class="line">        <span class="comment">// 当使用nio的transferTo传输 且 不需要加密时，使用 transferTo-based fast merge</span></span><br><span class="line">        <span class="keyword">if</span> (transferToEnabled &amp;&amp; !encryptionEnabled) &#123;</span><br><span class="line">          logger.debug(<span class="string">"Using transferTo-based fast merge"</span>);</span><br><span class="line">          partitionLengths = mergeSpillsWithTransferTo(spills, outputFile);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 否则使用 fileStream-based fast merge</span></span><br><span class="line">          logger.debug(<span class="string">"Using fileStream-based fast merge"</span>);</span><br><span class="line">          partitionLengths = mergeSpillsWithFileStream(spills, outputFile, <span class="literal">null</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logger.debug(<span class="string">"Using slow merge"</span>);</span><br><span class="line">        <span class="comment">// 使用 slow merge</span></span><br><span class="line">        partitionLengths = mergeSpillsWithFileStream(spills, outputFile, compressionCodec);</span><br><span class="line">      &#125;</span><br><span class="line">      writeMetrics.decBytesWritten(spills[spills.length - <span class="number">1</span>].file.length());</span><br><span class="line">      writeMetrics.incBytesWritten(outputFile.length());</span><br><span class="line">      <span class="keyword">return</span> partitionLengths;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (<span class="type">IOException</span> e) &#123;</span><br><span class="line">    <span class="keyword">if</span> (outputFile.exists() &amp;&amp; !outputFile.delete()) &#123;</span><br><span class="line">      logger.error(<span class="string">"Unable to delete output file &#123;&#125;"</span>, outputFile.getPath());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="mergeSpillsWithTransferTo-…"><a class="header-anchor" href="#mergeSpillsWithTransferTo-…"></a>mergeSpillsWithTransferTo(…)</h3><p>由于内部流程都差不多，就举一个为例，核心是个<strong>2重循环</strong>，将所有spill文件中同一分区的数据合并，并按分区号排列</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> long[] mergeSpillsWithTransferTo(<span class="type">SpillInfo</span>[] spills, <span class="type">File</span> outputFile) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert (spills.length &gt;= <span class="number">2</span>);</span><br><span class="line">  <span class="keyword">final</span> int numPartitions = partitioner.numPartitions();</span><br><span class="line">  <span class="keyword">final</span> long[] partitionLengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">  <span class="keyword">final</span> <span class="type">FileChannel</span>[] spillInputChannels = <span class="keyword">new</span> <span class="type">FileChannel</span>[spills.length];</span><br><span class="line">  <span class="keyword">final</span> long[] spillInputChannelPositions = <span class="keyword">new</span> long[spills.length];</span><br><span class="line">  <span class="type">FileChannel</span> mergedFileOutputChannel = <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">  boolean threwException = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 对每个 spill 文件产出输入流</span></span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; spills.length; i++) &#123;</span><br><span class="line">      spillInputChannels[i] = <span class="keyword">new</span> <span class="type">FileInputStream</span>(spills[i].file).getChannel();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 合并文件 的输出流</span></span><br><span class="line">    mergedFileOutputChannel = <span class="keyword">new</span> <span class="type">FileOutputStream</span>(outputFile, <span class="literal">true</span>).getChannel();</span><br><span class="line"></span><br><span class="line">    long bytesWrittenToMergedFile = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 2重循环，结果是将所有spill文件中同一分区的数据合并，并按分区号排列</span></span><br><span class="line">    <span class="keyword">for</span> (int partition = <span class="number">0</span>; partition &lt; numPartitions; partition++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; spills.length; i++) &#123;</span><br><span class="line">        <span class="keyword">final</span> long partitionLengthInSpill = spills[i].partitionLengths[partition];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileChannel</span> spillInputChannel = spillInputChannels[i];</span><br><span class="line">        <span class="keyword">final</span> long writeStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">        <span class="comment">// 将 spill 里的 指定分区数据 写入合并文件中</span></span><br><span class="line">        <span class="type">Utils</span>.copyFileStreamNIO(</span><br><span class="line">          spillInputChannel,             <span class="comment">// spill 文件输入流</span></span><br><span class="line">          mergedFileOutputChannel,       <span class="comment">// 合并文件输出流</span></span><br><span class="line">          spillInputChannelPositions[i], <span class="comment">// spill 中 该分区起始位置</span></span><br><span class="line">          partitionLengthInSpill);       <span class="comment">// spill 中 该分区长度</span></span><br><span class="line">        spillInputChannelPositions[i] += partitionLengthInSpill;</span><br><span class="line">        writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - writeStartTime);</span><br><span class="line">        bytesWrittenToMergedFile += partitionLengthInSpill;</span><br><span class="line">        partitionLengths[partition] += partitionLengthInSpill; <span class="comment">// 所有 spill 中该分区的总长度</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (mergedFileOutputChannel.position() != bytesWrittenToMergedFile) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(</span><br><span class="line">        <span class="string">"Current position "</span> + mergedFileOutputChannel.position() + <span class="string">" does not equal expected "</span> +</span><br><span class="line">          <span class="string">"position "</span> + bytesWrittenToMergedFile + <span class="string">" after transferTo. Please check your kernel"</span> +</span><br><span class="line">          <span class="string">" version to see if it is 2.6.32, as there is a kernel bug which will lead to "</span> +</span><br><span class="line">          <span class="string">"unexpected behavior when using transferTo. You can set spark.file.transferTo=false "</span> +</span><br><span class="line">          <span class="string">"to disable this NIO feature."</span></span><br><span class="line">      );</span><br><span class="line">    &#125;</span><br><span class="line">    threwException = <span class="literal">false</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; spills.length; i++) &#123;</span><br><span class="line">      assert(spillInputChannelPositions[i] == spills[i].file.length());</span><br><span class="line">      <span class="type">Closeables</span>.close(spillInputChannels[i], threwException);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Closeables</span>.close(mergedFileOutputChannel, threwException);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> partitionLengths;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark 三大ShuffleWriter 之 UnsafeShuffleWriter&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-shuffle之BypassMergeSortShuffleWriter</title>
    <link href="http://yoursite.com/2019/11/30/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BBypassMergeSortShuffleWriter/"/>
    <id>http://yoursite.com/2019/11/30/spark/spark源码-shuffle之BypassMergeSortShuffleWriter/</id>
    <published>2019-11-30T10:54:59.000Z</published>
    <updated>2020-01-10T09:31:03.774Z</updated>
    
    <content type="html"><![CDATA[<p>spark 三大ShuffleWriter 之 BypassMergeSortShuffleWriter</p><a id="more"></a><h1 id="特点"><a class="header-anchor" href="#特点"></a>特点</h1><ul><li><p>它<strong>只适用不需要 map-side aggregation</strong>的Shuffle操作，且<strong>Reducer任务数量比较少</strong>（默认200）</p></li><li><p>数据是直接写入文件，数据量较大的时候，网络I/O和内存负担较重</p></li><li><p>写分区文件时开了多个<code>DiskBlockObjectWriter</code>，对<strong>内存消耗比较大</strong></p></li></ul><h1 id="大致流程"><a class="header-anchor" href="#大致流程"></a>大致流程</h1><ol><li><p>为每个分区都创建一个<code>DiskBlockObjectWriter</code></p></li><li><p>遍历数据，使用<code>DiskBlockObjectWriter</code>的<code>write</code>方法将数据写入到不同分区文件中</p></li><li><p>刷写分区文件到磁盘</p></li><li><p>合并分区文件成一个大文件，并返回记录每个分区文件的长度的数组</p></li><li><p>根据分区长度数组生成索引文件</p></li><li><p>封装信息到<code>MapStatus</code>返回</p></li></ol><p>总的来说就是生成了一个<strong>由分区文件合并形成的大文件</strong>和一个<strong>索引文件</strong>（上面流程是主要流程，重命名什么的就不写了）。</p><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/shuffle/BypassMergeSortShuffleWriter.jpg" alt></p><h1 id="源码"><a class="header-anchor" href="#源码"></a>源码</h1><h2 id="write-…"><a class="header-anchor" href="#write-…"></a>write(…)</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public void write(<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert (partitionWriters == <span class="literal">null</span>);</span><br><span class="line">  <span class="keyword">if</span> (!records.hasNext()) &#123;</span><br><span class="line">    partitionLengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, <span class="literal">null</span>);</span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">final</span> <span class="type">SerializerInstance</span> serInstance = serializer.newInstance();</span><br><span class="line">  <span class="keyword">final</span> long openStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">  <span class="comment">// partitionWriter 数组：分区号即是数组偏移量。它们将数据按分区号分别写入不同的个文件，有多少个分区就形成多少个文件</span></span><br><span class="line">  <span class="comment">// 这里的 numPartitions 是分区后的数量</span></span><br><span class="line">  partitionWriters = <span class="keyword">new</span> <span class="type">DiskBlockObjectWriter</span>[numPartitions];</span><br><span class="line">  partitionWriterSegments = <span class="keyword">new</span> <span class="type">FileSegment</span>[numPartitions];</span><br><span class="line">  <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Tuple2</span>&lt;<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>&gt; tempShuffleBlockIdPlusFile =</span><br><span class="line">      blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">File</span> file = tempShuffleBlockIdPlusFile._2();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">BlockId</span> blockId = tempShuffleBlockIdPlusFile._1();</span><br><span class="line">    partitionWriters[i] =</span><br><span class="line">      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Creating the file to write to and creating a disk writer both involve interacting with</span></span><br><span class="line">  <span class="comment">// the disk, and can take a long time in aggregate when we open many files, so should be</span></span><br><span class="line">  <span class="comment">// included in the shuffle write time.</span></span><br><span class="line">  writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - openStartTime);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record = records.next();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">K</span> key = record._1();</span><br><span class="line">    <span class="comment">// 按分区器的规则写入数据</span></span><br><span class="line">    partitionWriters[partitioner.getPartition(key)].write(key, record._2());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">DiskBlockObjectWriter</span> writer = partitionWriters[i];</span><br><span class="line">    <span class="comment">// 刷写数据到磁盘</span></span><br><span class="line">    partitionWriterSegments[i] = writer.commitAndGet();</span><br><span class="line">    writer.close();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 文件名："shuffle_" + shuffleId + "_" + mapId + "_" + reduceId + ".data"</span></span><br><span class="line">  <span class="type">File</span> output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">  <span class="type">File</span> tmp = <span class="type">Utils</span>.tempFileWith(output);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 合并生成的分区文件，并返回每个分区文件的大小，用于计算偏移量</span></span><br><span class="line">    partitionLengths = writePartitionedFile(tmp);</span><br><span class="line">    <span class="comment">// 生成索引文件</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="comment">// 这里可以删除是因为 writeIndexFileAndCommit 中重命名了它</span></span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logger.error(<span class="string">"Error while deleting temp file &#123;&#125;"</span>, tmp.getAbsolutePath());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="writePartitionedFile-…"><a class="header-anchor" href="#writePartitionedFile-…"></a>writePartitionedFile(…)</h2><p>合并生成的分区文件，并返回每个分区文件的大小，用于计算偏移量</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> long[] writePartitionedFile(<span class="type">File</span> outputFile) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  <span class="comment">// Track location of the partition starts in the output file</span></span><br><span class="line">  <span class="comment">// lengths数组：记录每个分区文件的大小</span></span><br><span class="line">  <span class="keyword">final</span> long[] lengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">  <span class="keyword">if</span> (partitionWriters == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="comment">// We were passed an empty iterator</span></span><br><span class="line">    <span class="keyword">return</span> lengths;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并文件</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">FileOutputStream</span> out = <span class="keyword">new</span> <span class="type">FileOutputStream</span>(outputFile, <span class="literal">true</span>);</span><br><span class="line">  <span class="keyword">final</span> long writeStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">  boolean threwException = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">File</span> file = partitionWriterSegments[i].file();</span><br><span class="line">      <span class="keyword">if</span> (file.exists()) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileInputStream</span> in = <span class="keyword">new</span> <span class="type">FileInputStream</span>(file);</span><br><span class="line">        boolean copyThrewException = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 把分区文件 拷贝到 合并文件 中，存入文件大小到lengths数组中</span></span><br><span class="line">          lengths[i] = <span class="type">Utils</span>.copyStream(in, out, <span class="literal">false</span>, transferToEnabled);</span><br><span class="line">          copyThrewException = <span class="literal">false</span>;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          <span class="type">Closeables</span>.close(in, copyThrewException);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 删除分区文件</span></span><br><span class="line">        <span class="keyword">if</span> (!file.delete()) &#123;</span><br><span class="line">          logger.error(<span class="string">"Unable to delete file for partition &#123;&#125;"</span>, i);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    threwException = <span class="literal">false</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="type">Closeables</span>.close(out, threwException);</span><br><span class="line">    writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - writeStartTime);</span><br><span class="line">  &#125;</span><br><span class="line">  partitionWriters = <span class="literal">null</span>;</span><br><span class="line">  <span class="keyword">return</span> lengths;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="writeIndexFileAndCommit-…"><a class="header-anchor" href="#writeIndexFileAndCommit-…"></a>writeIndexFileAndCommit(…)</h2><p>生成索引文件</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeIndexFileAndCommit</span></span>(</span><br><span class="line">    shuffleId: <span class="type">Int</span>,</span><br><span class="line">    mapId: <span class="type">Int</span>,</span><br><span class="line">    lengths: <span class="type">Array</span>[<span class="type">Long</span>],</span><br><span class="line">    dataTmp: <span class="type">File</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> indexFile = getIndexFile(shuffleId, mapId)</span><br><span class="line">  <span class="keyword">val</span> indexTmp = <span class="type">Utils</span>.tempFileWith(indexFile)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> dataFile = getDataFile(shuffleId, mapId)</span><br><span class="line">    <span class="comment">// There is only one IndexShuffleBlockResolver per executor, this synchronization make sure</span></span><br><span class="line">    <span class="comment">// the following check and rename are atomic.</span></span><br><span class="line">    synchronized &#123;</span><br><span class="line">      <span class="keyword">val</span> existingLengths = checkIndexAndDataFile(indexFile, dataFile, lengths.length)</span><br><span class="line">      <span class="keyword">if</span> (existingLengths != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Another attempt for the same task has already written our map outputs successfully,</span></span><br><span class="line">        <span class="comment">// so just use the existing partition lengths and delete our temporary map outputs.</span></span><br><span class="line">        <span class="type">System</span>.arraycopy(existingLengths, <span class="number">0</span>, lengths, <span class="number">0</span>, lengths.length)</span><br><span class="line">        <span class="keyword">if</span> (dataTmp != <span class="literal">null</span> &amp;&amp; dataTmp.exists()) &#123;</span><br><span class="line">          dataTmp.delete()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// This is the first successful attempt in writing the map outputs for this task,</span></span><br><span class="line">        <span class="comment">// so override any existing index and data files with the ones we wrote.</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">DataOutputStream</span>(<span class="keyword">new</span> <span class="type">BufferedOutputStream</span>(<span class="keyword">new</span> <span class="type">FileOutputStream</span>(indexTmp)))</span><br><span class="line">        <span class="type">Utils</span>.tryWithSafeFinally &#123;</span><br><span class="line">          <span class="comment">// We take in lengths of each block, need to convert it to offsets.</span></span><br><span class="line">          <span class="comment">// 索引其实就是按 分区文件大小 叠上去而已</span></span><br><span class="line">          <span class="keyword">var</span> offset = <span class="number">0</span>L</span><br><span class="line">          out.writeLong(offset)</span><br><span class="line">          <span class="keyword">for</span> (length &lt;- lengths) &#123;</span><br><span class="line">            offset += length</span><br><span class="line">            out.writeLong(offset)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; &#123;</span><br><span class="line">          out.close()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (indexFile.exists()) &#123;</span><br><span class="line">          indexFile.delete()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (dataFile.exists()) &#123;</span><br><span class="line">          dataFile.delete()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 重命名 indexTmp</span></span><br><span class="line">        <span class="keyword">if</span> (!indexTmp.renameTo(indexFile)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"fail to rename file "</span> + indexTmp + <span class="string">" to "</span> + indexFile)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 重命名 dataTmp</span></span><br><span class="line">        <span class="keyword">if</span> (dataTmp != <span class="literal">null</span> &amp;&amp; dataTmp.exists() &amp;&amp; !dataTmp.renameTo(dataFile)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"fail to rename file "</span> + dataTmp + <span class="string">" to "</span> + dataFile)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (indexTmp.exists() &amp;&amp; !indexTmp.delete()) &#123;</span><br><span class="line">      logError(<span class="string">s"Failed to delete temporary index file at <span class="subst">$&#123;indexTmp.getAbsolutePath&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark 三大ShuffleWriter 之 BypassMergeSortShuffleWriter&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-shuffle之ShuffleManager</title>
    <link href="http://yoursite.com/2019/11/29/spark/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BShuffleManager/"/>
    <id>http://yoursite.com/2019/11/29/spark/spark源码-shuffle之ShuffleManager/</id>
    <published>2019-11-29T06:45:04.000Z</published>
    <updated>2020-01-10T09:31:07.239Z</updated>
    
    <content type="html"><![CDATA[<p>spark shuffle 学习起始篇，涉及源码中shuffle的相关概念 加 SortShuffleManager</p><a id="more"></a><h1 id="Spark-里的-Shuffle"><a class="header-anchor" href="#Spark-里的-Shuffle"></a>Spark 里的 Shuffle</h1><p>shuffle 主要分为:</p><ul><li><strong>ShuffleWrite</strong> 阶段：上一个stage 的尾任务<code>ShuffleMapTask</code> 把数据写入磁盘，也叫<code>ShuffleMap</code></li><li><strong>ShuffleRead</strong> 阶段： 下一个stage 拉取数据，也叫<code>ShuffleReduce</code></li></ul><p>源码中的一些概念：</p><ul><li><p>如果把 spark 整个流程看成一辆火车，那么除了最后一节是<code>ResultStage </code>，其它每一节车厢就是一个<code>ShuffleMapStage </code>，连接车厢的部分就是<code>shuffle</code>。</p></li><li><p>车厢头进行 <strong>read</strong>，车厢尾进行 <strong>write</strong>。很容易理解<code>ShuffleMapStage</code>需要读前一个stage内容，也需要把输出写入下一个stage；而<code>ResultStage</code>只需要读。这些可以在源码中发现。</p></li><li><p><code>ShuffleMapStage</code>对应 <code>ShuffleMapTask</code>，<code>ResultStage</code> 对应 <code>ResultTask</code>。</p></li><li><p><strong>read</strong> 实质是<code>ShuffleReader</code>里的 <code>read()</code>方法；<strong>write</strong> 是实质是<code>ShuffleWriter</code>里的 <code>write()</code>方法。</p></li><li><p><code>ShuffleReader</code> 和 <code>ShuffleWriter</code> 这两大组件都由<code>ShuffleManager</code>进行选择。</p></li></ul><p>好了，脑子里有了这些概念，就可以对这3个模块进行仔细研究了。</p><h1 id="SortShuffleManager"><a class="header-anchor" href="#SortShuffleManager"></a>SortShuffleManager</h1><p>由于 Spark 2.0以后，<code>ShuffleManager</code>只提供一种实现：<code>SortShuffleManager</code>，因此只深入研究它。</p><p>以下是<code>ShuffleMapTask</code>中的写操作。可以发现 <code>shuffleManager</code> 是 <code>SparkEnv</code> 中的属性。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 从 SparkEnv 中 得到 shuffleManager</span></span><br><span class="line"><span class="keyword">val</span> manager = <span class="type">SparkEnv</span>.get.shuffleManager</span><br><span class="line"><span class="comment">// 从 shuffleManager 中得到 ShuffleWriter</span></span><br><span class="line">writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class="line"><span class="comment">// 执行 ShuffleWriter 里的 write 方法</span></span><br><span class="line">writer.write(rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])</span><br><span class="line"><span class="comment">// 成功写入，收尾工作</span></span><br><span class="line">writer.stop(success = <span class="literal">true</span>).get</span><br></pre></td></tr></table></figure><p>这里事先预告下，有3种<code>ShuffleWriter</code>，1种<code>ShuffleReader</code>。</p><p>那么如何选择呢？注意上面，它取决于<code>dep.shuffleHandle</code>，而它来自<code>shuffleManager</code>的<code>registerShuffle()</code>方法：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> shuffleHandle: <span class="type">ShuffleHandle</span> = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">  shuffleId, _rdd.partitions.length, <span class="keyword">this</span>)</span><br></pre></td></tr></table></figure><p>好的，让我们进入<code>SortShuffleManager</code>中一探究竟。</p><h2 id="registerShuffle-…"><a class="header-anchor" href="#registerShuffle-…"></a>registerShuffle(…)</h2><p>其实就是选择 <code>handle</code> 的过程</p><ol><li><p>如果 <strong>不需要map端的聚合操作</strong> 且 <strong>shuffle 后的分区数量小于等于200</strong>（<code>spark.shuffle.sort.bypassMergeThreshold</code>），就选择 <code>BypassMergeSortShuffleHandle</code>。否则进入第二步</p></li><li><p>如果 <strong>序列化器支持重定位</strong> 且 <strong>不需要map端聚合</strong> 且 <strong>shuffle 后的分区数目小于等于2^24)</strong>，就选择 <code>SerializedShuffleHandle</code>。否则进入第三步</p></li><li><p>选择 <code>BaseShuffleHandle</code></p></li></ol><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    shuffleId: <span class="type">Int</span>,</span><br><span class="line">    numMaps: <span class="type">Int</span>,</span><br><span class="line">    dependency: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]): <span class="type">ShuffleHandle</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, numMaps, dependency)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="getWriter-…"><a class="header-anchor" href="#getWriter-…"></a>getWriter(…)</h2><p>根据上面得到的<code>handle</code>，进行模式匹配选择<code>ShuffleWriter</code>，有3种：</p><p><code>BypassMergeSortHandle</code>  --&gt; <code>BypassMergeSortShuffleWriter</code></p><p><code>SerializedShuffleHandle</code> --&gt; <code>UnsafeShuffleWriter</code></p><p><code>other(BaseShuffleHandle)</code> --&gt; <code>SortShuffleWriter</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getWriter</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">    handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">    mapId: <span class="type">Int</span>,</span><br><span class="line">    context: <span class="type">TaskContext</span>): <span class="type">ShuffleWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">  numMapsForShuffle.putIfAbsent(</span><br><span class="line">    handle.shuffleId, handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[_, _, _]].numMaps)</span><br><span class="line">  <span class="keyword">val</span> env = <span class="type">SparkEnv</span>.get</span><br><span class="line">  handle <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> unsafeShuffleHandle: <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">UnsafeShuffleWriter</span>(</span><br><span class="line">        env.blockManager,</span><br><span class="line">        shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">        context.taskMemoryManager(),</span><br><span class="line">        unsafeShuffleHandle,</span><br><span class="line">        mapId,</span><br><span class="line">        context,</span><br><span class="line">        env.conf)</span><br><span class="line">    <span class="keyword">case</span> bypassMergeSortHandle: <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleWriter</span>(</span><br><span class="line">        env.blockManager,</span><br><span class="line">        shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">        bypassMergeSortHandle,</span><br><span class="line">        mapId,</span><br><span class="line">        context,</span><br><span class="line">        env.conf)</span><br><span class="line">    <span class="keyword">case</span> other: <span class="type">BaseShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>, _] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">SortShuffleWriter</span>(shuffleBlockResolver, other, mapId, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="getReader-…"><a class="header-anchor" href="#getReader-…"></a>getReader(…)</h2><p>只有一种<code>ShuffleReader</code>：<code>BlockStoreShuffleReader</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getReader</span></span>[<span class="type">K</span>, <span class="type">C</span>](</span><br><span class="line">    handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">    startPartition: <span class="type">Int</span>,</span><br><span class="line">    endPartition: <span class="type">Int</span>,</span><br><span class="line">    context: <span class="type">TaskContext</span>): <span class="type">ShuffleReader</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">BlockStoreShuffleReader</span>(</span><br><span class="line">    handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[<span class="type">K</span>, _, <span class="type">C</span>]], startPartition, endPartition, context)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark shuffle 学习起始篇，涉及源码中shuffle的相关概念 加 SortShuffleManager&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-Partitioner</title>
    <link href="http://yoursite.com/2019/11/28/spark/spark%E6%BA%90%E7%A0%81-Partitioner/"/>
    <id>http://yoursite.com/2019/11/28/spark/spark源码-Partitioner/</id>
    <published>2019-11-28T01:42:43.000Z</published>
    <updated>2020-01-10T09:31:00.590Z</updated>
    
    <content type="html"><![CDATA[<p>Partitioner（分区器）学习</p><a id="more"></a><h1 id="Partitioner"><a class="header-anchor" href="#Partitioner"></a>Partitioner</h1><p>分区器，RDD五大特性之五（只针对（k,v）类型的RDD）。它的核心作用是使用 <code>getPartition(key: Any)</code>对每条数据进行分区</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它有两大实现，<code>HashPartitioner</code> 和 <code>RangePartitioner</code>。分区是为了并行处理，所以让每个分区的大小差不多是首要目标。</p><h1 id="HashPartitioner"><a class="header-anchor" href="#HashPartitioner"></a>HashPartitioner</h1><p>这个是最简单的，直接通过 key 的 hashCode 取模分区，能让数据大致均匀地分布在各个分区。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class HashPartitioner(partitions: Int) extends Partitioner &#123;</span><br><span class="line">  require(partitions &gt;= 0, s&quot;Number of partitions ($partitions) cannot be negative.&quot;)</span><br><span class="line"></span><br><span class="line">  def numPartitions: Int = partitions</span><br><span class="line"></span><br><span class="line">  // 看这里</span><br><span class="line">  def getPartition(key: Any): Int = key match &#123;</span><br><span class="line">    case null =&gt; 0</span><br><span class="line">    case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def equals(other: Any): Boolean = other match &#123;</span><br><span class="line">    case h: HashPartitioner =&gt;</span><br><span class="line">      h.numPartitions == numPartitions</span><br><span class="line">    case _ =&gt;</span><br><span class="line">      false</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def hashCode: Int = numPartitions</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="RangePartitioner"><a class="header-anchor" href="#RangePartitioner"></a>RangePartitioner</h1><p>既然有了hash分区，为什么还要range分区呢。事想需要全局排序时，如果使用hash分区，排序后只是分区有序，然后再对分区进行归并排序，这样工作量是不是特别大。所以排序时一般用 <code>RangePartitioner</code> ，比如 <code>sortByKey</code>。它的效果让是一个分区中的元素肯定都是比另一个分区内的元素小或者大。这样分区排序后的数据就是全局有序的。并且它通过采样操作可以让数据比较均匀地分布到各个分区。</p><p>它的大致步骤是：对每个分区进行采样（蓄水池采样） -&gt; 判断每个分区的采样结果是否合格，如果不合格再次采样 -&gt; 把采样数据排序，每条采样数据都有权重，按权重，计算出分解边界数组<code>rangeBounds</code> -&gt; 按边界，把数据划分到不同分区<code>getPartition(key: Any)</code>。</p><h2 id="rangeBounds"><a class="header-anchor" href="#rangeBounds"></a>rangeBounds</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> rangeBounds: <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (partitions &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="type">Array</span>.empty</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// This is the sample size we need to have roughly balanced output partitions, capped at 1M.</span></span><br><span class="line">    <span class="comment">// Cast to double to avoid overflowing ints or longs</span></span><br><span class="line">    <span class="comment">// 总采样点的个数，不超过 1e6，注意 partitions 是分区后的分区个数</span></span><br><span class="line">    <span class="keyword">val</span> sampleSize = math.min(samplePointsPerPartitionHint.toDouble * partitions, <span class="number">1e6</span>)</span><br><span class="line">    <span class="comment">// Assume the input partitions are roughly balanced and over-sample a little bit.</span></span><br><span class="line">    <span class="comment">// 每个分区的采样点个数，并乘了3进行过采样</span></span><br><span class="line">    <span class="keyword">val</span> sampleSizePerPartition = math.ceil(<span class="number">3.0</span> * sampleSize / rdd.partitions.length).toInt</span><br><span class="line">    <span class="comment">// 使用蓄水池采样法（见下）进行采样，返回总数据个数，和每个分区的采样情况(partitionId, 该分区数据总个数, sample)</span></span><br><span class="line">    <span class="keyword">val</span> (numItems, sketched) = <span class="type">RangePartitioner</span>.sketch(rdd.map(_._1), sampleSizePerPartition)</span><br><span class="line">    <span class="keyword">if</span> (numItems == <span class="number">0</span>L) &#123;</span><br><span class="line">      <span class="type">Array</span>.empty</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// If a partition contains much more than the average number of items, we re-sample from it</span></span><br><span class="line">      <span class="comment">// to ensure that enough items are collected from that partition.</span></span><br><span class="line">      <span class="keyword">val</span> fraction = math.min(sampleSize / math.max(numItems, <span class="number">1</span>L), <span class="number">1.0</span>)</span><br><span class="line">      <span class="keyword">val</span> candidates = <span class="type">ArrayBuffer</span>.empty[(<span class="type">K</span>, <span class="type">Float</span>)]</span><br><span class="line">      <span class="keyword">val</span> imbalancedPartitions = mutable.<span class="type">Set</span>.empty[<span class="type">Int</span>]</span><br><span class="line">      sketched.foreach &#123; <span class="keyword">case</span> (idx, n, sample) =&gt;</span><br><span class="line">        <span class="comment">// 如果一个分区采样过多，就重新采样它</span></span><br><span class="line">        <span class="keyword">if</span> (fraction * n &gt; sampleSizePerPartition) &#123;</span><br><span class="line">          imbalancedPartitions += idx</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// The weight is 1 over the sampling probability.</span></span><br><span class="line">          <span class="comment">// 权重 = 该分区数据总个数 / 采样点数</span></span><br><span class="line">          <span class="keyword">val</span> weight = (n.toDouble / sample.length).toFloat</span><br><span class="line">          <span class="keyword">for</span> (key &lt;- sample) &#123;</span><br><span class="line">            candidates += ((key, weight))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (imbalancedPartitions.nonEmpty) &#123;</span><br><span class="line">        <span class="comment">// Re-sample imbalanced partitions with the desired sampling probability.</span></span><br><span class="line">        <span class="comment">// 重新采样</span></span><br><span class="line">        <span class="keyword">val</span> imbalanced = <span class="keyword">new</span> <span class="type">PartitionPruningRDD</span>(rdd.map(_._1), imbalancedPartitions.contains)</span><br><span class="line">        <span class="keyword">val</span> seed = byteswap32(-rdd.id - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> reSampled = imbalanced.sample(withReplacement = <span class="literal">false</span>, fraction, seed).collect()</span><br><span class="line">        <span class="comment">// 以采样率的倒数做权重</span></span><br><span class="line">        <span class="keyword">val</span> weight = (<span class="number">1.0</span> / fraction).toFloat</span><br><span class="line">        candidates ++= reSampled.map(x =&gt; (x, weight))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 返回每个分区边界数据的数组（见下），数组长度为分区个数 - 1 (很好理解，切4份西瓜，需要3刀)</span></span><br><span class="line">      <span class="type">RangePartitioner</span>.determineBounds(candidates, math.min(partitions, candidates.size))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="蓄水池采样算法（Reservoir-Sampling）"><a class="header-anchor" href="#蓄水池采样算法（Reservoir-Sampling）"></a>蓄水池采样算法（Reservoir Sampling）</h2><ul><li>场景：数据流长度N很大且不可知，不能一次性存入内存；保证时间复杂度为O(N)；随机选取k个数，每个数被选中的概率为 k/N。</li><li>步骤：<ol><li>如果数据总量小于k，则依次放入蓄水池。池子满了，进入步骤2。</li><li>当遍历到第i个数据时，在[0, i]范围内取以随机数d，若d的落在[0, k-1]范围内，则用该数据替换蓄水池中的第d个数据。</li><li>重复步骤2，直到遍历完。</li></ol></li></ul><p>想深究原理的看<a href="https://www.jianshu.com/p/7a9ea6ece2af" target="_blank" rel="noopener">这个</a>，下面是 spark 中对该算法是实现。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sketch</span></span>[<span class="type">K</span> : <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">K</span>],</span><br><span class="line">    sampleSizePerPartition: <span class="type">Int</span>): (<span class="type">Long</span>, <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Long</span>, <span class="type">Array</span>[<span class="type">K</span>])]) = &#123;</span><br><span class="line">  <span class="keyword">val</span> shift = rdd.id</span><br><span class="line">  <span class="comment">// val classTagK = classTag[K] // to avoid serializing the entire partitioner object</span></span><br><span class="line">  <span class="keyword">val</span> sketched = rdd.mapPartitionsWithIndex &#123; (idx, iter) =&gt;</span><br><span class="line">    <span class="keyword">val</span> seed = byteswap32(idx ^ (shift &lt;&lt; <span class="number">16</span>))</span><br><span class="line">    <span class="keyword">val</span> (sample, n) = <span class="type">SamplingUtils</span>.reservoirSampleAndCount(</span><br><span class="line">      iter, sampleSizePerPartition, seed)</span><br><span class="line">    <span class="type">Iterator</span>((idx, n, sample))</span><br><span class="line">  &#125;.collect()</span><br><span class="line">  <span class="keyword">val</span> numItems = sketched.map(_._2).sum</span><br><span class="line">  (numItems, sketched)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心步骤是通过 <code>SamplingUtils.reservoirSampleAndCount(xxx)</code> 得到采样结果</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reservoirSampleAndCount</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    input: <span class="type">Iterator</span>[<span class="type">T</span>],</span><br><span class="line">    k: <span class="type">Int</span>,</span><br><span class="line">    seed: <span class="type">Long</span> = <span class="type">Random</span>.nextLong())</span><br><span class="line">  : (<span class="type">Array</span>[<span class="type">T</span>], <span class="type">Long</span>) = &#123;</span><br><span class="line">  <span class="keyword">val</span> reservoir = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](k) <span class="comment">// 装采样点的蓄水池</span></span><br><span class="line">  <span class="comment">// Put the first k elements in the reservoir.</span></span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> (i &lt; k &amp;&amp; input.hasNext) &#123;</span><br><span class="line">    <span class="keyword">val</span> item = input.next()</span><br><span class="line">    reservoir(i) = item</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If we have consumed all the elements, return them. Otherwise do the replacement.</span></span><br><span class="line">  <span class="keyword">if</span> (i &lt; k) &#123;</span><br><span class="line">    <span class="comment">// If input size &lt; k, trim the array to return only an array of input size.</span></span><br><span class="line">    <span class="comment">// 如果数据总个数不足采样个数，那就全部采样了，然后返回</span></span><br><span class="line">    <span class="keyword">val</span> trimReservoir = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](i)</span><br><span class="line">    <span class="type">System</span>.arraycopy(reservoir, <span class="number">0</span>, trimReservoir, <span class="number">0</span>, i)</span><br><span class="line">    (trimReservoir, i)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// If input size &gt; k, continue the sampling process.</span></span><br><span class="line">    <span class="keyword">var</span> l = i.toLong</span><br><span class="line">    <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">XORShiftRandom</span>(seed)</span><br><span class="line">    <span class="comment">// 对整个 input 遍历一次</span></span><br><span class="line">    <span class="keyword">while</span> (input.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> item = input.next()</span><br><span class="line">      l += <span class="number">1</span></span><br><span class="line">      <span class="comment">// There are k elements in the reservoir, and the l-th element has been</span></span><br><span class="line">      <span class="comment">// consumed. It should be chosen with probability k/l. The expression</span></span><br><span class="line">      <span class="comment">// below is a random long chosen uniformly from [0,l)</span></span><br><span class="line">      <span class="keyword">val</span> replacementIndex = (rand.nextDouble() * l).toLong <span class="comment">// 取[0,l)的随机数d</span></span><br><span class="line">      <span class="comment">// 如果 d 在 k 的范围内，则用 item 替换池子里的第d个数据。</span></span><br><span class="line">      <span class="keyword">if</span> (replacementIndex &lt; k) &#123;</span><br><span class="line">        reservoir(replacementIndex.toInt) = item</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    (reservoir, l)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="按权重选择边界"><a class="header-anchor" href="#按权重选择边界"></a>按权重选择边界</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Determines the bounds for range partitioning from candidates with weights indicating how many</span></span><br><span class="line"><span class="comment">  * items each represents. Usually this is 1 over the probability used to sample this candidate.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param candidates unordered candidates with weights</span></span><br><span class="line"><span class="comment">  * @param partitions number of partitions</span></span><br><span class="line"><span class="comment">  * @return selected bounds</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">determineBounds</span></span>[<span class="type">K</span> : <span class="type">Ordering</span> : <span class="type">ClassTag</span>](</span><br><span class="line">     candidates: <span class="type">ArrayBuffer</span>[(<span class="type">K</span>, <span class="type">Float</span>)],</span><br><span class="line">     partitions: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">   <span class="keyword">val</span> ordering = implicitly[<span class="type">Ordering</span>[<span class="type">K</span>]]</span><br><span class="line">   <span class="keyword">val</span> ordered = candidates.sortBy(_._1) <span class="comment">// 把采样点排好序</span></span><br><span class="line">   <span class="keyword">val</span> numCandidates = ordered.size</span><br><span class="line">   <span class="keyword">val</span> sumWeights = ordered.map(_._2.toDouble).sum</span><br><span class="line">   <span class="keyword">val</span> step = sumWeights / partitions</span><br><span class="line">   <span class="keyword">var</span> cumWeight = <span class="number">0.0</span></span><br><span class="line">   <span class="keyword">var</span> target = step</span><br><span class="line">   <span class="keyword">val</span> bounds = <span class="type">ArrayBuffer</span>.empty[<span class="type">K</span>]</span><br><span class="line">   <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">   <span class="keyword">var</span> j = <span class="number">0</span></span><br><span class="line">   <span class="keyword">var</span> previousBound = <span class="type">Option</span>.empty[<span class="type">K</span>]</span><br><span class="line">   <span class="keyword">while</span> ((i &lt; numCandidates) &amp;&amp; (j &lt; partitions - <span class="number">1</span>)) &#123;</span><br><span class="line">     <span class="keyword">val</span> (key, weight) = ordered(i)</span><br><span class="line">     cumWeight += weight</span><br><span class="line">     <span class="keyword">if</span> (cumWeight &gt;= target) &#123;</span><br><span class="line">       <span class="comment">// Skip duplicate values.</span></span><br><span class="line">       <span class="keyword">if</span> (previousBound.isEmpty || ordering.gt(key, previousBound.get)) &#123;</span><br><span class="line">         bounds += key</span><br><span class="line">         target += step</span><br><span class="line">         j += <span class="number">1</span></span><br><span class="line">         previousBound = <span class="type">Some</span>(key)</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     i += <span class="number">1</span></span><br><span class="line">   &#125;</span><br><span class="line">   bounds.toArray</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h2 id="getPartition-key-Any"><a class="header-anchor" href="#getPartition-key-Any"></a>getPartition(key: Any)</h2><p>有了边界，<code>getPartition(key: Any)</code>就很好计算了，其实就是个在有序区间找位置的过程。分区少就一个个比过去，如果区间数大于128，就使用二分查找获取分区位置。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">K</span>]</span><br><span class="line">  <span class="keyword">var</span> partition = <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> (rangeBounds.length &lt;= <span class="number">128</span>) &#123;</span><br><span class="line">    <span class="comment">// If we have less than 128 partitions naive search</span></span><br><span class="line">    <span class="keyword">while</span> (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) &#123;</span><br><span class="line">      partition += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 二分查找</span></span><br><span class="line">    partition = binarySearch(rangeBounds, k)</span><br><span class="line">    <span class="comment">// binarySearch either returns the match location or -[insertion point]-1</span></span><br><span class="line">    <span class="keyword">if</span> (partition &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      partition = -partition<span class="number">-1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (partition &gt; rangeBounds.length) &#123;</span><br><span class="line">      partition = rangeBounds.length</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 根据升序还是降序，返回相应的PartitionId。</span></span><br><span class="line">  <span class="keyword">if</span> (ascending) &#123;</span><br><span class="line">    partition</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    rangeBounds.length - partition</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Partitioner（分区器）学习&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark杂谈-使用textFile读取HDFS的分区规则</title>
    <link href="http://yoursite.com/2019/11/27/spark/spark%E6%9D%82%E8%B0%88-%E4%BD%BF%E7%94%A8textFile%E8%AF%BB%E5%8F%96HDFS%E7%9A%84%E5%88%86%E5%8C%BA%E8%A7%84%E5%88%99/"/>
    <id>http://yoursite.com/2019/11/27/spark/spark杂谈-使用textFile读取HDFS的分区规则/</id>
    <published>2019-11-27T06:43:54.000Z</published>
    <updated>2020-01-10T09:31:18.859Z</updated>
    
    <content type="html"><![CDATA[<p>使用 textFile 读取HDFS的数据分区规则</p><a id="more"></a><h1 id="跟着源码走"><a class="header-anchor" href="#跟着源码走"></a>跟着源码走</h1><p>测试文件：大小 516.06 MB ，54个 block，blockSize 大小是128M，但每个 block 里面的数据只有10M 左右</p><p><strong>1. 进入 <code>sc.textFile()</code></strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> textFile: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"hdfs://xxxx"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// textFile() 有个默认值：minPartitions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions)</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 它是取 defaultParallelism 和 2 的最小值 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">defaultMinPartitions</span></span>: <span class="type">Int</span> = math.min(defaultParallelism, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这个 defaultParallelism 不指定就是 totalCores，我这里是4</span></span><br><span class="line">scheduler.conf.getInt(<span class="string">"spark.default.parallelism"</span>, totalCores)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 所以 defaultMinPartitions 最终为2</span></span><br></pre></td></tr></table></figure><p><strong>2. 创建 <code>HadoopRDD</code></strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">HadoopRDD</span>(</span><br><span class="line">  <span class="keyword">this</span>,</span><br><span class="line">  confBroadcast,</span><br><span class="line">  <span class="type">Some</span>(setInputPathsFunc),</span><br><span class="line">  inputFormatClass,</span><br><span class="line">  keyClass,</span><br><span class="line">  valueClass,</span><br><span class="line">  minPartitions).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3. 每个RDD 都有一个 <code>getPartitions</code> 函数，由它得到分区号</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">val</span> allInputSplits = getInputFormat(jobConf).getSplits(jobConf, minPartitions)</span><br><span class="line">  ...</span><br><span class="line">   <span class="keyword">val</span> array = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Partition</span>](inputSplits.size)</span><br><span class="line">   <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until inputSplits.size) &#123;</span><br><span class="line">   array(i) = <span class="keyword">new</span> <span class="type">HadoopPartition</span>(id, i, inputSplits(i))</span><br><span class="line">   &#125;</span><br><span class="line">   array</span><br><span class="line">   &#125;</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>4. 采用<code>FileInputFormat</code> 里的 <code>getSplits()</code> 划分分区，先计算 splitSize</strong></p><p><code>getPartitions</code> 的 核心是 <code>getSplits()</code>，下面是计算分区关键步骤</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 总大小除2，为258M</span></span><br><span class="line">long goalSize = totalSize / (long)(numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这是人为设定的分区最小值，这个很好理解</span></span><br><span class="line">long minSize = <span class="type">Math</span>.max(job.getLong(<span class="string">"mapreduce.input.fileinputformat.split.minsize"</span>, <span class="number">1</span>L), <span class="keyword">this</span>.minSplitSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">// HDFS 文件的块大小，128M</span></span><br><span class="line">long blockSize = file.getBlockSize();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算 splitSize</span></span><br><span class="line">long splitSize = <span class="keyword">this</span>.computeSplitSize(goalSize, minSize, blockSize);</span><br></pre></td></tr></table></figure><ul><li><p>假设文件大小为 20M： <code>splitSize = max（1，min(10,128)) = 10M</code></p></li><li><p>假设文件大小为 516M：<code>splitSize  = max(1, min(258,128)) = 128M </code>（本文）</p></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> long computeSplitSize(long goalSize, long minSize, long blockSize) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="type">Math</span>.max(minSize, <span class="type">Math</span>.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>5. 最后，按 splitSize 切分区</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 可以发现为了防止最后一个分区过小的问题，引入了数字 1.1，保证最后一个分区的大小大于 splitSize  的 10%</span></span><br><span class="line"><span class="keyword">for</span>(bytesRemaining = length; (double)bytesRemaining / (double)splitSize &gt; <span class="number">1.1</span>D; bytesRemaining -= splitSize) &#123;</span><br><span class="line">splitHosts = <span class="keyword">this</span>.getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);</span><br><span class="line">splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, splitSize, splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>L) &#123;</span><br><span class="line">splitHosts = <span class="keyword">this</span>.getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, bytesRemaining, clusterMap);</span><br><span class="line">splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, bytesRemaining, splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>6 分区结果</strong></p><p>每块128M，最后一块略大，符合预期。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs://xxxx:0+134217728</span><br><span class="line">hdfs://xxxx:134217728+134217728</span><br><span class="line">hdfs://xxxx:268435456+134217728</span><br><span class="line">hdfs://xxxx:402653184+138476793</span><br></pre></td></tr></table></figure><h1 id="小结"><a class="header-anchor" href="#小结"></a>小结</h1><ul><li><p>这里比较奇葩的是 minPartitions 这个设定，它最大只能是2。我觉得之所以这样设定，是防止文件切的过小。假设整个文件大小只有5M，公式：<code>Math.min(goalSize, blockSize)</code> blockSize假定128M，此时 splitSize 由 minPartitions 决定（不考虑人为设定的那个minSize）。那么它最多只能被切成2份。</p></li><li><p>当文件较大时（大于blockSize两倍），只和 blockSize 有关。尽管我的测试文件中每个 block 实际大小只有10M，然鹅这个并没有什么软用。</p></li><li><p>这是单文件情况，如果是读一个目录下的多文件，那就是单独对每个文件进行切分。（从源码可以发现，其中的 totalSize 是所有文件大小总和）。</p></li><li><p>当然这只是分区划分，实际读取数据没这么简单。假如我们是一条一条读，那么如果该分区最后一条数据没读完，它会接着向下一块继续读，参考<a href="https://hadoopi.wordpress.com/2013/05/27/understand-recordreader-inputsplit/" target="_blank" rel="noopener">它</a>。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用 textFile 读取HDFS的数据分区规则&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>CentOS根目录扩容</title>
    <link href="http://yoursite.com/2019/11/23/linux/CentOS%E6%A0%B9%E7%9B%AE%E5%BD%95%E6%89%A9%E5%AE%B9/"/>
    <id>http://yoursite.com/2019/11/23/linux/CentOS根目录扩容/</id>
    <published>2019-11-23T08:40:56.000Z</published>
    <updated>2019-11-23T09:13:42.301Z</updated>
    
    <content type="html"><![CDATA[<p>记录对 /root 目录的扩容</p><a id="more"></a><h1 id="问题：-root-的空间用满了"><a class="header-anchor" href="#问题：-root-的空间用满了"></a>问题：<code>/root</code> 的空间用满了</h1><p>本来打算直接动态扩容，也就是按鸟哥写的放大LV容量，把 <code>/home</code> 的空间分点给 <code>/root</code> 。结果发现 xfs 文件系统只支持动态增加，不能减少。因此咱只能备份重装了。</p><h1 id="解决："><a class="header-anchor" href="#解决："></a>解决：</h1><h2 id="1-我的版本"><a class="header-anchor" href="#1-我的版本"></a>1 我的版本</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /etc/redhat-release</span><br><span class="line">CentOS Linux release 7.6.1810 (Core)</span><br></pre></td></tr></table></figure><h2 id="2-分区情况"><a class="header-anchor" href="#2-分区情况"></a>2 分区情况</h2><p>CentOS 的 <code>/root</code> 和 <code>/home</code> 目录使用了LVM（逻辑卷分区）</p><p>我准备给 <code>/root</code> 加100G，把 <code>/home</code> 改为700G，预留 50G</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ df -hl</span><br><span class="line">Filesystem           Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/cl-root   50G   46G  5.0G  91% /</span><br><span class="line">...</span><br><span class="line">/dev/mapper/cl-home  849G  220G  629G  26% /home</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们要调的就是这个LV Size</span></span><br><span class="line">$ lvdisplay</span><br><span class="line">  ...</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/home</span><br><span class="line">  LV Name                home</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                &lt;849.07 GiB</span><br><span class="line">  Current LE             217361</span><br><span class="line"></span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/root</span><br><span class="line">  LV Name                root</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                50.00 GiB</span><br><span class="line">  Current LE             12800</span><br></pre></td></tr></table></figure><h2 id="3-备份-home"><a class="header-anchor" href="#3-备份-home"></a>3 备份 <code>/home</code></h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar 压缩，-p 保留权限，-z 使用 gzip</span></span><br><span class="line">$ tar cvpfz /mnt/data/homeback.tgz /home</span><br></pre></td></tr></table></figure><h2 id="4-删除-home"><a class="header-anchor" href="#4-删除-home"></a>4 删除 <code>/home</code></h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 干掉/home文件系统的进程</span></span><br><span class="line">$ fuser -km /home/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载/home，如果没用，加 -l 强制卸载</span></span><br><span class="line">$ umount /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 /home 的lv</span></span><br><span class="line">$ lvremove /dev/mapper/cl-home</span><br></pre></td></tr></table></figure><h2 id="5-扩容-root"><a class="header-anchor" href="#5-扩容-root"></a>5 扩容 <code>/root</code></h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我这里加 100G</span></span><br><span class="line">$ lvextend -L +100G /dev/mapper/cl-root</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刷新 /root文件系统</span></span><br><span class="line">$ xfs_growfs /dev/mapper/cl-root</span><br></pre></td></tr></table></figure><h2 id="6-恢复-home"><a class="header-anchor" href="#6-恢复-home"></a>6 恢复 <code>/home</code></h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分 700G 给它。（预留 50G 的 空闲空间）</span></span><br><span class="line">$ lvcreate -L 700G -n /dev/mapper/cl-home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建文件系统</span></span><br><span class="line">$ mkfs.xfs /dev/mapper/cl-home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂载 /home</span></span><br><span class="line">$ mount /dev/mapper/cl-home /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件恢复</span></span><br><span class="line">$ tar xvpfz /mnt/data/homeback.tgz -C /</span><br></pre></td></tr></table></figure><h2 id="7-检查结果"><a class="header-anchor" href="#7-检查结果"></a>7 检查结果</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ df -hl</span><br><span class="line">Filesystem           Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/cl-root  150G   46G  105G  31% /</span><br><span class="line">...</span><br><span class="line">/dev/mapper/cl-home  700G  220G  480G  32% /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查VG，还有 50G 剩余，稳稳的</span></span><br><span class="line">$ vgdisplay</span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               cl</span><br><span class="line">  Cur LV                3</span><br><span class="line">  ...</span><br><span class="line">  VG Size               &lt;930.51 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              238210</span><br><span class="line">  Alloc PE / Size       225648 / &lt;881.44 GiB</span><br><span class="line">  Free  PE / Size       12562 / 49.07 GiB</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 检查LV，和预想一样</span></span><br><span class="line">$ lvdisplay</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/root</span><br><span class="line">  LV Name                root</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                150.00 GiB</span><br><span class="line">  Current LE             38400</span><br><span class="line"></span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/home</span><br><span class="line">  LV Name                home</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                700.00 GiB</span><br><span class="line">  Current LE             179200</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录对 /root 目录的扩容&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="LVM" scheme="http://yoursite.com/tags/LVM/"/>
    
  </entry>
  
</feed>
