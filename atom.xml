<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZxysHexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-11-29T06:50:21.706Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zouxxyy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>spark-shuffle之ShuffleManager</title>
    <link href="http://yoursite.com/2019/11/29/spark-shuffle%E4%B9%8BShuffleManager/"/>
    <id>http://yoursite.com/2019/11/29/spark-shuffle之ShuffleManager/</id>
    <published>2019-11-29T06:45:04.000Z</published>
    <updated>2019-11-29T06:50:21.706Z</updated>
    
    <content type="html"><![CDATA[<p>spark shuffle 学习起始篇，涉及源码中shuffle的相关概念 加 SortShuffleManager</p><a id="more"></a><p>spark 版本 2.4.3</p><h1 id="Spark-里的-Shuffle"><a href="#Spark-里的-Shuffle" class="headerlink" title="Spark 里的 Shuffle"></a>Spark 里的 Shuffle</h1><p>shuffle 主要分为:</p><ul><li><strong>ShuffleWrite</strong> 阶段：上一个stage 的尾任务<code>ShuffleMapTask</code> 把数据写入磁盘，也叫<code>ShuffleMap</code></li><li><strong>ShuffleRead</strong> 阶段： 下一个stage 拉取数据，也叫<code>ShuffleReduce</code></li></ul><p>源码中的一些概念：</p><ul><li><p>如果把 spark 整个流程看成一辆火车，那么除了最后一节是<code>ResultStage</code>，其它每一节车厢就是一个<code>ShuffleMapStage</code>，连接车厢的部分就是<code>shuffle</code>。</p></li><li><p>车厢头进行 <strong>read</strong>，车厢尾进行 <strong>write</strong>。很容易理解<code>ShuffleMapStage</code>需要读前一个stage内容，也需要把输出写入下一个stage；而<code>ResultStage</code>只需要读。这些可以在源码中发现。</p></li><li><p><strong>read</strong> 实质是<code>ShuffleReader</code>里的 <code>read()</code>方法；<strong>write</strong> 是实质是<code>ShuffleReader</code>里的 <code>write()</code>方法。</p></li><li><p><code>ShuffleReader</code> 和 <code>ShuffleWriter</code> 这两大组件都由<code>ShuffleManager</code>进行选择。</p></li></ul><p>好了，脑子里有了这些概念，就可以对这3个模块进行仔细研究了。</p><h1 id="SortShuffleManager"><a href="#SortShuffleManager" class="headerlink" title="SortShuffleManager"></a>SortShuffleManager</h1><p>由于 Spark 2.0以后，<code>ShuffleManager</code>只提供一种实现：<code>SortShuffleManager</code>，因此只深入研究它。</p><p>以下是<code>ShuffleMapTask</code>中的写操作。可以发现 <code>shuffleManager</code> 是 <code>SparkEnv</code> 中的属性。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从 SparkEnv 中 得到 shuffleManager</span></span><br><span class="line"><span class="keyword">val</span> manager = <span class="type">SparkEnv</span>.get.shuffleManager</span><br><span class="line"><span class="comment">// 从 shuffleManager 中得到 ShuffleWriter</span></span><br><span class="line">writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class="line"><span class="comment">// 执行 ShuffleWriter 里的 write 方法</span></span><br><span class="line">writer.write(rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])</span><br><span class="line"><span class="comment">// 成功写入，收尾工作</span></span><br><span class="line">writer.stop(success = <span class="literal">true</span>).get</span><br></pre></td></tr></table></figure><p>这里事先预告下，有3种<code>ShuffleWriter</code>，1种<code>ShuffleReader</code>。</p><p>那么如何选择呢？注意上面，它取决于<code>dep.shuffleHandle</code>，而它来自<code>shuffleManager</code>的<code>registerShuffle()</code>方法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> shuffleHandle: <span class="type">ShuffleHandle</span> = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">  shuffleId, _rdd.partitions.length, <span class="keyword">this</span>)</span><br></pre></td></tr></table></figure><p>好的，让我们进入<code>SortShuffleManager</code>中一探究竟。</p><h2 id="registerShuffle-…"><a href="#registerShuffle-…" class="headerlink" title="registerShuffle(…)"></a>registerShuffle(…)</h2><p>其实就是选择 <code>handle</code> 的过程</p><ol><li>如果 <strong>不需要map端的聚合操作</strong> 且 <strong>shuffle 后的分区数量小于等于200</strong>（<code>spark.shuffle.sort.bypassMergeThreshold</code>），就选择 <code>BypassMergeSortShuffleHandle</code>。否则进入第二步</li></ol><ol start="2"><li><p>如果 <strong>序列化器支持重定位</strong> 且 <strong>不需要map端聚合</strong> 且 <strong>shuffle 后的分区数目小于等于2^24)</strong>，就选择 <code>SerializedShuffleHandle</code>。否则进入第三步</p></li><li><p>选择 <code>BaseShuffleHandle</code></p></li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    shuffleId: <span class="type">Int</span>,</span><br><span class="line">    numMaps: <span class="type">Int</span>,</span><br><span class="line">    dependency: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]): <span class="type">ShuffleHandle</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, numMaps, dependency)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="getWriter-…"><a href="#getWriter-…" class="headerlink" title="getWriter(…)"></a>getWriter(…)</h2><p>根据上面得到的<code>handle</code>，进行模式匹配选择<code>ShuffleWriter</code>，有3种：</p><p><code>BypassMergeSortHandle</code>  –&gt; <code>BypassMergeSortShuffleWriter</code></p><p><code>SerializedShuffleHandle</code> –&gt; <code>UnsafeShuffleWriter</code></p><p><code>other(BaseShuffleHandle)</code> –&gt; <code>SortShuffleWriter</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getWriter</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">    handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">    mapId: <span class="type">Int</span>,</span><br><span class="line">    context: <span class="type">TaskContext</span>): <span class="type">ShuffleWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">  numMapsForShuffle.putIfAbsent(</span><br><span class="line">    handle.shuffleId, handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[_, _, _]].numMaps)</span><br><span class="line">  <span class="keyword">val</span> env = <span class="type">SparkEnv</span>.get</span><br><span class="line">  handle <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> unsafeShuffleHandle: <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">UnsafeShuffleWriter</span>(</span><br><span class="line">        env.blockManager,</span><br><span class="line">        shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">        context.taskMemoryManager(),</span><br><span class="line">        unsafeShuffleHandle,</span><br><span class="line">        mapId,</span><br><span class="line">        context,</span><br><span class="line">        env.conf)</span><br><span class="line">    <span class="keyword">case</span> bypassMergeSortHandle: <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleWriter</span>(</span><br><span class="line">        env.blockManager,</span><br><span class="line">        shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">        bypassMergeSortHandle,</span><br><span class="line">        mapId,</span><br><span class="line">        context,</span><br><span class="line">        env.conf)</span><br><span class="line">    <span class="keyword">case</span> other: <span class="type">BaseShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>, _] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">SortShuffleWriter</span>(shuffleBlockResolver, other, mapId, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="getReader-…"><a href="#getReader-…" class="headerlink" title="getReader(…)"></a>getReader(…)</h2><p>只有一种<code>ShuffleReader</code>：<code>BlockStoreShuffleReader</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getReader</span></span>[<span class="type">K</span>, <span class="type">C</span>](</span><br><span class="line">    handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">    startPartition: <span class="type">Int</span>,</span><br><span class="line">    endPartition: <span class="type">Int</span>,</span><br><span class="line">    context: <span class="type">TaskContext</span>): <span class="type">ShuffleReader</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">BlockStoreShuffleReader</span>(</span><br><span class="line">    handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[<span class="type">K</span>, _, <span class="type">C</span>]], startPartition, endPartition, context)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark shuffle 学习起始篇，涉及源码中shuffle的相关概念 加 SortShuffleManager&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark-Partitioner学习</title>
    <link href="http://yoursite.com/2019/11/28/spark-Partitioner%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/11/28/spark-Partitioner学习/</id>
    <published>2019-11-28T01:42:43.000Z</published>
    <updated>2019-11-28T01:54:09.314Z</updated>
    
    <content type="html"><![CDATA[<p>Partitioner（分区器）学习</p><a id="more"></a><p>spark 版本 2.4.3</p><h1 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h1><p>分区器，RDD五大特性之五（只针对（k,v）类型的RDD）。它的核心作用是使用 <code>getPartition(key: Any)</code>对每条数据进行分区</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它有两大实现，<code>HashPartitioner</code> 和 <code>RangePartitioner</code>。分区是为了并行处理，所以让每个分区的大小差不多是首要目标。</p><h1 id="HashPartitioner"><a href="#HashPartitioner" class="headerlink" title="HashPartitioner"></a>HashPartitioner</h1><p>这个是最简单的，直接通过 key 的 hashCode 取模分区，能让数据大致均匀地分布在各个分区。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class HashPartitioner(partitions: Int) extends Partitioner &#123;</span><br><span class="line">  require(partitions &gt;= 0, s&quot;Number of partitions ($partitions) cannot be negative.&quot;)</span><br><span class="line"></span><br><span class="line">  def numPartitions: Int = partitions</span><br><span class="line"></span><br><span class="line">  // 看这里</span><br><span class="line">  def getPartition(key: Any): Int = key match &#123;</span><br><span class="line">    case null =&gt; 0</span><br><span class="line">    case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def equals(other: Any): Boolean = other match &#123;</span><br><span class="line">    case h: HashPartitioner =&gt;</span><br><span class="line">      h.numPartitions == numPartitions</span><br><span class="line">    case _ =&gt;</span><br><span class="line">      false</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def hashCode: Int = numPartitions</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="RangePartitioner"><a href="#RangePartitioner" class="headerlink" title="RangePartitioner"></a>RangePartitioner</h1><p>既然有了hash分区，为什么还要range分区呢。事想需要全局排序时，如果使用hash分区，排序后只是分区有序，然后再对分区进行归并排序，这样工作量是不是特别大。所以排序时一般用 <code>RangePartitioner</code> ，比如 <code>sortByKey</code>。它的效果让是一个分区中的元素肯定都是比另一个分区内的元素小或者大。这样分区排序后的数据就是全局有序的。并且它通过采样操作可以让数据比较均匀地分布到各个分区。</p><p>它的大致步骤是：对每个分区进行采样（蓄水池采样） -&gt; 判断每个分区的采样结果是否合格，如果不合格再次采样 -&gt; 把采样数据排序，每条采样数据都有权重，按权重，计算出分解边界数组<code>rangeBounds</code> -&gt; 按边界，把数据划分到不同分区<code>getPartition(key: Any)</code>。</p><h2 id="rangeBounds"><a href="#rangeBounds" class="headerlink" title="rangeBounds"></a>rangeBounds</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> rangeBounds: <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (partitions &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="type">Array</span>.empty</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// This is the sample size we need to have roughly balanced output partitions, capped at 1M.</span></span><br><span class="line">    <span class="comment">// Cast to double to avoid overflowing ints or longs</span></span><br><span class="line">    <span class="comment">// 总采样点的个数，不超过 1e6，注意 partitions 是分区后的分区个数</span></span><br><span class="line">    <span class="keyword">val</span> sampleSize = math.min(samplePointsPerPartitionHint.toDouble * partitions, <span class="number">1e6</span>)</span><br><span class="line">    <span class="comment">// Assume the input partitions are roughly balanced and over-sample a little bit.</span></span><br><span class="line">    <span class="comment">// 每个分区的采样点个数，并乘了3进行过采样</span></span><br><span class="line">    <span class="keyword">val</span> sampleSizePerPartition = math.ceil(<span class="number">3.0</span> * sampleSize / rdd.partitions.length).toInt</span><br><span class="line">    <span class="comment">// 使用蓄水池采样法（见下）进行采样，返回总数据个数，和每个分区的采样情况(partitionId, 该分区数据总个数, sample)</span></span><br><span class="line">    <span class="keyword">val</span> (numItems, sketched) = <span class="type">RangePartitioner</span>.sketch(rdd.map(_._1), sampleSizePerPartition)</span><br><span class="line">    <span class="keyword">if</span> (numItems == <span class="number">0</span>L) &#123;</span><br><span class="line">      <span class="type">Array</span>.empty</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// If a partition contains much more than the average number of items, we re-sample from it</span></span><br><span class="line">      <span class="comment">// to ensure that enough items are collected from that partition.</span></span><br><span class="line">      <span class="keyword">val</span> fraction = math.min(sampleSize / math.max(numItems, <span class="number">1</span>L), <span class="number">1.0</span>)</span><br><span class="line">      <span class="keyword">val</span> candidates = <span class="type">ArrayBuffer</span>.empty[(<span class="type">K</span>, <span class="type">Float</span>)]</span><br><span class="line">      <span class="keyword">val</span> imbalancedPartitions = mutable.<span class="type">Set</span>.empty[<span class="type">Int</span>]</span><br><span class="line">      sketched.foreach &#123; <span class="keyword">case</span> (idx, n, sample) =&gt;</span><br><span class="line">        <span class="comment">// 如果一个分区采样过多，就重新采样它</span></span><br><span class="line">        <span class="keyword">if</span> (fraction * n &gt; sampleSizePerPartition) &#123;</span><br><span class="line">          imbalancedPartitions += idx</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// The weight is 1 over the sampling probability.</span></span><br><span class="line">          <span class="comment">// 权重 = 该分区数据总个数 / 采样点数</span></span><br><span class="line">          <span class="keyword">val</span> weight = (n.toDouble / sample.length).toFloat</span><br><span class="line">          <span class="keyword">for</span> (key &lt;- sample) &#123;</span><br><span class="line">            candidates += ((key, weight))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (imbalancedPartitions.nonEmpty) &#123;</span><br><span class="line">        <span class="comment">// Re-sample imbalanced partitions with the desired sampling probability.</span></span><br><span class="line">        <span class="comment">// 重新采样</span></span><br><span class="line">        <span class="keyword">val</span> imbalanced = <span class="keyword">new</span> <span class="type">PartitionPruningRDD</span>(rdd.map(_._1), imbalancedPartitions.contains)</span><br><span class="line">        <span class="keyword">val</span> seed = byteswap32(-rdd.id - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> reSampled = imbalanced.sample(withReplacement = <span class="literal">false</span>, fraction, seed).collect()</span><br><span class="line">        <span class="comment">// 以采样率的倒数做权重</span></span><br><span class="line">        <span class="keyword">val</span> weight = (<span class="number">1.0</span> / fraction).toFloat</span><br><span class="line">        candidates ++= reSampled.map(x =&gt; (x, weight))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 返回每个分区边界数据的数组（见下），数组长度为分区个数 - 1 (很好理解，切4份西瓜，需要3刀)</span></span><br><span class="line">      <span class="type">RangePartitioner</span>.determineBounds(candidates, math.min(partitions, candidates.size))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="蓄水池采样算法（Reservoir-Sampling）"><a href="#蓄水池采样算法（Reservoir-Sampling）" class="headerlink" title="蓄水池采样算法（Reservoir Sampling）"></a>蓄水池采样算法（Reservoir Sampling）</h2><ul><li>场景：数据流长度N很大且不可知，不能一次性存入内存；保证时间复杂度为O(N)；随机选取k个数，每个数被选中的概率为 k/N。</li><li>步骤：<ol><li>如果数据总量小于k，则依次放入蓄水池。池子满了，进入步骤2。</li><li>当遍历到第i个数据时，在[0, i]范围内取以随机数d，若d的落在[0, k-1]范围内，则用该数据替换蓄水池中的第d个数据。</li><li>重复步骤2，直到遍历完。</li></ol></li></ul><p>想深究原理的看<a href="https://www.jianshu.com/p/7a9ea6ece2af" target="_blank" rel="noopener">这个</a>，下面是 spark 中对该算法是实现。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sketch</span></span>[<span class="type">K</span> : <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">K</span>],</span><br><span class="line">    sampleSizePerPartition: <span class="type">Int</span>): (<span class="type">Long</span>, <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Long</span>, <span class="type">Array</span>[<span class="type">K</span>])]) = &#123;</span><br><span class="line">  <span class="keyword">val</span> shift = rdd.id</span><br><span class="line">  <span class="comment">// val classTagK = classTag[K] // to avoid serializing the entire partitioner object</span></span><br><span class="line">  <span class="keyword">val</span> sketched = rdd.mapPartitionsWithIndex &#123; (idx, iter) =&gt;</span><br><span class="line">    <span class="keyword">val</span> seed = byteswap32(idx ^ (shift &lt;&lt; <span class="number">16</span>))</span><br><span class="line">    <span class="keyword">val</span> (sample, n) = <span class="type">SamplingUtils</span>.reservoirSampleAndCount(</span><br><span class="line">      iter, sampleSizePerPartition, seed)</span><br><span class="line">    <span class="type">Iterator</span>((idx, n, sample))</span><br><span class="line">  &#125;.collect()</span><br><span class="line">  <span class="keyword">val</span> numItems = sketched.map(_._2).sum</span><br><span class="line">  (numItems, sketched)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心步骤是通过 <code>SamplingUtils.reservoirSampleAndCount(xxx)</code> 得到采样结果</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reservoirSampleAndCount</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    input: <span class="type">Iterator</span>[<span class="type">T</span>],</span><br><span class="line">    k: <span class="type">Int</span>,</span><br><span class="line">    seed: <span class="type">Long</span> = <span class="type">Random</span>.nextLong())</span><br><span class="line">  : (<span class="type">Array</span>[<span class="type">T</span>], <span class="type">Long</span>) = &#123;</span><br><span class="line">  <span class="keyword">val</span> reservoir = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](k) <span class="comment">// 装采样点的蓄水池</span></span><br><span class="line">  <span class="comment">// Put the first k elements in the reservoir.</span></span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> (i &lt; k &amp;&amp; input.hasNext) &#123;</span><br><span class="line">    <span class="keyword">val</span> item = input.next()</span><br><span class="line">    reservoir(i) = item</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If we have consumed all the elements, return them. Otherwise do the replacement.</span></span><br><span class="line">  <span class="keyword">if</span> (i &lt; k) &#123;</span><br><span class="line">    <span class="comment">// If input size &lt; k, trim the array to return only an array of input size.</span></span><br><span class="line">    <span class="comment">// 如果数据总个数不足采样个数，那就全部采样了，然后返回</span></span><br><span class="line">    <span class="keyword">val</span> trimReservoir = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](i)</span><br><span class="line">    <span class="type">System</span>.arraycopy(reservoir, <span class="number">0</span>, trimReservoir, <span class="number">0</span>, i)</span><br><span class="line">    (trimReservoir, i)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// If input size &gt; k, continue the sampling process.</span></span><br><span class="line">    <span class="keyword">var</span> l = i.toLong</span><br><span class="line">    <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">XORShiftRandom</span>(seed)</span><br><span class="line">    <span class="comment">// 对整个 input 遍历一次</span></span><br><span class="line">    <span class="keyword">while</span> (input.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> item = input.next()</span><br><span class="line">      l += <span class="number">1</span></span><br><span class="line">      <span class="comment">// There are k elements in the reservoir, and the l-th element has been</span></span><br><span class="line">      <span class="comment">// consumed. It should be chosen with probability k/l. The expression</span></span><br><span class="line">      <span class="comment">// below is a random long chosen uniformly from [0,l)</span></span><br><span class="line">      <span class="keyword">val</span> replacementIndex = (rand.nextDouble() * l).toLong <span class="comment">// 取[0,l)的随机数d</span></span><br><span class="line">      <span class="comment">// 如果 d 在 k 的范围内，则用 item 替换池子里的第d个数据。</span></span><br><span class="line">      <span class="keyword">if</span> (replacementIndex &lt; k) &#123;</span><br><span class="line">        reservoir(replacementIndex.toInt) = item</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    (reservoir, l)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="按权重选择边界"><a href="#按权重选择边界" class="headerlink" title="按权重选择边界"></a>按权重选择边界</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Determines the bounds for range partitioning from candidates with weights indicating how many</span></span><br><span class="line"><span class="comment">  * items each represents. Usually this is 1 over the probability used to sample this candidate.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param candidates unordered candidates with weights</span></span><br><span class="line"><span class="comment">  * @param partitions number of partitions</span></span><br><span class="line"><span class="comment">  * @return selected bounds</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">determineBounds</span></span>[<span class="type">K</span> : <span class="type">Ordering</span> : <span class="type">ClassTag</span>](</span><br><span class="line">     candidates: <span class="type">ArrayBuffer</span>[(<span class="type">K</span>, <span class="type">Float</span>)],</span><br><span class="line">     partitions: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">   <span class="keyword">val</span> ordering = implicitly[<span class="type">Ordering</span>[<span class="type">K</span>]]</span><br><span class="line">   <span class="keyword">val</span> ordered = candidates.sortBy(_._1) <span class="comment">// 把采样点排好序</span></span><br><span class="line">   <span class="keyword">val</span> numCandidates = ordered.size</span><br><span class="line">   <span class="keyword">val</span> sumWeights = ordered.map(_._2.toDouble).sum</span><br><span class="line">   <span class="keyword">val</span> step = sumWeights / partitions</span><br><span class="line">   <span class="keyword">var</span> cumWeight = <span class="number">0.0</span></span><br><span class="line">   <span class="keyword">var</span> target = step</span><br><span class="line">   <span class="keyword">val</span> bounds = <span class="type">ArrayBuffer</span>.empty[<span class="type">K</span>]</span><br><span class="line">   <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">   <span class="keyword">var</span> j = <span class="number">0</span></span><br><span class="line">   <span class="keyword">var</span> previousBound = <span class="type">Option</span>.empty[<span class="type">K</span>]</span><br><span class="line">   <span class="keyword">while</span> ((i &lt; numCandidates) &amp;&amp; (j &lt; partitions - <span class="number">1</span>)) &#123;</span><br><span class="line">     <span class="keyword">val</span> (key, weight) = ordered(i)</span><br><span class="line">     cumWeight += weight</span><br><span class="line">     <span class="keyword">if</span> (cumWeight &gt;= target) &#123;</span><br><span class="line">       <span class="comment">// Skip duplicate values.</span></span><br><span class="line">       <span class="keyword">if</span> (previousBound.isEmpty || ordering.gt(key, previousBound.get)) &#123;</span><br><span class="line">         bounds += key</span><br><span class="line">         target += step</span><br><span class="line">         j += <span class="number">1</span></span><br><span class="line">         previousBound = <span class="type">Some</span>(key)</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     i += <span class="number">1</span></span><br><span class="line">   &#125;</span><br><span class="line">   bounds.toArray</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h2 id="getPartition-key-Any"><a href="#getPartition-key-Any" class="headerlink" title="getPartition(key: Any)"></a>getPartition(key: Any)</h2><p>有了边界，<code>getPartition(key: Any)</code>就很好计算了，其实就是个在有序区间找位置的过程。分区少就一个个比过去，如果区间数大于128，就使用二分查找获取分区位置。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">K</span>]</span><br><span class="line">  <span class="keyword">var</span> partition = <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> (rangeBounds.length &lt;= <span class="number">128</span>) &#123;</span><br><span class="line">    <span class="comment">// If we have less than 128 partitions naive search</span></span><br><span class="line">    <span class="keyword">while</span> (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) &#123;</span><br><span class="line">      partition += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 二分查找</span></span><br><span class="line">    partition = binarySearch(rangeBounds, k)</span><br><span class="line">    <span class="comment">// binarySearch either returns the match location or -[insertion point]-1</span></span><br><span class="line">    <span class="keyword">if</span> (partition &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      partition = -partition<span class="number">-1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (partition &gt; rangeBounds.length) &#123;</span><br><span class="line">      partition = rangeBounds.length</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 根据升序还是降序，返回相应的PartitionId。</span></span><br><span class="line">  <span class="keyword">if</span> (ascending) &#123;</span><br><span class="line">    partition</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    rangeBounds.length - partition</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Partitioner（分区器）学习&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark-使用textFile读取HDFS的分区规则</title>
    <link href="http://yoursite.com/2019/11/27/spark-%E4%BD%BF%E7%94%A8textFile%E8%AF%BB%E5%8F%96HDFS%E7%9A%84%E5%88%86%E5%8C%BA%E8%A7%84%E5%88%99/"/>
    <id>http://yoursite.com/2019/11/27/spark-使用textFile读取HDFS的分区规则/</id>
    <published>2019-11-27T06:43:54.000Z</published>
    <updated>2019-11-28T01:52:08.508Z</updated>
    
    <content type="html"><![CDATA[<p>使用 textFile 读取HDFS的数据分区规则</p><a id="more"></a><p>spark 版本 2.4.3</p><h1 id="跟着源码走"><a href="#跟着源码走" class="headerlink" title="跟着源码走"></a>跟着源码走</h1><p>测试文件：大小 516.06 MB ，54个 block，blockSize 大小是128M，但每个 block 里面的数据只有10M 左右</p><p><strong>1. 进入 <code>sc.textFile()</code></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFile: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"hdfs://xxxx"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// textFile() 有个默认值：minPartitions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions)</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 它是取 defaultParallelism 和 2 的最小值 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">defaultMinPartitions</span></span>: <span class="type">Int</span> = math.min(defaultParallelism, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这个 defaultParallelism 不指定就是 totalCores，我这里是4</span></span><br><span class="line">scheduler.conf.getInt(<span class="string">"spark.default.parallelism"</span>, totalCores)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 所以 defaultMinPartitions 最终为2</span></span><br></pre></td></tr></table></figure><p><strong>2. 创建 <code>HadoopRDD</code></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">HadoopRDD</span>(</span><br><span class="line">  <span class="keyword">this</span>,</span><br><span class="line">  confBroadcast,</span><br><span class="line">  <span class="type">Some</span>(setInputPathsFunc),</span><br><span class="line">  inputFormatClass,</span><br><span class="line">  keyClass,</span><br><span class="line">  valueClass,</span><br><span class="line">  minPartitions).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3. 每个RDD 都有一个 <code>getPartitions</code> 函数，由它得到分区号</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">val</span> allInputSplits = getInputFormat(jobConf).getSplits(jobConf, minPartitions)</span><br><span class="line">  ...</span><br><span class="line">   <span class="keyword">val</span> array = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Partition</span>](inputSplits.size)</span><br><span class="line">   <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until inputSplits.size) &#123;</span><br><span class="line">   array(i) = <span class="keyword">new</span> <span class="type">HadoopPartition</span>(id, i, inputSplits(i))</span><br><span class="line">   &#125;</span><br><span class="line">   array</span><br><span class="line">   &#125;</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>4. 采用<code>FileInputFormat</code> 里的 <code>getSplits()</code> 划分分区，先计算 splitSize</strong></p><p><code>getPartitions</code> 的 核心是 <code>getSplits()</code>，下面是计算分区关键步骤</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 总大小除2，为258M</span></span><br><span class="line">long goalSize = totalSize / (long)(numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这是人为设定的分区最小值，这个很好理解</span></span><br><span class="line">long minSize = <span class="type">Math</span>.max(job.getLong(<span class="string">"mapreduce.input.fileinputformat.split.minsize"</span>, <span class="number">1</span>L), <span class="keyword">this</span>.minSplitSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">// HDFS 文件的块大小，128M</span></span><br><span class="line">long blockSize = file.getBlockSize();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算 splitSize</span></span><br><span class="line">long splitSize = <span class="keyword">this</span>.computeSplitSize(goalSize, minSize, blockSize);</span><br></pre></td></tr></table></figure><ul><li><p>假设文件大小为 20M： <code>splitSize = max（1，min(10,128)) = 10M</code></p></li><li><p>假设文件大小为 516M：<code>splitSize  = max(1, min(258,128)) = 128M</code>（本文）</p></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> long computeSplitSize(long goalSize, long minSize, long blockSize) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="type">Math</span>.max(minSize, <span class="type">Math</span>.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>5. 最后，按 splitSize 切分区</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 可以发现为了防止最后一个分区过小的问题，引入了数字 1.1，保证最后一个分区的大小大于 splitSize  的 10%</span></span><br><span class="line"><span class="keyword">for</span>(bytesRemaining = length; (double)bytesRemaining / (double)splitSize &gt; <span class="number">1.1</span>D; bytesRemaining -= splitSize) &#123;</span><br><span class="line">splitHosts = <span class="keyword">this</span>.getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);</span><br><span class="line">splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, splitSize, splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>L) &#123;</span><br><span class="line">splitHosts = <span class="keyword">this</span>.getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, bytesRemaining, clusterMap);</span><br><span class="line">splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, bytesRemaining, splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>6 分区结果</strong></p><p>每块128M，最后一块略大，符合预期。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs://xxxx:0+134217728</span><br><span class="line">hdfs://xxxx:134217728+134217728</span><br><span class="line">hdfs://xxxx:268435456+134217728</span><br><span class="line">hdfs://xxxx:402653184+138476793</span><br></pre></td></tr></table></figure><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><ul><li><p>这里比较奇葩的是 minPartitions 这个设定，它最大只能是2。我觉得之所以这样设定，是防止文件切的过小。假设整个文件大小只有5M，公式：<code>Math.min(goalSize, blockSize)</code> blockSize假定128M，此时 splitSize 由 minPartitions 决定（不考虑人为设定的那个minSize）。那么它最多只能被切成2份。</p></li><li><p>当文件较大时（大于blockSize两倍），只和 blockSize 有关。尽管我的测试文件中每个 block 实际大小只有10M，然鹅这个并没有什么软用。</p></li><li><p>这是单文件情况，如果是读一个目录下的多文件，那就是单独对每个文件进行切分。（从源码可以发现，其中的 totalSize 是所有文件大小总和）。</p></li><li><p>当然这只是分区划分，实际读取数据没这么简单。假如我们是一条一条读，那么如果该分区最后一条数据没读完，它会接着向下一块继续读，参考<a href="https://hadoopi.wordpress.com/2013/05/27/understand-recordreader-inputsplit/" target="_blank" rel="noopener">它</a>。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用 textFile 读取HDFS的数据分区规则&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>CentOS根目录扩容</title>
    <link href="http://yoursite.com/2019/11/23/CentOS%E6%A0%B9%E7%9B%AE%E5%BD%95%E6%89%A9%E5%AE%B9/"/>
    <id>http://yoursite.com/2019/11/23/CentOS根目录扩容/</id>
    <published>2019-11-23T08:40:56.000Z</published>
    <updated>2019-11-23T09:13:42.301Z</updated>
    
    <content type="html"><![CDATA[<p>记录对 /root 目录的扩容</p><a id="more"></a><h1 id="问题：-root-的空间用满了"><a href="#问题：-root-的空间用满了" class="headerlink" title="问题：/root 的空间用满了"></a>问题：<code>/root</code> 的空间用满了</h1><p>本来打算直接动态扩容，也就是按鸟哥写的放大LV容量，把 <code>/home</code> 的空间分点给 <code>/root</code> 。结果发现 xfs 文件系统只支持动态增加，不能减少。因此咱只能备份重装了。</p><h1 id="解决："><a href="#解决：" class="headerlink" title="解决："></a>解决：</h1><h2 id="1-我的版本"><a href="#1-我的版本" class="headerlink" title="1 我的版本"></a>1 我的版本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/redhat-release</span><br><span class="line">CentOS Linux release 7.6.1810 (Core)</span><br></pre></td></tr></table></figure><h2 id="2-分区情况"><a href="#2-分区情况" class="headerlink" title="2 分区情况"></a>2 分区情况</h2><p>CentOS 的 <code>/root</code> 和 <code>/home</code> 目录使用了LVM（逻辑卷分区）</p><p>我准备给 <code>/root</code> 加100G，把 <code>/home</code> 改为700G，预留 50G </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ df -hl</span><br><span class="line">Filesystem           Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/cl-root   50G   46G  5.0G  91% /</span><br><span class="line">...</span><br><span class="line">/dev/mapper/cl-home  849G  220G  629G  26% /home</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们要调的就是这个LV Size</span></span><br><span class="line">$ lvdisplay</span><br><span class="line">  ...</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/home</span><br><span class="line">  LV Name                home</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                &lt;849.07 GiB</span><br><span class="line">  Current LE             217361</span><br><span class="line"></span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/root</span><br><span class="line">  LV Name                root</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                50.00 GiB</span><br><span class="line">  Current LE             12800</span><br></pre></td></tr></table></figure><h2 id="3-备份-home"><a href="#3-备份-home" class="headerlink" title="3 备份 /home"></a>3 备份 <code>/home</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tar 压缩，-p 保留权限，-z 使用 gzip</span></span><br><span class="line">$ tar cvpfz /mnt/data/homeback.tgz /home</span><br></pre></td></tr></table></figure><h2 id="4-删除-home"><a href="#4-删除-home" class="headerlink" title="4 删除 /home"></a>4 删除 <code>/home</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 干掉/home文件系统的进程</span></span><br><span class="line">$ fuser -km /home/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载/home，如果没用，加 -l 强制卸载</span></span><br><span class="line">$ umount /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 /home 的lv</span></span><br><span class="line">$ lvremove /dev/mapper/cl-home</span><br></pre></td></tr></table></figure><h2 id="5-扩容-root"><a href="#5-扩容-root" class="headerlink" title="5 扩容 /root"></a>5 扩容 <code>/root</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我这里加 100G</span></span><br><span class="line">$ lvextend -L +100G /dev/mapper/cl-root</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刷新 /root文件系统</span></span><br><span class="line">$ xfs_growfs /dev/mapper/cl-root</span><br></pre></td></tr></table></figure><h2 id="6-恢复-home"><a href="#6-恢复-home" class="headerlink" title="6 恢复 /home"></a>6 恢复 <code>/home</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分 700G 给它。（预留 50G 的 空闲空间）</span></span><br><span class="line">$ lvcreate -L 700G -n /dev/mapper/cl-home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建文件系统</span></span><br><span class="line">$ mkfs.xfs /dev/mapper/cl-home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂载 /home</span></span><br><span class="line">$ mount /dev/mapper/cl-home /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件恢复</span></span><br><span class="line">$ tar xvpfz /mnt/data/homeback.tgz -C /</span><br></pre></td></tr></table></figure><h2 id="7-检查结果"><a href="#7-检查结果" class="headerlink" title="7 检查结果"></a>7 检查结果</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">$ df -hl</span><br><span class="line">Filesystem           Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/cl-root  150G   46G  105G  31% /</span><br><span class="line">...</span><br><span class="line">/dev/mapper/cl-home  700G  220G  480G  32% /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查VG，还有 50G 剩余，稳稳的</span></span><br><span class="line">$ vgdisplay</span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               cl</span><br><span class="line">  Cur LV                3</span><br><span class="line">  ...</span><br><span class="line">  VG Size               &lt;930.51 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              238210</span><br><span class="line">  Alloc PE / Size       225648 / &lt;881.44 GiB</span><br><span class="line">  Free  PE / Size       12562 / 49.07 GiB</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 检查LV，和预想一样</span></span><br><span class="line">$ lvdisplay</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/root</span><br><span class="line">  LV Name                root</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                150.00 GiB</span><br><span class="line">  Current LE             38400</span><br><span class="line"></span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/home</span><br><span class="line">  LV Name                home</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                700.00 GiB</span><br><span class="line">  Current LE             179200</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录对 /root 目录的扩容&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="LVM" scheme="http://yoursite.com/tags/LVM/"/>
    
  </entry>
  
  <entry>
    <title>hbase-基本原理</title>
    <link href="http://yoursite.com/2019/09/29/hbase-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2019/09/29/hbase-基本原理/</id>
    <published>2019-09-29T09:20:52.000Z</published>
    <updated>2019-10-08T02:10:35.843Z</updated>
    
    <content type="html"><![CDATA[<p>简单介绍了hbase的基本原理：数据结构、读写操作、flush、合并与切分</p><a id="more"></a><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="逻辑结构"><a href="#逻辑结构" class="headerlink" title="逻辑结构"></a>逻辑结构</h2><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/luoji.png" alt></p><p>一个<strong>namespace</strong>可以有多个<strong>表</strong>，如<code>zxylearn:student</code>，代表<code>zxylearn</code>命名空间下的<code>student</code>表。</p><p>一个<strong>表</strong>可以有多个<strong>region</strong>，如上图有3个region。一个region一个文件夹。可以预分区（推荐）或者当一个region过大时会自动触发切分。</p><p>一个<strong>region</strong>可以有多个<strong>store</strong>（按列族划分），如上图有两个列族。一个store一个文件夹，文件夹名是列族名。</p><p>一个<strong>store</strong>可以有多个<strong>HFile</strong>，flush一次产生一个，可以合并（后面会讲）</p><p><strong>HFile</strong>中就是具体数据了，逻辑上是一行行序列化的数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># HDFS中的一个HFile的完整路径：</span><br><span class="line">/hbase/data/zxylearn/student/0fe474b3eb5fef1573d85acae8e5b787/info1/2dce02f62ea446f892a0b3cfc4e4db8a</span><br><span class="line"># namespace  zxylearn</span><br><span class="line"># 表名        student</span><br><span class="line"># region     0fe474b3eb5fef1573d85acae8e5b787</span><br><span class="line"># 列族名      info1</span><br><span class="line"># HFile      2dce02f62ea446f892a0b3cfc4e4db8a</span><br></pre></td></tr></table></figure><h2 id="物理结构"><a href="#物理结构" class="headerlink" title="物理结构"></a>物理结构</h2><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/wuli.png" alt></p><p>通过指定 <code>&#39;namespace：表名&#39;</code> 和 <code>&#39;Row Key&#39;</code>，可查找到数据的<code>列族</code>、<code>列名</code>、<code>timestamp</code>、<code>value</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; get &apos;zxylearn:student&apos;,&apos;1001&apos;</span><br><span class="line">COLUMN                  CELL</span><br><span class="line"> info1:name             timestamp=1569722805794, value=zhangsan</span><br></pre></td></tr></table></figure><h1 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h1><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/write.png" alt></p><ol><li>客户端向ZK请求返回 元数据表所在的RegionServer地址。（我们可以在ZK客户端中用<code>get /hbase/meta-region-server</code>看到它）</li><li>客户端接收地址，向它请求返回 待写数据所在的RegionServer地址。（我们可以在hbase客户端用<code>scan &#39;hbase:meta&#39;</code>看到它：<code>column=info:server, timestamp=1569658344714, value=localhost:16020</code>）</li><li>向该地址请求写数据。</li><li>先将写操作记录到操作日志（<strong>WAL</strong>）中，接着将数据写入<strong>内存</strong>中。</li></ol><h1 id="Flush"><a href="#Flush" class="headerlink" title="Flush"></a>Flush</h1><p>可以看出，写数据是将数据写入到内存中。当执行Flush刷写操作时，才会将数据写入磁盘，也就是HDFS中，形成一个HFile。</p><p><strong>Flush时机</strong></p><ul><li>大小超过限制：如RegionServer的全局内存大小，默认是堆大小的40%时刷写；单个region大小，默认128M。</li><li>时间：从最后一条数据写入后，1h（默认）没有新数据。</li></ul><p><strong>Flush细节</strong></p><ul><li><strong>flush会删除过期的数据</strong>。假设建表时数据版本数设置为1，那么写入磁盘的的数据最多只有一个版本；如果删除标记是DeleteColumn，那么会删除比它低版本的数据。(<strong>删除标记有多种</strong>，如 Delete：只删自己；DeleteColumn：删除自己与比自己低的)</li><li><strong>flush不会删除带删除标记的数据</strong>。原因：设想，我们原本想删除一条数据，给它打上删除标记。如果flush删除了，假如磁盘中的其它HFile中有该数据的旧版本，那么它们在<strong>合并操作</strong>（后面会讲）时就不会被删除了。</li></ul><p>反正记住这里会干掉不要的数据（版本数与删除标记决定），但<strong>不会干掉带删除标记的数据</strong>。</p><h1 id="读数据流程"><a href="#读数据流程" class="headerlink" title="读数据流程"></a>读数据流程</h1><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/read.png" alt></p><ol><li>找到数据所在RS服务器的流程和写数据基本一样。</li><li>向该地址请求读数据。</li><li>读取cache（缓存）和MemStore（内存），查找该数据。</li><li>如果cache中没有，再去StoreFile（磁盘）中找该数据。</li><li>取时间戳最新的数据返回，并记录到cache中。</li></ol><p><strong>为什么在内存中找到了，还要去磁盘中找？</strong></p><p><strong>时间戳的缘故</strong>：我们用户自然是查找最新的数据，内存中的数据的时间戳不能保证一定比磁盘中的新，所有要把它们都找到，然后比较返回最新的数据。因此，这导致了<strong>hbase读数据比写数据还慢</strong>。所以，<strong>用cache缓存查到的数据</strong>，可以一定程度提高读数据的速度。</p><h1 id="compact-合并-与split-切分"><a href="#compact-合并-与split-切分" class="headerlink" title="compact(合并)与split(切分)"></a>compact(合并)与split(切分)</h1><h2 id="compact-合并"><a href="#compact-合并" class="headerlink" title="compact(合并)"></a>compact(合并)</h2><p>合并是<strong>将若干个小的HFile，合并成一个大的HFile</strong>。分<strong>Minor Compaction</strong> 和 <strong>Major Compaction</strong>。</p><ul><li><p><strong>Minor Compaction</strong> <strong>不会</strong>清理不要的数据和带删除标记的数据</p></li><li><p><strong>Major Compaction</strong> <strong>会</strong>清理不要的数据和带标记删除的数据。当HFile数大于等于3时，执行<code>compact</code>时执行的是它。</p></li></ul><p><strong>清理细节</strong></p><p>与flush不同，这里的清理不仅会清除不要的数据，还会<strong>清理带删除标记的数据</strong>（干掉它，和比它旧的数据）。</p><h2 id="split-切分"><a href="#split-切分" class="headerlink" title="split(切分)"></a>split(切分)</h2><p><strong>切分是将大的region切分成若干个小的region</strong>。可以预分区（推荐）或者当一个region过大时会自动触发切分。</p><p><strong>为什么推荐只用一个列族呢？</strong></p><p><strong>多个列族可能导致小文件过多。</strong>假设一个列族的数据的很密集，另一列族很稀疏，那么在触发<strong>flush或者split</strong>时，密集的列族形成的HFile文件足够大没问题，但是稀疏的生成的就是小文件了，久而久之会形成过多小文件使效率降低。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单介绍了hbase的基本原理：数据结构、读写操作、flush、合并与切分&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>hbase-安装与配置</title>
    <link href="http://yoursite.com/2019/09/28/hbase-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2019/09/28/hbase-安装与配置/</id>
    <published>2019-09-28T08:20:25.000Z</published>
    <updated>2019-09-29T09:23:27.889Z</updated>
    
    <content type="html"><![CDATA[<p>介绍hbase的安装与简单使用</p><a id="more"></a><h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><p>我的版本：</p><p><strong>hadoop 2.7.7</strong></p><p><strong>zookeeper 3.5.5</strong></p><p><strong>hbase 1.3.5</strong></p><p>先装好 hadoop 和 zookeeper</p><h1 id="配置-HBase"><a href="#配置-HBase" class="headerlink" title="配置 HBase"></a>配置 HBase</h1><h2 id="修改-hbase-env-sh"><a href="#修改-hbase-env-sh" class="headerlink" title="修改 hbase-env.sh"></a>修改 hbase-env.sh</h2><p>修改<code>hbase-1.3.5/conf</code>下的<code>hbase-env.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=$(/usr/libexec/java_home)</span><br><span class="line"><span class="comment"># 使用自己安装的ZK</span></span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure><h2 id="修改-hbase-site-xml"><a href="#修改-hbase-site-xml" class="headerlink" title="修改 hbase-site.xml"></a>修改 hbase-site.xml</h2><p>修改<code>hbase-1.3.5/conf</code>下的<code>hbase-site.xml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.master&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:60000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;localhost:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/<span class="built_in">local</span>/apache-zookeeper-3.5.5-bin/zkData&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="添加-core-site-xml-和-hdfs-site-xml"><a href="#添加-core-site-xml-和-hdfs-site-xml" class="headerlink" title="添加 core-site.xml 和 hdfs-site.xml"></a>添加 core-site.xml 和 hdfs-site.xml</h2><p>直接在<code>hbase-1.3.5/conf</code>下创建<strong>软连接</strong>即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/<span class="built_in">local</span>/hadoop-2.7.7/etc/hadoop/core-site.xml /usr/<span class="built_in">local</span>/hbase-1.3.5/conf/core-site.xml</span><br><span class="line"></span><br><span class="line">ln -s /usr/<span class="built_in">local</span>/hadoop-2.7.7/etc/hadoop/hdfs-site.xml /usr/<span class="built_in">local</span>/hbase-1.3.5/conf/hdfs-site.xml</span><br></pre></td></tr></table></figure><h2 id="修改-regionservers"><a href="#修改-regionservers" class="headerlink" title="修改 regionservers"></a>修改 regionservers</h2><p>在<code>/hbase-1.3.5/conf/regionservers</code> 中添加<strong>集群节点的名字</strong>，用于群启与群关</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先开HDFS和ZK</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 单节点启动与关闭</span></span><br><span class="line">bin/hbase-daemon.sh start master</span><br><span class="line">bin/hbase-daemon.sh start regionserver</span><br><span class="line">bin/hbase-daemon.sh stop master</span><br><span class="line">bin/hbase-daemon.sh stop regionserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># 群启与群关</span></span><br><span class="line">bin/start-hbase.sh</span><br><span class="line">bin/stop-hbase.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化网页</span></span><br><span class="line">http://localhost:16010</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正常启动后的JPS</span></span><br><span class="line">977 ResourceManager</span><br><span class="line">865 NameNode</span><br><span class="line">4289 Jps</span><br><span class="line">4146 HRegionServer</span><br><span class="line">915 DataNode</span><br><span class="line">1028 NodeManager</span><br><span class="line">2360 QuorumPeerMain</span><br><span class="line">4062 HMaster</span><br></pre></td></tr></table></figure><h1 id="简单shell操作"><a href="#简单shell操作" class="headerlink" title="简单shell操作"></a>简单shell操作</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入shell</span></span><br><span class="line">bin/hbase shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># 命名空间的相关操作</span></span><br><span class="line">create_namespace <span class="string">'zxylearn'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 表的相关操作</span></span><br><span class="line"><span class="comment"># 建表</span></span><br><span class="line">create <span class="string">'zxylearn:student'</span>,<span class="string">'info1'</span>,<span class="string">'info2'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 看表</span></span><br><span class="line">describe <span class="string">'zxylearn:student'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删表</span></span><br><span class="line"><span class="built_in">disable</span> <span class="string">'zxylearn:student'</span></span><br><span class="line">drop <span class="string">'zxylearn:student'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据的相关操作</span></span><br><span class="line"><span class="comment"># 增</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:name'</span>,<span class="string">'zhangsan'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:sex'</span>,<span class="string">'man'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1002'</span>,<span class="string">'info1:age'</span>,<span class="string">'22'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1002'</span>,<span class="string">'info2:addr'</span>,<span class="string">'wuhan'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1003'</span>,<span class="string">'info2:addr'</span>,<span class="string">'beijing'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查</span></span><br><span class="line">scan <span class="string">'zxylearn:student'</span>,&#123;STARTROW=&gt;<span class="string">'1001'</span>,STOPROW=&gt;<span class="string">'1003'</span>&#125;</span><br><span class="line">get <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span></span><br><span class="line">get <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:name'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 改</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:name'</span>,<span class="string">'zxy'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删</span></span><br><span class="line">delete <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:sex'</span></span><br><span class="line">deleteall <span class="string">'zxylearn:student'</span>, <span class="string">'1002'</span></span><br><span class="line">truncate <span class="string">'zxylearn:student'</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍hbase的安装与简单使用&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper-基本原理</title>
    <link href="http://yoursite.com/2019/09/24/zookeeper-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2019/09/24/zookeeper-基本原理/</id>
    <published>2019-09-24T08:24:14.000Z</published>
    <updated>2019-09-24T08:36:42.582Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了zookeeper的基本原理和使用</p><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>ZooKeeper是个分布式的服务协调框架。具体用途如：<strong>统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理</strong>等。它基于<strong>观察者</strong>的设计模式；zookeeper = 文件系统 + 监听通知机制。</p><p>特点：</p><ol><li>最终一致性：Client不论连接到哪个Server，得到的数据都是一样的。</li><li>原子性：事务中的所有操作全部执行或者全部不执行。</li><li>顺序性：同一个client的请求顺序执行。</li><li>分区容错性：zookeeper是分布式的，所以在部分节点出现故障时，可以自己恢复。</li></ol><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p><strong>znode</strong>：zk是层级树状结构，一个节点就是znode。默认可存<strong>1M数据</strong>。</p><ul><li>操作节点时可设置<strong>watcher</strong>。当节点状态发生变化时，就会触发watcher对应的操作，只触发一次。</li><li><strong>永久节点</strong>：创建后永久存在，除非主动删除；<strong>临时节点</strong>：临时创建的，会话结束节点自动被删除</li><li><strong>顺序节点</strong>：节点名称后面自动增加一个10位数字的序列号；<strong>非顺序节点</strong>：不加序列号</li></ul><p>节点衍生出分类是为了迎合需求。临时节点可以记录服务器是否上线：当服务器下线，临时节点的数据消除。顺序节点可用于同一服务器多次上下线，每次名字都不同。很明显2个2分类，两两组合有4种节点。</p><p>下面是一个znode的数据结构：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个znode的数据结构</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">stat</span> /zxy</span><br><span class="line">cZxid = 0x39                                <span class="comment"># 节点创建时的zxid</span></span><br><span class="line">ctime = Mon Sep 23 16:38:23 CST 2019        <span class="comment"># 节点创建时的时间戳</span></span><br><span class="line">mZxid = 0x3d                                <span class="comment"># 节点修改时的zxid</span></span><br><span class="line">mtime = Mon Sep 23 16:38:44 CST 2019        <span class="comment"># 节点修改时的时间戳</span></span><br><span class="line">pZxid = 0x39                                <span class="comment"># 改变子节点时的zxid</span></span><br><span class="line">cversion = 0                                <span class="comment"># 子节点的版本号</span></span><br><span class="line">dataVersion = 3                             <span class="comment"># 数据版本：初始0，改变一次加1</span></span><br><span class="line">aclVersion = 0                              <span class="comment"># ACL版本（权限）</span></span><br><span class="line">ephemeralOwner = 0x0                        <span class="comment"># 数据拥有者：永久节点是0；临时节点是创建者的id</span></span><br><span class="line">dataLength = 13                             <span class="comment"># 数据长度</span></span><br><span class="line">numChildren = 0                             <span class="comment"># 子节点个数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对节点的操作无非是增删改查等，这里就不写了</span></span><br></pre></td></tr></table></figure><p>补充：</p><p><strong>zxid</strong>（ZooKeeper Transaction Id）：ZooKeeper每次状态变化将会产生一个叫zxid的时间戳。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>基本流程： </p><p>客户端发请求（可带watcher） -&gt;  zk 选举与恢复 (没leader时) -&gt; zk 读写数据 -&gt; 返回数据（可触发watcher）</p><p>两端主要是<strong>监听器的原理</strong>，中间主要用到<strong>ZAB协议</strong>。</p><h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><p>watcher 相当于一个的炸弹，客户端发请求时：如<code>ls，get，set</code>等，可以给节点绑上炸弹。</p><p>如果触发了爆炸条件：<code>ls</code>就是该节点有增加删减子节点；<code>get set</code>就是该节点数据改变。</p><p>炸弹爆炸（只炸一次）也就是执行里面的回调函数。</p><p>很明显这里用户端至少需要启动两个线程：connect线程负责网络连接通信：绑炸弹并把任务发给zk与后续数据互传；listener线程监听炸弹爆炸信号。</p><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/zookeeper/zk.png" alt></p><h2 id="ZAB协议"><a href="#ZAB协议" class="headerlink" title="ZAB协议"></a>ZAB协议</h2><p>Zab协议（Zookeeper Atomic Broadcast），通过它来保证分布式事务的最终一致性。<br>这个内容很多，详细的可以参考：</p><p><a href="https://www.jianshu.com/p/2bceacd60b8a" target="_blank" rel="noopener">Zookeeper——一致性协议:Zab协议</a></p><p><a href="https://www.cnblogs.com/felixzh/p/5869212.html" target="_blank" rel="noopener">Zookeeper的功能以及工作原理</a></p><p>主要功能：崩溃恢复(选举、数据恢复) 的 原子广播 (数据读写)</p><h3 id="崩溃恢复"><a href="#崩溃恢复" class="headerlink" title="崩溃恢复"></a>崩溃恢复</h3><p>集群中必须有一个leader，leader出现故障时，采用投票选举新leader，它需要满足以下条件：</p><ul><li>新选举出来的 Leader 不能包含未提交的 Proposal 。</li><li>新选举的 Leader 节点中含有最大的 zxid 。</li><li>得到超过一半选票者称为 Leader，因此<strong>zk集群个数为奇数</strong></li></ul><p>并不是所有节点都是 leader 和 follower ，还有observer，它不参与选举。作用是：可以增加集群数量，又减少投票选举时间。</p><p>选出leader后，进行数据恢复也就是同步，这个没啥，就是让它们其它节点数据都和leader同步，毕竟咱要确保<strong>最终一致性</strong>。恢复完毕后，就可以处理客户端的请求了。</p><h3 id="读写数据"><a href="#读写数据" class="headerlink" title="读写数据"></a>读写数据</h3><p><strong>一般流程：</strong></p><p><strong>读请求</strong>，就是直接从当前节点中读取数据</p><p><strong>写请求</strong></p><ol><li>客户端发起一个写操作请求。</li><li>Leader 将客户端的请求转化为事务（Proposal），每个 Proposal 分配一个全局的ID，即zxid。</li><li>Leader 为每个 Follower 分配一个单独的队列，然后将需要广播的 Proposal 依次放到队列中取，并且根据 FIFO 策略进行消息发送。</li><li>Follower 接收到 Proposal 后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 Ack 响应消息。</li><li>Leader 接收到<strong>超过半数以上</strong> Follower 的 Ack 响应消息后，即认为消息发送成功，可以发送 commit 消息。</li><li>Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交。Follower 接收到 commit 消息后，会将上一条事务提交。</li></ol><p><strong>有意思的点</strong></p><ul><li><strong>如何保证消息有序：</strong>在整个消息广播中，Leader会将每一个事务请求转换成对应的 proposal 来进行广播，并且在广播 事务Proposal 之前，Leader服务器会首先为这个事务Proposal分配一个全局单递增的唯一ID，称之为事务ID（即zxid），由于Zab协议需要保证每一个消息的严格的顺序关系，因此必须将每一个proposal按照其zxid的先后顺序进行排序和处理。</li></ul><ul><li><p><strong>用队列提高效率：</strong>Leader 服务器与每一个 Follower 服务器之间都维护了一个单独的 FIFO 消息队列进行收发消息，使用队列消息可以做到异步解耦。 Leader 和 Follower 之间只需要往队列中发消息即可。如果使用完全同步的方式会引起阻塞，性能要下降很多。(我感觉这里应该不是FIFO 消息队列，应该是最小队列吧)</p></li><li><p><strong>记住超过半数</strong> </p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍了zookeeper的基本原理和使用&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="zookeeper" scheme="http://yoursite.com/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>hive-DML</title>
    <link href="http://yoursite.com/2019/09/19/hive-DML/"/>
    <id>http://yoursite.com/2019/09/19/hive-DML/</id>
    <published>2019-09-19T09:02:24.000Z</published>
    <updated>2019-09-21T03:54:42.372Z</updated>
    
    <content type="html"><![CDATA[<p>hive中一些DML的操作</p><a id="more"></a><p>DML（数据操纵语言）主要指数据的增删查改</p><h1 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h1><p>有5种导入数据的方法，最常用的是 <strong>Load</strong> 和 <strong>Insert</strong></p><h2 id="Load"><a href="#Load" class="headerlink" title="Load"></a>Load</h2><p>从文件系统中导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> [<span class="keyword">local</span>] inpath <span class="string">'/xxxxxx'</span> </span><br><span class="line">[overwrite] <span class="keyword">into</span> <span class="keyword">table</span> tablename [<span class="keyword">partition</span> (partcol1=val1,…)];</span><br></pre></td></tr></table></figure><ul><li>local: 指本地文件系统，否则为HDFS</li><li>overwrite: 指覆盖表中已有数据，否则表示追加</li><li>partition: 表示上传到指定分区</li></ul><h2 id="Insert"><a href="#Insert" class="headerlink" title="Insert"></a>Insert</h2><p>通过查询语句导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 覆盖</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tablename [<span class="keyword">partition</span>(partcol1=val1,partclo2=val2)] select_statement;</span><br><span class="line"><span class="comment"># 追加</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> tablename [<span class="keyword">partition</span>(partcol1=val1,partclo2=val2)] select_statement;</span><br></pre></td></tr></table></figure><h2 id="As-Select"><a href="#As-Select" class="headerlink" title="As Select"></a>As Select</h2><p>查询语句中创建表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> tablename</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><h2 id="Location"><a href="#Location" class="headerlink" title="Location"></a>Location</h2><p>创建表时通过Location指定数据的路径，再直接put数据到hdfs上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /xxxxx /user/hive/warehouse/tablename;</span><br></pre></td></tr></table></figure><h2 id="Import"><a href="#Import" class="headerlink" title="Import"></a>Import</h2><p>只能导入export导出的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import table tablename partition(month='201909') from '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><h1 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h1><p>最常用的是 <strong>Insert</strong> 和 <strong>Hadoop</strong></p><h2 id="Insert-1"><a href="#Insert-1" class="headerlink" title="Insert"></a>Insert</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据导入本地（并格式化处理），不加local就是导入HDFS</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/xxxxx'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tablename;</span><br></pre></td></tr></table></figure><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hadoop命令导出</span></span><br><span class="line">hive &gt; dfs -get /user/hive/warehouse/student/month=201909/000000_0 /xxxxxxx;</span><br></pre></td></tr></table></figure><h2 id="Export"><a href="#Export" class="headerlink" title="Export"></a>Export</h2><p>这个导出的数据除了数据还有元数据，可用Import导入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive &gt; <span class="built_in">export</span> table tablename to <span class="string">'/user/hive/warehouse/export/student'</span>;</span><br></pre></td></tr></table></figure><h1 id="数据清除"><a href="#数据清除" class="headerlink" title="数据清除"></a>数据清除</h1><p>只能清除内部表的数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive &gt; truncate table tablename;</span><br></pre></td></tr></table></figure><h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><p>查询的关键字较多，要知道它们的<strong>顺序</strong>（重点）</p><p>写的顺序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> ... <span class="keyword">join</span> <span class="keyword">on</span> ... <span class="keyword">where</span> ... <span class="keyword">group</span> <span class="keyword">by</span> ... <span class="keyword">having</span> ... <span class="keyword">order</span> <span class="keyword">by</span> ... <span class="keyword">limit</span> ...</span><br></pre></td></tr></table></figure><p>执行顺序：大体思路是 限定(where)，分组，限定（having），选择，排序</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from -&gt; join on -&gt; where -&gt; group by -&gt; having -&gt; select -&gt; order by -&gt; limit</span><br></pre></td></tr></table></figure><h2 id="select…where…limit"><a href="#select…where…limit" class="headerlink" title="select…where…limit"></a>select…where…limit</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单查询</span></span><br><span class="line"><span class="keyword">select</span> empno, ename <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) cnt <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal &gt;<span class="number">1000</span> <span class="keyword">limit</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure><h2 id="group-by-和-having"><a href="#group-by-和-having" class="headerlink" title="group by 和 having"></a>group by 和 having</h2><p>group by 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算emp表每个部门的平均工资</span></span><br><span class="line"><span class="keyword">select</span> t.deptno, <span class="keyword">avg</span>(t.sal) avg_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br></pre></td></tr></table></figure><p>where 作用在 分组（group by）和聚合（sum等）计算之前，选取哪些行，也就是在查询前筛选；having 对分组后<strong>计算的</strong>数据进行过滤。它<strong>只用于</strong>group by分组统计语句。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求每个部门的平均薪水大于2000的部门</span></span><br><span class="line"><span class="keyword">select</span> deptno, <span class="keyword">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal &gt; <span class="number">2000</span>;</span><br></pre></td></tr></table></figure><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>Hive支持通常的SQL JOIN语句，但是<strong>只支持等值连接，不支持非等值连接</strong>。 且 连接谓词中<strong>不支持or</strong>。<br>这个非等值连接可以从以前学的<strong>reducejoin</strong>的流程思索原因，reducejoin是在shuffer时将数据按关联值相等的（on的条件）分为一组，再在reducer阶段进行处理。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 合并员工表和部门表</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="全局排序-Order-By"><a href="#全局排序-Order-By" class="headerlink" title="全局排序 Order By"></a>全局排序 Order By</h3><p>全局排序，只一个Reducer。全排很明显最后生成一个总的排序文件，<strong>1个reducer</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询员工信息按工资降序排列</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="按reducer排序-sort-by"><a href="#按reducer排序-sort-by" class="headerlink" title="按reducer排序 sort by"></a>按reducer排序 sort by</h3><p>每个reducer端都会做排序，出来的数据是有序的。假如有n个Reducer，就会生成n个有序文件。当n=1时，它就是<code>Order By</code>。</p><p>扩展一波，Reducer个数默认按原始数据256M一个，当然也可手动设置其个数。</p><h3 id="分区排序-Distribute-By…Sort-By"><a href="#分区排序-Distribute-By…Sort-By" class="headerlink" title="分区排序 Distribute By…Sort By"></a>分区排序 Distribute By…Sort By</h3><p>先分区，后排序。这个分区类型mapreduce的分区，多少个分区，就有多少个reduce任务，后面就生成多少个文件。说白了这个和上面的区别就是它通过<code>Distribute By</code>指定怎么分区，即指定怎么分reducer。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置reduce个数</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br><span class="line"><span class="comment"># 先按照部门编号分区，再按照员工编号降序排序</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/Users/zxy/IdeaProjects/bigdata-learning/hive-learning/data/output'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="分桶-Cluster-By"><a href="#分桶-Cluster-By" class="headerlink" title="分桶 Cluster By"></a>分桶 Cluster By</h3><p>当distribute by 和 sorts by <strong>字段相同</strong>时，可以使用cluster by方式。但<strong>排序只能是升序排序</strong>。</p><p>可以从取名看出，我没用分桶排序。你可以理解 Cluster 就是把数据分区，然后每个分区生成一个文件，这样就好解释为啥只能升序排序，我理解它压根就不需要排序，只是把数据分到不同区就ok。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下两种写法等价</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><p><strong>细节一：来波小结理一下  分区表 分区排序 分桶(前两个分区意思截然不同)</strong></p><ul><li><code>partition(month=&#39;201909&#39;)</code> 这个是分区表，针对的是数据的存储路径</li><li><code>Distribute By...Sort By</code> 这个是分区排序，和<strong>MR中的分区</strong>概念一样，多少个分区，就有多少个reduce任务，后面就生成多少个文件，分区之后对区里的数据进行排序。</li><li><code>Cluster By</code> 分桶，针对的是数据文件，将大的数据集分区。</li></ul><p>所以将 Cluster By 理解为分区， Distribute By…Sort By 理解为分区排序，岂不美哉</p><p><strong>细节二：注意导入数据到分桶中，要用insert，且 设置<code>hive.enforce.bucketing=true</code>和<code>hive.enforce.bucketing=true</code></strong></p><p><strong>细节三：分桶抽样查询</strong></p><p><code>select * from tablename tablesample(bucket x out of y on id);</code></p><p>x 表示从哪个bucket开始抽取</p><p>y 表示抽样间隔，共抽取 总数/y 个桶，且x的值必须<strong>小于等于</strong>y的值</p><p>举例：如果 x = 1, y = 4 ，共16个桶，那么将抽取16/4个桶，分别是 1、5、9、13</p><h2 id="行转列、列转行"><a href="#行转列、列转行" class="headerlink" title="行转列、列转行"></a>行转列、列转行</h2><p><strong>行转列</strong>：将不同行的聚合到一起</p><p><code>collect_set(col)</code>：函数<strong>只接受基本数据类型</strong>，它的主要作用是将某字段的值进行<strong>去重汇总</strong>(不去重用list)，产生array类型字段</p><p>例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    base,</span><br><span class="line">    <span class="keyword">concat_ws</span>(<span class="string">"|"</span>, collect_set(<span class="keyword">name</span>)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) base</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    constellation) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    base;</span><br></pre></td></tr></table></figure><p><strong>列转行</strong>：将列拆分成多行。</p><p><code>explode(col)</code> 将hive一列中复杂的array或者map结构拆分成多行</p><p><code>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</code> 用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p><p>例子： </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure><h2 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h2><p>基本结构：函数 over(范围)  。用前面的函数处理over中的规定的数据</p><p>除了count、sum等一些常用函数，还有<strong>只能配合over使用</strong>的函数：</p><ul><li><code>lag(col,n)</code>：往前第n行数据</li><li><code>lead(col,n)</code>：往后第n行数据</li><li><code>ntile(n)</code>：给数据编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。</li><li><code>rank()</code> 排序相同时会重复，总数不会变，如12225668</li><li><code>dense_rank()</code> 排序相同时会重复，总数会减少，如12223445</li><li><code>row_rank()</code> 单纯顺序计算，如12345678</li></ul><p>over里面可以规定窗口范围：</p><ul><li><code>()</code>：全部数据</li><li><code>(partition by xxx order by xxx)</code>：分区有序</li><li><code>(rows between xxxx and xxxx)</code>：手动指定范围<ul><li><code>current row</code>：当前行</li><li><code>n preceding</code>：往前n行数据</li><li><code>n following</code>：往后n行数据</li><li><code>unbounded preceding</code>： 从起点开始</li><li><code>unbounded following</code>： 到终点结束</li></ul></li></ul><p>一些例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">business.namebusiness.orderdatebusiness.cost</span><br><span class="line">jack2017-01-0110</span><br><span class="line">tony2017-01-0215</span><br><span class="line">jack2017-02-0323</span><br><span class="line">tony2017-01-0429</span><br><span class="line">jack2017-01-0546</span><br><span class="line">jack2017-04-0642</span><br><span class="line">tony2017-01-0750</span><br><span class="line">jack2017-01-0855</span><br><span class="line">mart2017-04-0862</span><br><span class="line">mart2017-04-0968</span><br><span class="line">neil2017-05-1012</span><br><span class="line">mart2017-04-1175</span><br><span class="line">neil2017-06-1280</span><br><span class="line">mart2017-04-1394</span><br><span class="line"></span><br><span class="line"><span class="comment">#（1）查询在2017年4月份购买过的顾客及总人数，over()针对groupby后的全部数据</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    <span class="keyword">count</span>(*) <span class="keyword">over</span> () </span><br><span class="line"><span class="keyword">from</span> business </span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">    <span class="keyword">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) = <span class="string">'2017-04'</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">    <span class="keyword">name</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（2）查询顾客的购买明细 并 让cost按照日期进行累加</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>,</span><br><span class="line">    <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) </span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    business;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（3）查看顾客上次的购买时间，在窗口中分区排序</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>, </span><br><span class="line">    lag(orderdate, <span class="number">1</span>, <span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate)</span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（4）查询前20%时间的订单信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加分组号</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>, </span><br><span class="line">    ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line"><span class="keyword">from</span> business; t1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过滤出组号为1的数据</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>, </span><br><span class="line">    ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line"><span class="keyword">from</span> business) t1</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">    sorted = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（5）计算每个人消费的排名</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>,</span><br><span class="line">    <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">cost</span> <span class="keyword">desc</span>)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    business;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;hive中一些DML的操作&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>hive-DDL</title>
    <link href="http://yoursite.com/2019/09/17/hive-DDL/"/>
    <id>http://yoursite.com/2019/09/17/hive-DDL/</id>
    <published>2019-09-17T12:29:08.000Z</published>
    <updated>2019-09-21T03:54:54.275Z</updated>
    
    <content type="html"><![CDATA[<p>hive中一些DDL的操作</p><a id="more"></a><h1 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h1><p>DDL（数据定义语言）用来处理数据库中的各种对象，如数据库、表等</p><h2 id="数据库-database"><a href="#数据库-database" class="headerlink" title="数据库(database)"></a>数据库(database)</h2><h3 id="增"><a href="#增" class="headerlink" title="增"></a>增</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个数据库，它在HDFS上的默认存储路径是/user/hive/warehouse/db_hive.db</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> db_hive;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 并 指定数据库在HDFS上存放的位置</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db_hive2 location <span class="string">'/db_hive2.db'</span>;</span><br></pre></td></tr></table></figure><h3 id="查"><a href="#查" class="headerlink" title="查"></a>查</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示数据库</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示数据库详细信息</span></span><br><span class="line">desc database extended db_hive;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换数据库</span></span><br><span class="line"><span class="keyword">use</span> db_hive;</span><br></pre></td></tr></table></figure><h3 id="改"><a href="#改" class="headerlink" title="改"></a>改</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改数据库</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> db_hive <span class="keyword">set</span> dbproperties(<span class="string">'createtime'</span>=<span class="string">'20190917'</span>);</span><br></pre></td></tr></table></figure><p>注意：数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。这个修改只是修改<code>DBPROPERTIES</code>里的键值对。</p><h3 id="删"><a href="#删" class="headerlink" title="删"></a>删</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> db_hive;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 强制删除非空数据库</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> db_hive <span class="keyword">cascade</span>;</span><br></pre></td></tr></table></figure><h2 id="表-table"><a href="#表-table" class="headerlink" title="表(table)"></a>表(table)</h2><h3 id="增-1"><a href="#增-1" class="headerlink" title="增"></a>增</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] <span class="comment">-- 注释</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] <span class="comment">-- 分区表</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="comment">-- 分桶表</span></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] <span class="comment">-- 不常用</span></span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] <span class="comment">-- 定义每行的格式</span></span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] <span class="comment">-- 指定存储文件类型</span></span><br><span class="line">[LOCATION hdfs_path] <span class="comment">-- 指定表在HDFS上的存储位置</span></span><br></pre></td></tr></table></figure><p><strong>一些细节：</strong></p><ul><li><strong>内部表与外部表</strong></li></ul><p><code>CREATE EXTERNAL TABLE</code> 用于创建外部表，默认是内部表（管理表）。它们的区别是 删除外部表并不会删除HDFS中的的数据，只会删除mysql中的元数据；而删除内部表都会删除。</p><p>可以这样理解外部表：HDFS上的数据是公有的，某个客户端建一了个hive表关联使用它，生成元数据，当该客户不用时，只删除他的元数据和hive表就行，公有数据仍然存在。</p><ul><li><strong>分区表</strong></li></ul><p>每个分区 对应一个HDFS文件系统上的独立的文件夹，该文件夹里包含该分区所有的数据。在查询时，可通过 WHERE 指定查询所需要的分区，查询效率会提高很多。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建表：</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test1(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据到指定分区：</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/xxxx'</span> <span class="keyword">into</span> <span class="keyword">table</span> test1 <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201909'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时，会数据会保存在 /user/hive/warehouse/test1/month=201909 中</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询指定分区的数据（可以直接将分区作为字段用）</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> test1 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201909'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二级分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test2(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li><strong>元数据和真实数据</strong></li></ul><p>元数据存在mysql中，包含数据库信息：ID、描述、HDFS路径、数据库名、所有者；分区信息；字段信息等等。而真实数据都存在HDFS中。</p><p><strong>它们可以自由独立存在</strong>，如外部表可以直接删除元数据。因此，创建数据时，如果直接放入分区中，由于元数据中没有分区信息，无法用where查到它，虽然数据存在。可以通过补充分区信息 或者 执行修复命令，让分区表和数据产生关联。</p><p>我的理解：先有数据，后有hive。hive要做的事就是关联到数据（生成元数据），然后CRUD它。</p><h3 id="改-1"><a href="#改-1" class="headerlink" title="改"></a>改</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重命名</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">rename</span> <span class="keyword">to</span> new_table_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test1 <span class="keyword">add</span> <span class="keyword">columns</span>(newdesc <span class="keyword">string</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test1 <span class="keyword">change</span> <span class="keyword">column</span> <span class="keyword">id</span> <span class="keyword">desc</span> <span class="built_in">int</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test1 <span class="keyword">replace</span> <span class="keyword">columns</span>(<span class="keyword">name</span> <span class="keyword">string</span>, loc <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure><h3 id="查-1"><a href="#查-1" class="headerlink" title="查"></a>查</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示表</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示表信息</span></span><br><span class="line">desc tablename;</span><br></pre></td></tr></table></figure><h3 id="删-1"><a href="#删-1" class="headerlink" title="删"></a>删</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> test1;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;hive中一些DDL的操作&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>随笔-中秋入学有感</title>
    <link href="http://yoursite.com/2019/09/16/%E9%9A%8F%E7%AC%94-20190916%E5%85%A5%E5%AD%A6/"/>
    <id>http://yoursite.com/2019/09/16/随笔-20190916入学/</id>
    <published>2019-09-16T13:27:55.000Z</published>
    <updated>2019-09-17T09:23:41.422Z</updated>
    
    <content type="html"><![CDATA[<p>很早想写，却迟迟未动笔。今有兴留以纪念。</p><a id="more"></a><p><strong>终于正式成为了研究生大军的一员～</strong></p><p>去年是中秋节结束进实验室的，今天亦是农历八月十六，按农历算刚好一年了呀。</p><p>这段时间，学了很多，杂七杂八的。学过人工智能，试过后台开发，如今又转战大数据。hh，可以说就差前端了（没系统学，但写后台多少会点）。。语言方面有：c++ ，Java，python ，scala。。可是，我觉得都只是会点皮毛而已。</p><p><strong>但是，我并不觉得是白学了。</strong></p><p>就拿现在大热的深度学习，我也学了可能大半年（算上大三下）。只有体验过，才知道深度学习没有那么玄乎；只有体验过，将来才可能不会说当初就应该学大热的深度学习了。毕竟，墙外的想进去，墙里面的想出来嘛。还记得，当初第一次看吴恩达的视频时，那感觉真像，发现了新大陆一样。可是渐渐也觉得有点厌倦。那时，我也喜欢在 leedcode 刷点算法题，对比我发现，比起调参，我可能更喜欢编程。比起不确定的准确率，一行行代码 让我更有成就感些～</p><p>后台开发 的学习让我对网站的整个流程有了认识。从有数据库的设计，到和前端的交互设计后台接口，再到网站部署，也算整了一个能用的东西出来。由于，后台更多是数据交互，它和大数据也是有很多联系之处的，而且主要都是基于Java的。hh，Java是很强大嘛。</p><p>一年的时光，我发现很多技术都是融合在一起的，后台和大数据都和数据打交道，集群的使用可以让人工智能的计算力更上一层楼。大数据更多的是分布式是思想，后台更专攻具体业务需求。很多后台人员可以过度到大数据，证明联系之深。同时人工智能的分布式计算环境的搭建，以后未尝不是一个热门需求呢。</p><p><strong>时光如白驹过隙。学的广，不如专精一门。</strong></p><p>如果说之前的学习，让我在几个大热领域都体验了一番，那么接下来要做的就是，在自己最喜欢的一个领域，专研下去。</p><p><strong>先定些小目标</strong></p><p>在第一个学期的前半个学期，把大数据的实用热门框架都过一遍，有个系统性的认识。后半个学期，看看是进阶下java，还是钻研下spark。在第二个学期，巩固大数据的几个最重要的框架，多研究java以及数据库。语言还是要学的精点好，学长说的好，就算你啥都不会，java玩的出神入化，照样抢着要。至于研二，到时再说吧～</p><p><strong>结尾了，打波鸡血</strong></p><p>《阿甘正传》里有个让我感触很深的情节。阿甘不知道该干什么，开始了一段跑步生涯。在这三年里，他做的仅仅只是不停的往前跑呀跑，跑呀跑。但是他的追随者从一个变两个，两个变三个，后面越来越多，越来越多，报道者也越来越多。你可能会说，这就是演电影吧？不！我觉得就算发生在现实，我相信结果同样会如此！</p><p><strong>可以说最容易的事是坚持，最难的也是坚持。</strong>不管你学的是什么，只要你肯一直坚持下去，即使只是简单的每天跑，你也绝对会成功的！</p><p><img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1568649098110&di=9c138a4f9097ca8373cfeba8edf1bdab&imgtype=0&src=http%3A%2F%2Fpic.rmb.bdstatic.com%2Fe61012997dcaf9d2ca611c60bfa03db7.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很早想写，却迟迟未动笔。今有兴留以纪念。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://yoursite.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>hive-安装与配置</title>
    <link href="http://yoursite.com/2019/09/16/hive-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2019/09/16/hive-安装与配置/</id>
    <published>2019-09-16T12:00:24.000Z</published>
    <updated>2019-09-29T09:23:12.914Z</updated>
    
    <content type="html"><![CDATA[<p>介绍hive的安装，使用 mysql 做 hive 的元数据库</p><a id="more"></a><h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><p>我的版本：</p><p><strong>hadoop 2.7.7</strong></p><p><strong>hive 2.3.6</strong></p><p><strong>mysql 8.0.16</strong></p><p>先装好mysql（做hive的元数据库） 和 hadoop</p><h1 id="配置-Hive"><a href="#配置-Hive" class="headerlink" title="配置 Hive"></a>配置 Hive</h1><h2 id="修改-hive-env-sh"><a href="#修改-hive-env-sh" class="headerlink" title="修改 hive-env.sh"></a>修改 hive-env.sh</h2><p>修改<code>apache-hive-2.3.6-bin/conf</code>下的<code>hive-env.sh</code>，加上hadoop路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/usr/local/hadoop-2.7.7</span><br><span class="line">export HIVE_CONF_DIR=/usr/local/apache-hive-2.3.6-bin/conf</span><br></pre></td></tr></table></figure><h2 id="创建-hive-site-xml"><a href="#创建-hive-site-xml" class="headerlink" title="创建 hive-site.xml"></a>创建 hive-site.xml</h2><p>在<code>apache-hive-2.3.6-bin/conf</code>下创建<code>hive-site.xml</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://localhost:3306/metastore?useSSL=false&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;metadata is stored in a MySQL server&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;MySQL JDBC driver class&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;user name for connecting to mysql server&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;你的密码&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;password for connecting to mysql server&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="添加-java连接-mysql-的jar包"><a href="#添加-java连接-mysql-的jar包" class="headerlink" title="添加 java连接 mysql 的jar包"></a>添加 java连接 mysql 的jar包</h2><p>在<code>apache-hive-2.3.6-bin/lib</code>下添加 <a href="https://mvnrepository.com/artifact/mysql/mysql-connector-java" target="_blank" rel="noopener">java 连接 mysql 的jar包</a>（要对应mysql版本）。</p><p>我的是<code>mysql-connector-java-8.0.15.jar</code></p><h1 id="配置-mysql"><a href="#配置-mysql" class="headerlink" title="配置 mysql"></a>配置 mysql</h1><h2 id="配置远程登录权限-这个我没配，记着备用"><a href="#配置远程登录权限-这个我没配，记着备用" class="headerlink" title="配置远程登录权限(这个我没配，记着备用)"></a>配置远程登录权限(这个我没配，记着备用)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 切换成mysql库</span><br><span class="line">use mysql; </span><br><span class="line"># 查询用户信息</span><br><span class="line">select User,Host,authentication_string from user; </span><br><span class="line"># 设置远程登录权限</span><br><span class="line">grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;xxxxxxxx&apos; with grant option; </span><br><span class="line"># 刷新配置信息</span><br><span class="line">flush privileges;</span><br><span class="line"># 退出</span><br><span class="line">exit</span><br></pre></td></tr></table></figure><h2 id="建-metastore-表"><a href="#建-metastore-表" class="headerlink" title="建 metastore 表"></a>建 metastore 表</h2><p>表的名字与之前<code>hive-site.xml</code>中配置的目录名一样</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;value&gt;jdbc:mysql://localhost:3306/metastore?useSSL=false&lt;/value&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database metastore</span><br></pre></td></tr></table></figure><h1 id="开始使用"><a href="#开始使用" class="headerlink" title="开始使用"></a>开始使用</h1><h2 id="初始化mysql元数据库（首次使用时）"><a href="#初始化mysql元数据库（首次使用时）" class="headerlink" title="初始化mysql元数据库（首次使用时）"></a>初始化mysql元数据库（首次使用时）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure><h2 id="使用hive"><a href="#使用hive" class="headerlink" title="使用hive"></a>使用hive</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  apache-hive-2.3.6-bin bin/hive</span><br><span class="line">Logging initialized using configuration in file:/usr/local/apache-hive-2.3.6-bin/conf/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">hive &gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍hive的安装，使用 mysql 做 hive 的元数据库&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>flume-基本原理</title>
    <link href="http://yoursite.com/2019/09/12/flume-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2019/09/12/flume-基本原理/</id>
    <published>2019-09-12T08:25:19.000Z</published>
    <updated>2019-09-12T08:30:53.708Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了flume的基本原理和使用</p><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/flume/agent.png" alt></p><p>Flume是一种<strong>分布式</strong>，<strong>可靠且可用</strong>的服务，用于<strong>有效地收集，聚合和传输大量日志数据</strong>，是基于<strong>流式</strong>的简单灵活的架构。</p><p>它具有可靠的<strong>可靠性机制</strong>和许多<strong>故障转移和恢复机制</strong>，具有强大的容错性。</p><p>它使用简单的<strong>可扩展</strong>数据模型，允许在线分析应用程序。</p><h1 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h1><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/flume/flume.png" alt></p><h1 id="六大组件"><a href="#六大组件" class="headerlink" title="六大组件"></a>六大组件</h1><p>3大基本组件，3个辅助组件。这些组件都支持用户自定义。</p><h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><p>用于数据的收集。将数据捕获后可以先进行自定义处理，然后将数据封装到事件（event） 里（如event的body），最后将事件推入Channel中。</p><p><strong>常见的source</strong>:</p><p><strong>Avro Source</strong>、<strong>Exce Source</strong>、<strong>Spooling Directory Source</strong>、<strong>NetCat Source</strong>、Syslog Source、Syslog TCP Source、Syslog UDP Source、HTTP Source、<strong>HDFS Source</strong> 等等。</p><h2 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h2><p>用于连接Source和Sink的组件，是数据的缓冲区。它可以将事件暂存到内存中也可以持久化到本地磁盘上， 直到Sink处理完该事件。</p><p><strong>常见的channel</strong>:</p><p><strong>Memory Channel</strong>、<strong>File Channel</strong>、<strong>Kafka Channel</strong>等等。</p><h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><p>从channel中取出数据，再将数据存入相应的存储文件系统，数据库，或者提交到远程服务器。</p><p><strong>常见的sink</strong>:</p><p><strong>HDFS sink</strong>、<strong>Logger sink</strong>、<strong>Avro sink</strong>、<strong>File Roll sink</strong>、Null sink、HBase sink 等等。</p><h2 id="Interceptor"><a href="#Interceptor" class="headerlink" title="Interceptor"></a>Interceptor</h2><p>对Source收集的数据，进行<strong>分类</strong>或者<strong>拦截</strong>。可以将多个Interceptor连接形成拦截器链。</p><p><strong>用法</strong></p><ul><li>分类：自定义分类逻辑，将分类属性(k,v类型)，加入event的headers中，然后使用<strong>MultiplexingChannelSelector</strong>选择器 选择放入哪个channel中。</li></ul><ul><li>拦截：自定义丢弃逻辑，将不要的event设为<strong>null</strong>即可。</li></ul><h2 id="ChannelSelector"><a href="#ChannelSelector" class="headerlink" title="ChannelSelector"></a>ChannelSelector</h2><p>将event放入指定的channel中。</p><p><strong>2种ChannelSelector</strong>:</p><ul><li><p><strong>ReplicatingChannelSelector</strong>（默认） ：将事件放入所有channel中。（用于复制，也就是备份）</p></li><li><p><strong>MultiplexingChannelSelector</strong> ：结合<strong>Interceptor</strong>使用。根据header，将event放到指定的channel中。</p></li></ul><h2 id="sinkgroups中的SinkProcessor"><a href="#sinkgroups中的SinkProcessor" class="headerlink" title="sinkgroups中的SinkProcessor"></a>sinkgroups中的SinkProcessor</h2><p>按照指定算法将event分配到sink组的sink中。需要先指定一个sink组，再选择SinkProcessor，它会根据配置的分配方式自动将event分到组里的sink中。注意默认的SinkProcessor中，没有sink组的概念，不需要配置，也就是一对一。</p><p><strong>2种SinkProcessor</strong>:</p><ul><li><p><strong>LoadBalanceSinkProcessor</strong> ：负载均衡。可选择分配方式：如随机分配、轮询分配等等</p></li><li><p><strong>FailoverSinkProcessor</strong> ：优先级分配（多用于故障转移）。指定sink的优先级，按优先级分配。</p></li></ul><h1 id="两个Transaction"><a href="#两个Transaction" class="headerlink" title="两个Transaction"></a>两个Transaction</h1><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/flume/transaction.png" alt></p><h2 id="Put-Transaction"><a href="#Put-Transaction" class="headerlink" title="Put Transaction"></a>Put Transaction</h2><p>它实现 source 将 event 发送至 channel，带有容错机制，可以分为以下阶段：</p><ul><li><strong>doPut</strong>: 将批数据写入临时缓冲区putList</li><li><strong>doCommit</strong>: 检查channel内存队列是否足够合并。</li><li><strong>doRollback</strong>: channel内存队列空间不足，回滚数据</li></ul><h2 id="Take-Transaction"><a href="#Take-Transaction" class="headerlink" title="Take Transaction"></a>Take Transaction</h2><p>它实现 sink 将 event 从 channel 中提取出来，带有容错机制，可以分为以下阶段：</p><ul><li><strong>doTake</strong>: 将数据提取到临时缓冲区takeList</li><li><strong>doCommit</strong>: 数据发送成功的前提下，清除临时缓冲区takeList</li><li><strong>doRollback</strong>: 数据发送过程中如果出现异常，rollback将临时缓冲区takeList中的数据归还给channel内存队列。</li></ul><h1 id="一些案例"><a href="#一些案例" class="headerlink" title="一些案例"></a>一些案例</h1><ul><li><strong>日志复制（备份）</strong></li></ul><p>单source，多channel，多sink</p><p>使用 ReplicatingChannelSelector，将每个event分到多个channel再传到sink中，实现复制（备份）。</p><ul><li><strong>日志分类</strong></li></ul><p>单source，多channel，多sink， 配置拦截器</p><p>使用 MultiplexingChannelSelector，根据拦截器将每个event，分到指定的channel中再传到sink中，实现分类。</p><ul><li><strong>负载均衡</strong></li></ul><p>单source，单channel，多sink（组成一个sink组）</p><p>使用LoadBalanceSinkProcessor，选择分配方式：如随机分配、轮询分配，将event递给相应sink。</p><ul><li><strong>故障转移</strong></li></ul><p>单source，单channel，多sink（组成一个sink组）</p><p>使用FailoverSinkProcessor，给sink组里的sink指定优先级，只有优先级最高的会接收，当它挂了，次优先级的才会接收。</p><ul><li><strong>日志聚合</strong></li></ul><p>多source，单channel，单sink</p><p>直接将多个源的数据，用一个channel接收。</p><ul><li><strong>可以使用第三方框架Ganglia对flume实现监控</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍了flume的基本原理和使用&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flume" scheme="http://yoursite.com/tags/flume/"/>
    
  </entry>
  
  <entry>
    <title>spark-DataFrame和DataSet</title>
    <link href="http://yoursite.com/2019/09/09/spark-DataFrame%E5%92%8CDataSet/"/>
    <id>http://yoursite.com/2019/09/09/spark-DataFrame和DataSet/</id>
    <published>2019-09-09T01:26:14.000Z</published>
    <updated>2019-09-09T01:28:15.128Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了在spark中DataFrame和DataSet，以及它们之间的相互转换。</p><a id="more"></a><h1 id="概念分析"><a href="#概念分析" class="headerlink" title="概念分析"></a>概念分析</h1><p><strong>DataFrame</strong></p><p>类似传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。也就是普通RDD添加结构化信息得到。</p><p><strong>DataSet</strong></p><p>强类型的，存储的是对象。由<code>DataFrame</code>添加类属性得到。</p><p>相同点</p><ul><li><p>都是基于RDD的，所以都有RDD的特性，如懒加载，分布式，不可修改，分区等等。但执行sql性能比RDD高，因为spark自动会使用优化策略执行。说白了你手撸的干不过开发者写的。</p></li><li><p>均支持sparksql的操作，还能注册临时表，进行sql语句操作</p></li><li><p><code>DataFrame</code>和<code>Dataset</code>均可使用模式匹配获取各个字段的值和类型</p></li><li><p><code>DataFrame</code>也叫<code>Dataset[Row]</code>，每一行的类型是Row </p></li></ul><p>不同点</p><p>因为<code>DataFrame</code>也叫<code>Dataset[Row]</code>，所以我们理解了<strong>Row</strong>和<strong>普通对象</strong>的区别就好办了</p><ul><li><p><strong>Row</strong>的数据结构类似一个数组，只有顺序，切记。<strong>普通对象</strong>的数据结构也就是对象。</p></li><li><p>因此，访问<strong>Row</strong>只能通过如：<code>getInt(i: Int)</code>解析数据 或者 通过模式匹配得到数据；而<strong>普通对象</strong>可以通过 <code>.</code>号 直接访问对象中成员变量。</p></li><li><p>同理，<strong>Row</strong>中数据没类型，没办法在编译的时候检查是否有类型错误（弱类型的概念）；相反<strong>普通对象</strong>可以（强类型）。</p></li></ul><h1 id="RDD、DataFrame和DataSet转换"><a href="#RDD、DataFrame和DataSet转换" class="headerlink" title="RDD、DataFrame和DataSet转换"></a>RDD、DataFrame和DataSet转换</h1><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/dataframe2dataset.png" alt></p><p><strong>注意</strong></p><ul><li><p>原始RDD类型是 <code>RDD[(Int, String)]</code></p></li><li><p>DataFrame -&gt; RDD 时，变成了<code>RDD[Row]</code></p></li><li><p>DataSet -&gt; RDD时，变成了<code>RDD[User]</code></p></li></ul><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="SQL风格（主要）"><a href="#SQL风格（主要）" class="headerlink" title="SQL风格（主要）"></a>SQL风格（主要）</h2><ul><li>创建一个DataFrame</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df: DataFrame = spark.read.json(<span class="string">"people.json"</span>)</span><br></pre></td></tr></table></figure><ul><li>对DataFrame创建一个临时表(临时表是Session范围内有效，也可以创建全局的)</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br></pre></td></tr></table></figure><ul><li>通过SQL语句实现对表的操作</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"SELECT * FROM people"</span>).show()</span><br></pre></td></tr></table></figure><h2 id="DSL风格（次要）"><a href="#DSL风格（次要）" class="headerlink" title="DSL风格（次要）"></a>DSL风格（次要）</h2><ul><li>创建一个DataFrame</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df: DataFrame = spark.read.json(<span class="string">"people.json"</span>)</span><br></pre></td></tr></table></figure><ul><li>使用DataFrame的api</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">"name"</span>).show()</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select($<span class="string">"name"</span>, $<span class="string">"age"</span> + <span class="number">1</span>).show()</span><br></pre></td></tr></table></figure><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p><code>DataFrame</code>和<code>Dataset</code>都是为了方便我们执行sql的，因此当我们把数据转化成它们之后，写好sql逻辑，剩下的就交给咱们spark吧！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍了在spark中DataFrame和DataSet，以及它们之间的相互转换。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="sparksql" scheme="http://yoursite.com/tags/sparksql/"/>
    
  </entry>
  
  <entry>
    <title>spark-窄、宽依赖和任务划分</title>
    <link href="http://yoursite.com/2019/09/09/spark-%E7%AA%84%E3%80%81%E5%AE%BD%E4%BE%9D%E8%B5%96%E5%92%8C%E4%BB%BB%E5%8A%A1%E5%88%92%E5%88%86/"/>
    <id>http://yoursite.com/2019/09/09/spark-窄、宽依赖和任务划分/</id>
    <published>2019-09-09T01:13:52.000Z</published>
    <updated>2019-09-09T01:25:22.763Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了在spark中窄、宽依赖的划分以及任务划分</p><a id="more"></a><h1 id="窄依赖和宽依赖"><a href="#窄依赖和宽依赖" class="headerlink" title="窄依赖和宽依赖"></a>窄依赖和宽依赖</h1><h2 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h2><ul><li>每一个父RDD的Partition最多被子RDD的一个Partition使用</li><li>独生子女：一个爹RDD只有一个子</li></ul><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/dependence/Narrow-dependence.png" alt></p><h2 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h2><ul><li>每一个父RDD的Partition被子RDD的多个Partition使用，伴随shuffle</li><li>超生：一个爹RDD有多个子</li></ul><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/dependence/Wide.jpg" alt></p><h2 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h2><p>由于还没学shuffle，所以从宏观简单思考。学一个东西不能死记硬背，最好的理解就是：问问自己<strong>为啥要分窄依赖和宽依赖？</strong></p><p>先分析例子：</p><ul><li>例1：用map时，一个分区里的数据经过函数，形成新的数据，大家你搞你的我搞我的，互不干扰。</li><li>例2：用合并操作时，多个分区合到一个分区，同样，各走各的，顶走跑之前计算下新偏移量（这个偏移别人没跑完我也知道），也可以说是互不干扰</li><li>例3：用groupbykey时，这下可不是互不干扰了，因为需要比较洗牌，你得等你的伙伴（另一个分区）算完了，才能执行groupbykey。</li></ul><p>因此我觉得这就是所谓的宽依赖：<strong>别的分区没跑完，不能执行下一步，需要等待</strong>。只有当大家都准备好了，才可以一起进行洗牌。由于分区里的数据顺序之前是乱的，所以shuffle时一般都会拆开，然后送到不同的子分区。这就造成了结果——超生。说实话，如果你从结果出发去思考，是不好区分例2例3的。</p><p>接着，划分窄依赖（<strong>别的分区没跑完，可以执行下一步</strong>）和宽依赖（<strong>别的分区没跑完，不可以执行下一步</strong>）的原因显而易见。我们可以把窄依赖的步骤划分到一起，它可以一路执行，不需要等待，直到宽依赖步骤卡住（必须等其它分区执行完）。这个从窄依赖一路执行到宽依赖的过程，可以在逻辑上划分成一个<strong>stage</strong>。这也就是常说的<strong>宽依赖是划分Stage的依据</strong>。</p><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/dependence/stage.jpg" alt></p><h1 id="任务划分"><a href="#任务划分" class="headerlink" title="任务划分"></a>任务划分</h1><p>RDD任务的切分，分为：Application、Job、Stage和Task，而且每一层都是<strong>1对n</strong>的关系</p><h2 id="4个名词"><a href="#4个名词" class="headerlink" title="4个名词"></a>4个名词</h2><ul><li><strong>Application</strong>：初始化一个SparkContext即生成一个Application</li><li><strong>Job</strong>：一个Action算子就会生成一个Job</li><li><strong>Stage</strong>：根据RDD之间的依赖关系的不同将Job划分成不同的Stage，遇到一个宽依赖则划分一个Stage。</li><li><strong>Task</strong>：Stage是一个TaskSet，将Stage划分的结果发送到不同的Executor执行即为一个Task。</li></ul><h2 id="个人理解-1"><a href="#个人理解-1" class="headerlink" title="个人理解"></a>个人理解</h2><p>同样思考为啥要划分这么多东西？</p><ul><li><strong>Application</strong></li></ul><p>一个spark不止跑一个程序吧，所以一个程序一个 Application理所当然，进而生成一个AppMaster管理它。</p><ul><li><strong>Job</strong></li></ul><p>一个程序有许多转换算子和行动算子。只有执行到<strong>行动操作才真正改变数据</strong>，所以把截止到行动算子的算子划一个job合情合理吧。而且我们从源码也可以看到，执行一个行动操作，就会执行<code>sc.runJob(...)</code></p><ul><li><strong>Stage</strong></li></ul><p>在一个Job中，有的可一路执行到宽依赖的，不需要等待，按这个划分为一个Stage。这个不理解的再看看上面的分析。</p><ul><li><strong>Task</strong></li></ul><p>在一个Stage中，我们观察最后一组分区，也就是shuffer前的，由于到这里都是可以一路执行的，所以按最后一组分区的个数，一个分区划一个Task。此时都划到分区了，自然不用划分了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍了在spark中窄、宽依赖的划分以及任务划分&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark-Yarn流程解析</title>
    <link href="http://yoursite.com/2019/09/02/spark-Yarn%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2019/09/02/spark-Yarn流程解析/</id>
    <published>2019-09-02T08:13:28.000Z</published>
    <updated>2019-09-09T01:12:27.955Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了在spark中Yarn的工作流程和一些总结</p><a id="more"></a><h1 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h1><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/spark-yarn.png" alt="YARN-Cluster流程图" title="YARN-Cluster流程图"></p><p>主要流程和<a href="https://zouxxyy.github.io/2019/08/31/hadoop-Yarn%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/#more" target="_blank" rel="noopener">Yarn的流程</a>一样，不同的就是紫色部分。这里采用的是spark的<strong>yarn-cluster</strong>模式，driver在APPMaster中。</p><h1 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h1><h2 id="解耦思想"><a href="#解耦思想" class="headerlink" title="解耦思想"></a>解耦思想</h2><ul><li><p>ResourceManager管理资源调度，与NodeManager直接联系；Driver负责执行计算，与Executor也就是一个个Task直接联系。</p></li><li><p><strong>计算和资源调度解耦</strong>：ResourceManager和Driver靠中间件AppMaster联系起来；Executor和NodeManager靠中间件Container联系起来</p></li><li><p>此时计算框架是<strong>可插拔</strong>的，如：spark计算框架（紫色部分）代替mapreduce。</p></li></ul><h2 id="Client和Cluster模式"><a href="#Client和Cluster模式" class="headerlink" title="Client和Cluster模式"></a>Client和Cluster模式</h2><p>spark上yarn有两种管理模式，<strong>YARN-Client</strong>和<strong>YARN-Cluster</strong>。</p><p>主要区别是：SparkContext初始化位置不同，也就是了Driver所在位置的不同。</p><table><thead><tr><th align="center">client</th><th align="center">master</th></tr></thead><tbody><tr><td align="center">driver在Client上</td><td align="center">driver在AppMaster上</td></tr><tr><td align="center">日志可以直接在Client上看到</td><td align="center">日志在某个节点上</td></tr><tr><td align="center">Client连接不能断开</td><td align="center">Client连接可以断开</td></tr><tr><td align="center">适合交互和调试</td><td align="center">适合生产环境</td></tr></tbody></table><h1 id="其它管理模式"><a href="#其它管理模式" class="headerlink" title="其它管理模式"></a>其它管理模式</h1><h2 id="local模式"><a href="#local模式" class="headerlink" title="local模式"></a>local模式</h2><p>单机模式 <code>--master local[*]</code></p><h2 id="Standalone模式"><a href="#Standalone模式" class="headerlink" title="Standalone模式"></a>Standalone模式</h2><p>不用Yarn，用Spark自带的Standalone资源管理器，它把节点分成<strong>Master</strong>和<strong>Worker</strong>。类似RM和NM，但它没有AppMaster。也分为Client模式和cluster模式。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍了在spark中Yarn的工作流程和一些总结&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="yarn" scheme="http://yoursite.com/tags/yarn/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>hadoop-Yarn流程解析</title>
    <link href="http://yoursite.com/2019/08/31/hadoop-Yarn%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2019/08/31/hadoop-Yarn流程解析/</id>
    <published>2019-08-31T10:50:22.000Z</published>
    <updated>2019-09-02T03:25:39.261Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了Yarn的工作流程和一些总结</p><a id="more"></a><h1 id="Yarn运行机制流程图"><a href="#Yarn运行机制流程图" class="headerlink" title="Yarn运行机制流程图"></a>Yarn运行机制流程图</h1><p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/Yarn.png?raw=true" alt="Yarn运行机制流程图" title="Yarn运行机制流程图"></p><h1 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h1><p>（1）作业提交</p><ol start="0"><li>Client调用<code>job.waitForCompletion</code>方法，向整个集群提交MapReduce作业。</li><li>Client向RM申请一个作业id。 </li><li>RM给Client返回该job资源的提交路径(<code>hdfs://.../.staging/</code>)和作业<code>application_id</code>。</li><li>Client在该路径提交jar包、切片信息和配置文件。</li><li>Client提交完资源后，向RM申请运行MrAppMaster。</li></ol><p>（2）作业初始化</p><ol start="5"><li>当RM收到Client的请求后，将该job添加到资源调度器中，将job初始化成task。</li><li>某一个空闲的NM领取到该Job。</li><li>在该NM中创建Container，并产生MRAppmaster(一个job创建一个)，它管理该job。</li><li>下载之前Client提交的资源到本地。</li></ol><p>（3）任务分配</p><ol start="9"><li>MRAppMaster向RM申请运行多个MapTask任务资源。</li><li>RM将运行MapTask任务分配给另外两个NodeManager，另外两个NodeManager分别领取任务并创建容器。</li></ol><p>（4）任务执行</p><ol start="11"><li>MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</li><li>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</li><li>ReduceTask向MapTask获取相应分区的数据。</li><li>程序运行完毕后，MR会向RM申请注销自己。</li></ol><h1 id="一些细节"><a href="#一些细节" class="headerlink" title="一些细节"></a>一些细节</h1><h2 id="4大组件"><a href="#4大组件" class="headerlink" title="4大组件"></a>4大组件</h2><ul><li><strong>ResourceManager</strong>：总的老大：处理客户端请求，监控NodeManager，启动或监控ApplicationMaster，资源的分配与调度</li><li><strong>NodeManager</strong>：单个节点的老大：管理单个节点的资源，处理来自ResourceManager、ApplicationMaster的命令</li><li><strong>ApplicationMaster</strong>：单个job的老大：负责数据切分，为应用程序申请资源并分配内部的任务，任务的监控与容错</li><li><strong>Container</strong>：资源抽象：如内存、cpu、磁盘、网络等</li></ul><h2 id="3种资源调度器"><a href="#3种资源调度器" class="headerlink" title="3种资源调度器"></a>3种资源调度器</h2><ul><li><strong>FIFO</strong>：先进先出</li><li><strong>Capacity Scheduler</strong>：多FIFO队列，会对同一用户提交资源进行限定，会把任务分配给更闲的队列。</li><li><strong>Fair Scheduler</strong>：多队列，按缺额排序，缺额大者优先执行</li></ul><h2 id="任务推测执行机制"><a href="#任务推测执行机制" class="headerlink" title="任务推测执行机制"></a>任务推测执行机制</h2><ul><li>问题：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成。</li><li>办法：为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。</li><li>前提：每个Task只能有一个备份任务，当前Job已完成的Task必须不小于0.05（5%）。</li><li>不适用：任务间存在严重的负载倾斜；特殊任务，比如任务向数据库中写数据。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍了Yarn的工作流程和一些总结&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="yarn" scheme="http://yoursite.com/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>hadoop-HDFS流程解析</title>
    <link href="http://yoursite.com/2019/08/31/hadoop-HDFS%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2019/08/31/hadoop-HDFS流程解析/</id>
    <published>2019-08-31T07:04:57.000Z</published>
    <updated>2019-08-31T07:12:19.630Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了HDFS读写数据、NameNode和SecondaryNameNode、DataNode和NameNode的交互</p><a id="more"></a><h1 id="HDFS写数据"><a href="#HDFS写数据" class="headerlink" title="HDFS写数据"></a>HDFS写数据</h1><h2 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h2><p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/readHDFS.png?raw=true" alt="HDFS写数据" title="HDFS写数据"></p><h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><ol><li>客户端调用DS模块向NameNode请求上传文件。</li><li>NameNode会检查目标文件和父目录是否已存在，再返回是否可以上传</li><li>假设文件为200M，客户端请求上传第一个 Block ，希望得到DataNode服务器位置。</li><li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3，用它们存储数据。</li><li>客户端通过FSDataOutputStream模块请求dn1建立上传数据通道，dn1收到请求会继续请求dn2，然后dn2请求dn3，直到将这个通信管道建立完成。</li><li>dn3、dn2、dn1逐级应答客户端。</li><li>客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。（类似队列，以Packet为单位）</li><li>当一个Block（0-128M）传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</li><li>向NameNode汇报上传完毕。</li></ol><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ul><li>DataNode位置选择，以默认3副本为例：第一个副本是最近的一般是它自己；第二个副本选择同一机架（同一路由）的不同节点；第三个副本是另一机架的随机节点。</li><li>数据传递以包为单位，第一个节点收到一个包，就把包传递给下一个DataNode。并不是等数据传完，再传递。</li></ul><h1 id="HDFS读数据"><a href="#HDFS读数据" class="headerlink" title="HDFS读数据"></a>HDFS读数据</h1><h2 id="流程图-1"><a href="#流程图-1" class="headerlink" title="流程图"></a>流程图</h2><p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/writeHDFS.png?raw=true" alt="HDFS读数据" title="HDFS读数据"></p><h2 id="具体步骤-1"><a href="#具体步骤-1" class="headerlink" title="具体步骤"></a>具体步骤</h2><ol><li>客户端调用DS模块向NameNode请求下载文件。</li><li>NameNode会检查目标文件是否存在，再通过查询元数据，返回文件块所在的DataNode地址。</li><li>客户端通过FSDataInputStream模块向dn1（就近挑选）请求读取 Block1。</li><li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li><li>当一个Block（0-128M）传输完成之后，客户端再次请求下载Block2。（重复执行2-4步）。</li><li>向NameNode汇报下载完毕。</li></ol><h2 id="注意事项-1"><a href="#注意事项-1" class="headerlink" title="注意事项"></a>注意事项</h2><ul><li>如果块的第一个副本请求失败，会向第二个副本请求，依次类推。</li></ul><h1 id="NameNode和SecondaryNameNode"><a href="#NameNode和SecondaryNameNode" class="headerlink" title="NameNode和SecondaryNameNode"></a>NameNode和SecondaryNameNode</h1><h2 id="流程图-2"><a href="#流程图-2" class="headerlink" title="流程图"></a>流程图</h2><p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/NNand2NN.png?raw=true" alt="NameNode和SecondaryNameNode" title="NameNode和SecondaryNameNode"></p><h2 id="具体步骤-2"><a href="#具体步骤-2" class="headerlink" title="具体步骤"></a>具体步骤</h2><p>第一阶段：NameNode</p><ol><li>第一次启动NameNode（格式化）后，会创建Fsimage（镜像文件）和Edits（编辑日志）文件。以后启动，会直接加载镜像文件和编辑日志到内存，此时会进行合并操作。</li><li>假设此时客户端提出了增删改的请求。</li><li>NameNode记录之前的编辑日志（edits_n），更新新日志到滚动日志（edits_inprogress_n）中。</li><li>日志记录完毕后，NameNode在内存中对数据进行增删改。</li></ol><p>第二阶段：SecondaryNameNode</p><ol><li>Secondary NameNode向NameNode询问是否需要CheckPoint。</li><li>如果需要，Secondary NameNode请求执行CheckPoint。</li><li>NameNode滚动日志。</li><li>将滚动前的编辑日志（edits_001）和镜像文件(fsimage)拷贝到Secondary NameNode。</li><li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</li><li>合并后，生成新的镜像文件fsimage.chkpoint。</li><li>拷贝fsimage.chkpoint到NameNode。</li><li>NameNode将fsimage.chkpoint重新命名成fsimage。</li></ol><h2 id="注意事项-2"><a href="#注意事项-2" class="headerlink" title="注意事项"></a>注意事项</h2><ul><li>Fsimage 和 Edits文件？</li></ul><p>fsimage是NameNode内存中元数据序列化后形成的文件。Edits中记录客户端更新元数据信息的每一步操作。每次执行增删改时，先改日志再改文件。好处是：如果保证中途gg，可以保证操作不丢失，便于复原。</p><ul><li>为啥要Secondary NameNode？</li></ul><p>首先要知道只有NameNode重启时，edit.log才会合并到fsimage文件中，所以运行时间久了就会有3个问题：edis.log文件会变的很大；NameNode下次重启会花费很长时间；fsimage文件文件很旧，如果中途挂掉就很睿智。</p><p>为了解决上述问题，SecondaryNameNode诞生，每隔一定时间辅助合并NameNode的edit.log到fsimage文件中。从上述流程图就可以发现，它做的就是这个。</p><ul><li>什么时候执行CheckPoint？</li></ul><p>（1） 用户定时 （2）edit.log 满了</p><ul><li>Secondary NameNode是热备份吗？</li></ul><p>不是，可以发现Secondary NameNode合并的是滚动前的edis，它总是比NameNode的编辑日志少一点。</p><h1 id="DataNode和NameNode"><a href="#DataNode和NameNode" class="headerlink" title="DataNode和NameNode"></a>DataNode和NameNode</h1><h2 id="流程图-3"><a href="#流程图-3" class="headerlink" title="流程图"></a>流程图</h2><p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/DataNode.png?raw=true" alt="NameNode和DataNode" title="NameNode和DataNode"></p><h2 id="具体步骤-3"><a href="#具体步骤-3" class="headerlink" title="具体步骤"></a>具体步骤</h2><ol><li>DataNode启动后向NameNode注册。</li><li>NameNode告知注册成功。</li><li>DataNode周期性（1小时）的向NameNode上报所有的块信息。</li><li>DataNode每3秒发送一次心跳，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。</li><li>超过10分钟没有收到心跳，表示该节点不可用。</li></ol><h2 id="注意事项-3"><a href="#注意事项-3" class="headerlink" title="注意事项"></a>注意事项</h2><ul><li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度、校验和 以及时间戳。</li><li>节点增加：新节点配置好后，自动向NameNode注册的。</li><li>节点退役：NameNode可以通过白名单指定需要的节点；通过黑名单指定不要的节点。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍了HDFS读写数据、NameNode和SecondaryNameNode、DataNode和NameNode的交互&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="HDFS" scheme="http://yoursite.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>hadoop-MapReduce流程解析</title>
    <link href="http://yoursite.com/2019/08/30/hadoop-MapReduce%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2019/08/30/hadoop-MapReduce流程解析/</id>
    <published>2019-08-30T08:38:07.000Z</published>
    <updated>2019-09-17T09:22:42.855Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了MapReduce的详细流程和一些总结</p><a id="more"></a><h1 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h1><p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/MapTask.png?raw=true" alt="MapTask流程图" title="MapTask流程图"></p><p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/ReduceTask.png?raw=true" alt="ReduceTask流程图" title="ReduceTask流程图"></p><p><img src="https://github.com/Zouxxyy/bigdata-learning/blob/master/hadoop-learning/data/image/Shuffer.png?raw=true" alt="Shuffer流程图" title="Shuffer流程图"></p><h1 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h1><p>逻辑上可以这样划分：1-10是MapTask ；11-16是ReduceTask；7-14是shuffer</p><h3 id="1-待处理文本"><a href="#1-待处理文本" class="headerlink" title="1. 待处理文本"></a><strong>1. 待处理文本</strong></h3><p>这里假设是<code>/user/input</code>目录下的<code>ss.txt</code>    文件，大小为<strong>200M</strong>。</p><h3 id="2-客户端submit（）"><a href="#2-客户端submit（）" class="headerlink" title="2. 客户端submit（）"></a><strong>2. 客户端submit（）</strong></h3><p>发生在client端，主要获取3个信息：</p><p>（1）<strong>Job.split</strong> ：找到文件ss.txt，根据切片算法，得到切片的元数据信息（起始位置，长度以及所在节点等）如把ss.txt分成两片 0-128M 和 128M-200M</p><p>（2）<strong>Job.xml</strong>：任务的配置信息</p><p>（3）<strong>wc.jar</strong>：任务的jar包</p><p>（可以在<code>/tmp/hadoop-zxy/mapred/staging/zxy1248702679/.staging/</code>下找到它们）</p><h3 id="3-提交信息"><a href="#3-提交信息" class="headerlink" title="3. 提交信息"></a><strong>3. 提交信息</strong></h3><p>将刚刚获取的任务规划信息，提交到资源管理器上，我们这里用Yarn。</p><h3 id="4-RM计算MapTask数量"><a href="#4-RM计算MapTask数量" class="headerlink" title="4. RM计算MapTask数量"></a><strong>4. RM计算MapTask数量</strong></h3><p>接着向Yarn的RM申请资源，RM根据任务规划信息用户Job分成Task，并把任务下发给节点。这里我们数据分成了2片，根据默认规则，会有2个MapTask各自处理一片数据。</p><h3 id="5-根据采用的InputFormat读取数据"><a href="#5-根据采用的InputFormat读取数据" class="headerlink" title="5. 根据采用的InputFormat读取数据"></a><strong>5. 根据采用的InputFormat读取数据</strong></h3><p>这里采用默认的TextInputFormat类，按行读取每条记录。key是行偏移量，value是该行的内容。</p><h3 id="6-执行Mapper的map"><a href="#6-执行Mapper的map" class="headerlink" title="6. 执行Mapper的map()"></a><strong>6. 执行Mapper的map()</strong></h3><p>根据用户的代码执行map逻辑，把结果写入Context中。</p><h3 id="7-向环形缓存区写入数据"><a href="#7-向环形缓存区写入数据" class="headerlink" title="7. 向环形缓存区写入数据"></a><strong>7. 向环形缓存区写入数据</strong></h3><p>环形缓存区取一点：一边写索引，一边写真实数据。达到80%时发生溢写</p><h3 id="8-分区、排序"><a href="#8-分区、排序" class="headerlink" title="8. 分区、排序"></a><strong>8. 分区、排序</strong></h3><p>一种2次排序，先按区号排，再对key排序（快排）。得到一组按区排好序的数据。注意：这步是在环形缓存区就可以执行的，且排序排的是索引，真实数据不用动。且此时可以使用第一次Combiner合并操作。</p><h3 id="9-溢出写入文件"><a href="#9-溢出写入文件" class="headerlink" title="9. 溢出写入文件"></a><strong>9. 溢出写入文件</strong></h3><p>环形缓存区达到80%时，溢写到磁盘上。注意写磁盘前已经完成了分区、排序、合并、压缩等操作。此时生成第一组溢写文件<code>spillN.out</code> 与元数据<code>spillN.out.index</code>。</p><h3 id="10-MapTask的归并排序"><a href="#10-MapTask的归并排序" class="headerlink" title="10. MapTask的归并排序"></a><strong>10. MapTask的归并排序</strong></h3><p>将多组溢写文件，以分区为单位进行归并排序，写入磁盘形成大文件<code>output/file.out</code>，与索引文件<code>output/file.out.index</code>。此时一个MapTask任务完成，得到一个分区有序的数据。注意：在归并排序时可以使用第二次Combiner合并操作。</p><h3 id="11-启动ReduceTask"><a href="#11-启动ReduceTask" class="headerlink" title="11. 启动ReduceTask"></a><strong>11. 启动ReduceTask</strong></h3><p>假设分区数为2，此时启动2个ReduceTask，一个ReduceTask处理一个区的数据。</p><h3 id="12-copy数据"><a href="#12-copy数据" class="headerlink" title="12. copy数据"></a><strong>12. copy数据</strong></h3><p>ReduceTask从各个MapTask上拷贝它要处理的区的数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p><h3 id="13-ReduceTask的归并排序"><a href="#13-ReduceTask的归并排序" class="headerlink" title="13. ReduceTask的归并排序"></a><strong>13. ReduceTask的归并排序</strong></h3><p>把同区的数据复制到同一个ReduceTask后，对它们进行归并排序</p><h3 id="14-分组"><a href="#14-分组" class="headerlink" title="14. 分组"></a><strong>14. 分组</strong></h3><p>默认把key相同的数据分到一组。用户可以继承WritableComparator，自定义分组规则。</p><h3 id="15-执行Reducer的Reduce"><a href="#15-执行Reducer的Reduce" class="headerlink" title="15. 执行Reducer的Reduce()"></a><strong>15. 执行Reducer的Reduce()</strong></h3><p>根据用户的代码执行reduce逻辑，把结果写入Context中。注意：一次读一组，value是迭代器对象，包含一个组的全部数据。</p><h3 id="16-根据采用的OutputFormat读取数据"><a href="#16-根据采用的OutputFormat读取数据" class="headerlink" title="16. 根据采用的OutputFormat读取数据"></a><strong>16. 根据采用的OutputFormat读取数据</strong></h3><p>这里采用默认的TextOutputFormat类，按行写入key和value，key和value用tab分开。</p><h1 id="一些总结"><a href="#一些总结" class="headerlink" title="一些总结"></a>一些总结</h1><h3 id="1个逻辑"><a href="#1个逻辑" class="headerlink" title="1个逻辑"></a><strong>1个逻辑</strong></h3><p><strong>先分区 -&gt; 再排序 -&gt; 再分组</strong></p><p>分区：用户定义分区数后，默认按hash分区。用户也可以继承<code>Partitioner</code>，自定义分区规则。ReduceTask的个数一般等于分区数。</p><p>排序：默认对key排序，key必须实现<code>WritableComparable</code>接口。用户可以重写<code>WritableComparable</code>接口的<code>compareTo()</code>方法，定义自己的排序规则。</p><p>分组：默认把key相同的数据分到一组。用户也可以继承WritableComparator，自定义分组规则。用于reduce阶段，一次读取一组.</p><h3 id="2次合并"><a href="#2次合并" class="headerlink" title="2次合并"></a><strong>2次合并</strong></h3><p>Combiner的父类就是Reducer，它可以通过对Map阶段的局部结果进行汇总，减少输出。</p><p>时机： 2次，<strong>分区排序后、MapTask的归并排序时</strong>。</p><p>条件：不能影响业务逻辑 且 输入输出的范型一致</p><h3 id="3次排序"><a href="#3次排序" class="headerlink" title="3次排序"></a><strong>3次排序</strong></h3><p>MapTask：</p><p><strong>分区排序</strong>：在缓行缓冲区进行，是一种2次排序。先按分区号排序，再对key排序（快排）。</p><p><strong>归并排序</strong>：对每组溢写的数据，进行的按区，归并排序。</p><p>ReduceTask：</p><p><strong>归并排序</strong>：对从MapTask拷贝的同区数据，进行的归并排序。</p><h3 id="分片和分区"><a href="#分片和分区" class="headerlink" title="分片和分区"></a><strong>分片和分区</strong></h3><p>分片：<strong>分片数决定MapTask的个数</strong>。在客户端即完成，举FileInputFormat切片机制为例：简单的按文件长度进行切片，切片大小等于块大小（默认128M），切片时是对文件单独切片。</p><p>分区：<strong>分区数决定ReduceTask的个数</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍了MapReduce的详细流程和一些总结&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="mapreduce" scheme="http://yoursite.com/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop-单节点伪分布式搭建</title>
    <link href="http://yoursite.com/2019/08/24/hadoop-%E5%8D%95%E8%8A%82%E7%82%B9%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2019/08/24/hadoop-单节点伪分布式搭建/</id>
    <published>2019-08-24T03:43:32.000Z</published>
    <updated>2019-09-29T09:22:55.207Z</updated>
    
    <content type="html"><![CDATA[<p>在mac上搭建hadoop伪分布式</p><a id="more"></a><p>操作系统： macOS Mojave 10.14.5</p><p>JDK : 1.8</p><p>hadoop: 2.7.7</p><h2 id="1-Java和Hadoop安装"><a href="#1-Java和Hadoop安装" class="headerlink" title="1. Java和Hadoop安装"></a>1. Java和Hadoop安装</h2><p>下载和安装相信都没问题</p><p>注意的就是：</p><ul><li>环境变量设置好，我是mac所以javahome是$(/usr/libexec/java_home);我是zsh所以修改.zshrc，修改完别忘了source。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME=$(/usr/libexec/java_home)</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop-2.7.7</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><ul><li>由于我将hadoop放在<code>/usr/local/</code>目录下，所以需要更改hadoop文件夹权限</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chown -R zxy:admin /usr/local/hadoop-2.7.7</span><br></pre></td></tr></table></figure><h2 id="2-配置SSH"><a href="#2-配置SSH" class="headerlink" title="2. 配置SSH"></a>2. 配置SSH</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -P <span class="string">""</span></span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">ssh localhost</span><br></pre></td></tr></table></figure><h2 id="3-伪分布式配置"><a href="#3-伪分布式配置" class="headerlink" title="3. 伪分布式配置"></a>3. 伪分布式配置</h2><h3 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h3><p>修改 <code>/usr/local/hadoop-2.7.7/etc/hadoop/core-site.xml</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/local/hadoop-2.7.7/data/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li><p><strong>fs.defaultFS</strong>  HDFS 的NameNode地址</p></li><li><p><strong>hadoop.tmp.dir</strong>  hadoop 临时文件地址，自己指定</p></li></ul><h3 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h3><p>修改 <code>/usr/local/hadoop-2.7.7/etc/hadoop/hdfs-site.xml</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>dfs.replication</strong>  HDFS文件存储的副本个数，默认3。因为我们这只有一个节点，所以设置1.（单一节点至多存一份节点）</li></ul><h3 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h3><p>修改 <code>/usr/local/hadoop-2.7.7/etc/hadoop/yarn-site.xml</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;localhost&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 开启聚合日志 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;http://localhost:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>arn.log-aggregation-enable</strong> 开启日志聚合</li><li><strong>yarn.resourcemanager.hostname</strong>  yarn的ResourceManager地址</li></ul><h3 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h3><p>修改 <code>/usr/local/hadoop-2.7.7/etc/hadoop/mapred-site.xml</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;localhost:10020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;localhost:19888&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>mapreduce.framework.name</strong>  采用yarn管理MR</li><li><strong>mapreduce.jobhistory.address</strong>  历史服务器端口地址</li><li><strong>mapreduce.jobhistory.webapp.address</strong>  历史服务器web端地址</li></ul><h3 id="检查JAVA-HOME"><a href="#检查JAVA-HOME" class="headerlink" title="检查JAVA_HOME"></a>检查JAVA_HOME</h3><p>hadoop-env.sh、mapred-env.sh、yarn-env.sh，在这三个文件检查是否添加JAVA_HOME路径，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=$JAVA_HOME</span><br></pre></td></tr></table></figure><h2 id="4-使用"><a href="#4-使用" class="headerlink" title="4. 使用"></a>4. 使用</h2><ul><li>开HDFS</li></ul><p>第一次使用要格式化(仅限第一次使用时，以后要格式化需删除log、data目录下的文件)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure><p>开启namenode、datanode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure><ul><li>开yarn</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line">yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><ul><li>开historyserver</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><ul><li>可以用jps查看效果</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br><span class="line">35953 JobHistoryServer</span><br><span class="line">32930</span><br><span class="line">35682 NodeManager</span><br><span class="line">35990 Jps</span><br><span class="line">35559 DataNode</span><br><span class="line">35624 ResourceManager</span><br><span class="line">35502 NameNode</span><br></pre></td></tr></table></figure><ul><li>测试</li></ul><p>创建一个文件夹zxytest，里面随便放一个文件，上传到hdfs测试wordcount</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put zxytest /</span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /zxytest /zxyout</span><br></pre></td></tr></table></figure><ul><li>关闭    </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mr-jobhistory-daemon.sh stop historyserver</span><br><span class="line">yarn-daemon.sh stop resourcemanager</span><br><span class="line">yarn-daemon.sh stop nodemanager</span><br><span class="line">hadoop-daemon.sh stop namenode</span><br><span class="line">hadoop-daemon.sh stop datanode</span><br></pre></td></tr></table></figure><ul><li>可视化地址</li></ul><p>所有任务: <a href="http://localhost:8088/" target="_blank" rel="noopener">http://localhost:8088/</a></p><p>DataNode: <a href="http://localhost:50070/" target="_blank" rel="noopener">http://localhost:50070/</a></p><p>历史服务器: <a href="http://localhost:19888/" target="_blank" rel="noopener">http://localhost:19888/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在mac上搭建hadoop伪分布式&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>3种Linux命令后台执行方法：&amp;、nohup、tmux</title>
    <link href="http://yoursite.com/2019/06/26/3%E7%A7%8DLinux%E5%91%BD%E4%BB%A4%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8C%E6%96%B9%E6%B3%95%EF%BC%9A-%E3%80%81nohup%E3%80%81tmux/"/>
    <id>http://yoursite.com/2019/06/26/3种Linux命令后台执行方法：-、nohup、tmux/</id>
    <published>2019-06-26T03:25:03.000Z</published>
    <updated>2019-06-26T03:28:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>让命令在后台跑起来！</p><a id="more"></a><h1 id="amp"><a href="#amp" class="headerlink" title="&amp;"></a>&amp;</h1><p>用法：<code>指令 &amp;</code></p><p>说明： 将指令放入后台执行，会将输出打印到前台，<strong>当执行该指令的终端gg时，它也gg</strong></p><p>终止方法：</p><ul><li><code>jobs</code> 查看它 -&gt; <code>fg %num</code> 取出它 -&gt; <code>Ctrl+c</code>终止它</li><li>直接退出终端</li></ul><h1 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h1><p>用法：<code>nohup 指令 &amp;</code></p><p>说明： 将指令放入后台执行，不会将输出打印到前台，<strong>当执行该指令的终端gg时，它不gg</strong></p><p>终止方法：</p><ul><li>未退出终端时：<code>jobs</code> 查看它 -&gt; <code>fg %num</code> 取出它 -&gt; <code>Ctrl+c</code>终止它</li><li>退出终端时：在新终端连接中，找到PID号，kill它</li></ul><h1 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h1><p>简介：</p><p>tmux可以在后台新建一个终端，并且用户退出后创建的终端仍然存在</p><p>用法：</p><ul><li>创建session</li></ul><p><code>tmux new -s $session_name</code></p><ul><li>列出session</li></ul><p><code>tmux ls</code></p><ul><li>临时退出session</li></ul><p><code>Ctrl+b d</code> </p><ul><li>进入已存在的session</li></ul><p><code>tmux a -t $session_name</code></p><ul><li>删除指定session</li></ul><p><code>tmux kill-session -t $session_name</code></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li><p>&amp;  简单，安全，退出终端，程序自动结束</p></li><li><p>nohup 退出终端后，必须通过pid号杀程序</p></li><li><p>tmux 谁用谁知道，一般情况，时间长的程序，我都用它 </p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;让命令在后台跑起来！&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="shell" scheme="http://yoursite.com/tags/shell/"/>
    
  </entry>
  
</feed>
