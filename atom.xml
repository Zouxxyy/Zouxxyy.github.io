<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZxysHexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-01-02T01:06:30.500Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zouxxyy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>我的2019</title>
    <link href="http://yoursite.com/2019/12/31/%E6%88%91%E7%9A%842019/"/>
    <id>http://yoursite.com/2019/12/31/我的2019/</id>
    <published>2019-12-31T08:59:34.000Z</published>
    <updated>2020-01-02T01:06:30.500Z</updated>
    
    <content type="html"><![CDATA[<p>2019小结，多图慎点</p><a id="more"></a><h1>github</h1><p>一度执着commit，一度觉得太水，一度又执着，一度又… <strong>希望明年加油，争取贡献PR</strong></p><p><img src="https://i.loli.net/2019/12/31/UtnVwE4IDSa1cZe.png" alt></p><h1>Hexo</h1><p>2019年，在小破站写(shui)了<strong>49篇文(shui)章(wen)</strong>。学习上，打60分吧，勉强及格，明年要更加努力！</p><p><img src="https://i.loli.net/2019/12/31/ZuN1XwA7nH8ylsf.png" alt></p><h1>读书</h1><p>欣赏来自豆瓣的蜜汁黑色散点图，一次更新后变成了这样，实在无力吐槽</p><p><img src="https://i.loli.net/2019/12/31/PLiUoA8uN4nfsBb.png" alt></p><p>微信读书的2019成绩单，<strong>勉强超过平均水平?!</strong> 今年相比去年热情降低了不少，感觉很难遇到让人眼前一亮的书了。</p><p>今年看过最好看的3本是：<strong>《平凡的世界》 《霍乱时期的爱情》 《追风筝的人》</strong>。经典不愧是经典～</p><p>夸夸<strong>良心</strong>的微信读书：开局一条狗，<strong>无线卡全靠白嫖</strong>。倘若收费，我必奉陪。</p><p><img src="https://i.loli.net/2019/12/31/IKZmn3JNlhv8BT7.jpg" alt></p><h1>billbill</h1><p>9月开始玩的，王者时刻生成后再自己剪一下，投了几篇，自娱自乐加朋友圈形成完美闭环。奈何画(A)质(V)不忍直视，真想放弃，希望<strong>王者的自动剪辑画质能提升点</strong>！</p><p>也夸夸B站，在B站上学习了很多免费学习资源，而且用户体验也挺不错的，果然程(si)序(fei)员(zai)懂二次元，奥利给！至于大会员嘛，下次一定，ヾﾉ≧∀≦)o</p><p><img src="https://i.loli.net/2019/12/31/EzojgVJNAnXhrCY.jpg" alt></p><h1>音乐</h1><p>2020年，也要<strong>将非主流贯彻到底</strong>！年度听的最多竟然是嵩哥的，是不是计算错了？？</p><p><img src="https://i.loli.net/2019/12/31/Oq9WUB1KQ3gMj7s.jpg" alt></p><h1>王者荣耀</h1><p>人老了反应跟不上了，混混王者就行，以后玩的时间越来越少了。话说，我的<strong>李白荣耀典藏又跳票了</strong>，<strong>韩信的鼠年传说被伽罗抢了</strong>。🐶 策划，我要转战隔壁lol手游！</p><p>貌似没找到王者的2019年报，也太偷懒了吧？？</p><h1>2020 加油！</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2019小结，多图慎点&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式版本控制系统-Git</title>
    <link href="http://yoursite.com/2019/12/31/%E5%88%86%E5%B8%83%E5%BC%8F%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F-Git/"/>
    <id>http://yoursite.com/2019/12/31/分布式版本控制系统-Git/</id>
    <published>2019-12-31T01:10:52.000Z</published>
    <updated>2019-12-31T01:14:45.901Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要包括Git的用法加底层</p><h1>使用</h1><h2 id="初始化相关"><a class="header-anchor" href="#初始化相关">¶</a>初始化相关</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --system 操作系统级别； --global 用户级别； 不加： 项目级别 </span></span><br><span class="line">$ git config --global user.name <span class="string">"zouxxyy"</span></span><br><span class="line">$ git config --global user.email <span class="string">"xxxxxxxxxxxxxx"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看配置</span></span><br><span class="line">$ git config --list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建git对象，会生成 .git 目录</span></span><br><span class="line">$ git init</span><br><span class="line">$ ls</span><br><span class="line">HEAD                 指示当前分支的文件</span><br><span class="line">config               配置文件</span><br><span class="line">hooks.  （目录）      存放钩子脚本，比如提交代码前或者后的自动执行操作</span><br><span class="line">objects （目录）      存放所有历史记录的</span><br><span class="line">branches             指示目前被检测出的分支</span><br><span class="line">description          仓库描述性信息的文件</span><br><span class="line">info    （目录）      存放全局性的排除文件。比如 mac 进去查看就有.DS_Store</span><br><span class="line">refs    （目录）      存放分支与tags的提交对象的指针</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="文件相关"><a class="header-anchor" href="#文件相关">¶</a>文件相关</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加新文件（由未跟踪到跟踪状态，同时也是暂存状态）或者添加修改文件（由修改状态到暂存状态）（粗略这样说，其实底层做了更多事）</span></span><br><span class="line">$ git add test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交文件（由暂存状态到提交状态）；可加参数 -a 跳过 add 步骤（未跟踪的不能提交）</span></span><br><span class="line">$ git commit -m <span class="string">"commit message"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件的状态</span></span><br><span class="line">$ git status</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看未被 add 的修改</span></span><br><span class="line">$ git diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看已 add 但未被 commit 的修改</span></span><br><span class="line">$ git diff --staged</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件 （同时让文件由跟踪变成未跟踪状态）（注意它 和手动删除文件再执行 git add 效果一样）</span></span><br><span class="line">$ git rm test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重命名</span></span><br><span class="line">$ git mv test.txt new.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 commit log；可加 --oneline 参数简写</span></span><br><span class="line">$ git <span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 完整 commit log（只要改动了HEAD）</span></span><br><span class="line">$ git reflog</span><br></pre></td></tr></table></figure><h2 id="分支相关"><a class="header-anchor" href="#分支相关">¶</a>分支相关</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建分支</span></span><br><span class="line">$ git branch branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换分支 (切换前保证 status 干净)</span></span><br><span class="line">$ git checkout branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并切换</span></span><br><span class="line">$ git checkout -b branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示所有分支 可加参数 -v 查看它们的最后一次提交</span></span><br><span class="line">$ git branch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除分支（需先切到另一个分支）可加参数 -D 强制删除</span></span><br><span class="line">$ git branch -d branchname</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建分支，并指向指定的提交对象 (时光机)</span></span><br><span class="line">$ git branch branchname commitHash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分支合并 （合并前切回主分支，再合并需要合并的分支）</span></span><br><span class="line"><span class="comment"># 合并可能会产生冲突，需要手动修改冲突的文件，再提交</span></span><br><span class="line">$ git merge otherBranch</span><br></pre></td></tr></table></figure><h2 id="stash相关"><a class="header-anchor" href="#stash相关">¶</a>stash相关</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有时想切分支，但又不想提交，可以用到 stash 功能，是一种暂存的栈。这个栈是针对分支的～</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前分支的 stash</span></span><br><span class="line">$ git stash list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将未提交的改动 入栈（接着可以安全执行切分支操作）</span></span><br><span class="line">$ git stash</span><br><span class="line"></span><br><span class="line"><span class="comment"># （切回分支后）弹出 改动</span></span><br><span class="line">$ git pop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后面两个一般不用，我们栈里只放一个元素，git stash 入栈，git pop 出栈</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只取指定改动，不删除</span></span><br><span class="line">$ git stash apply stashName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定的栈里的东西</span></span><br><span class="line">$ git stash drop stashName</span><br></pre></td></tr></table></figure><h2 id="回退相关"><a class="header-anchor" href="#回退相关">¶</a>回退相关</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 撤回工作目录的修改（未add -&gt; 未修改）</span></span><br><span class="line">$ git checkout -- fileName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回暂存区的修改 （未commit -&gt; 未add）</span></span><br><span class="line">$ git reset [HEAD] fileName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可用于修改刚刚提交的message（相当于后退一步，再重新提交）</span></span><br><span class="line">$ git commit --amend</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意下面3个命令都可以撤回到指定提交对象（把 HEAD~ 改为指定的 commitHash 即可）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回上一次提交 （commit -&gt; 未commit）</span></span><br><span class="line">$ git reset --soft HEAD~</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回上一次提交同时撤回暂存区的修改 （commit -&gt; 未add）</span></span><br><span class="line">$ git reset [--mix] HEAD~</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回上一次提交同时撤回暂存区的修改同时撤回工作目录的修改 （commit -&gt; 未修改，危险！）</span></span><br><span class="line">$ git reset --hard HEAD~</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撤回少用 reset --hard，最好使用 branch </span></span><br><span class="line">$ git branch recoverBranchName commitHash</span><br></pre></td></tr></table></figure><h2 id="标签相关"><a class="header-anchor" href="#标签相关">¶</a>标签相关</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出所有tag</span></span><br><span class="line">$ git tag</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打tag</span></span><br><span class="line">$ git tag tagName commitHash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定tag</span></span><br><span class="line">$ git tag -d tagName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切到指定tag，然后创新分支，防止头部分离</span></span><br><span class="line">$ git checkout tagName</span><br><span class="line">$ git checkout -b branchName</span><br></pre></td></tr></table></figure><h2 id="远程相关"><a class="header-anchor" href="#远程相关">¶</a>远程相关</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看全部远程分支</span></span><br><span class="line">$ git remote -v</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加远程分支别名(注意：orgin 只个别名)</span></span><br><span class="line">$ git remote add orgin https://xxxx.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 克隆远程仓库到本地（此时自动创建全部远程跟踪分支，同时生成一个关联远程master的本地分支（注意））</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://xxxx.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由本地分支创建远程跟踪分支，再推到远程分支，但并不关联（注意）</span></span><br><span class="line"><span class="comment"># 推荐仅当创建一个新分支，并想把它推到远程时，才使用它；并且接着执行关联操作，也就下一行，之后仅用 git push 即可</span></span><br><span class="line">$ git push orgin branchName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本地分支（已创建）关联远程跟踪分支</span></span><br><span class="line">$ git branch -u orgin/branchName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取远程分支（全部分支）到远程跟踪分支中</span></span><br><span class="line"><span class="comment"># 推荐仅当远程有了新分支时，才使用它（注意）</span></span><br><span class="line">$ git fetch orgin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个本地分支，同时关联远程跟踪分支</span></span><br><span class="line">$ git branch -b branchName orgin/branchName</span><br><span class="line">$ git branch --track orgin/branchName (效果一样，快捷写法)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉去远程分支到远程跟踪分支和本地分支（本地分支已经关联远程跟踪分支），相当于执行 git fetch + git merge</span></span><br><span class="line">$ git pull</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除远程分支</span></span><br><span class="line">$ git push orgin --delete branchName</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出没用远程跟踪分支，并删除</span></span><br><span class="line">$ git remote prune orgin --dry-run</span><br><span class="line">$ git remote orgin</span><br></pre></td></tr></table></figure><p>看起来比较复杂，但只要记住一个逻辑链就好，也就是按以下3步走：</p><ol><li><p><strong>建立远程跟踪分支</strong></p><ul><li><code>git clone https://xxxx.git</code></li><li><code>git push orgin branchName</code> 推<strong>新分支</strong>到远程</li><li><code>git fetch orgin</code> 拉远程的<strong>新分支</strong></li></ul></li><li><p><strong>本地分支关联远程跟踪分支</strong></p><ul><li><code>git checkout branchName</code> 创建本地新分支（和远程分支同名） 并关联 (还是它简单)</li><li><code>git branch -u orgin/branchName</code> 当前分支 关联</li><li><code>git branch --track orgin/branchName</code> 创建本地新分支 关联</li></ul></li><li><p><strong>愉快使用</strong></p><ul><li><code>git pull</code> 拉到本地</li><li><code>git push</code> 推到远程（先 pull，解决冲突再 push）</li></ul></li></ol><p>上面是自己团队的项目（<strong>有权限</strong>），当想为开源项目做贡献（<strong>没权限</strong>）时，使用 <strong>pull request</strong>， 大致流程（用到再研究）为：</p><ol><li><p><strong>folk 一份</strong></p></li><li><p><strong>在 folk 的库中，作出改动并提交，注意提交前先拉远程库，保持最新。</strong></p></li><li><p><strong>进网站（如github），点 pullRequest</strong></p></li></ol><h1>底层</h1><p>git 的底层数据结构是一种在 <code>object</code> 目录中存放的 k-v 类型的数据。key 是 hash值 (前两位是子文件夹名，后面是文件名)，value 是 数据内容。</p><p>数据内容 分为 <strong>git 对象</strong>（<code>blob</code>）、<strong>树对象</strong>（<code>tree</code>）、<strong>提交对象</strong>（<code>commit</code>）</p><h2 id="git-对象"><a class="header-anchor" href="#git-对象">¶</a>git 对象</h2><p><strong>代表文件，它是真正存数据的</strong>。它是一对一的（注意：<strong>一个文件的历史文件都算一个单独的文件</strong>）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向数据库中写数据，并返回hash键值。存的是一个blob对象</span></span><br><span class="line">$ git <span class="built_in">hash</span>-object -w test.txt</span><br><span class="line">915c628f360b2d8c3edbe1ac65cf575b69029b61</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据是经过压缩的，可以用cat-file查看该数据</span></span><br><span class="line"><span class="comment"># -p 显示内容； -t 查看类型</span></span><br><span class="line">$ git cat-file -p 915c628f360b2d8c3edbe1ac65cf575b69029b61</span><br><span class="line">hello git</span><br></pre></td></tr></table></figure><h2 id="树对象"><a class="header-anchor" href="#树对象">¶</a>树对象</h2><p><strong>代表版本</strong>，可以理解成一个树对象指向多个 git 对象，形成一个版本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 暂存区 写入 test.txt 的首个版本（指针信息）</span></span><br><span class="line"><span class="comment"># --add 文件首次添加；--cacheinfo 表示添加的文件位于git数据库；100644 文件类型； test.txt 文件名</span></span><br><span class="line">$ git update-index --add --cacheinfo 100644 915c628f360b2d8c3edbe1ac65cf575b69029b61 test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看暂存区</span></span><br><span class="line">$ git ls-files -s</span><br><span class="line">100644 915c628f360b2d8c3edbe1ac65cf575b69029b61 0 test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成树对象。暂存区未清空</span></span><br><span class="line">$ git write-tree</span><br><span class="line">72203871fa4668ad777833634034dcd3426879db</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看树对象内容</span></span><br><span class="line">$ git cat-file -p 72203871fa4668ad777833634034dcd3426879db</span><br><span class="line">100644 blob 915c628f360b2d8c3edbe1ac65cf575b69029b61 test.txt</span><br></pre></td></tr></table></figure><h2 id="提交对象"><a class="header-anchor" href="#提交对象">¶</a>提交对象</h2><p><strong>相当于对版本添加描述信息</strong>。一个提交对象封装一个树对象，而且它是<strong>链式</strong>的，里面指向上一个提交对象。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成提交对象，后接树对象hash值</span></span><br><span class="line"><span class="comment"># 这里是第一次提交。以后可以加 -p 指定父提交对象</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">'first commit'</span> | git commit-tree 72203871fa4668ad777833634034dcd3426879db</span><br><span class="line">5402d3560f35d66f7f1e4e4665dd63bf3d786495</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看提交对象的内容</span></span><br><span class="line">$ git cat-file -p 5402d3560f35d66f7f1e4e4665dd63bf3d786495</span><br><span class="line">tree 72203871fa4668ad777833634034dcd3426879db</span><br><span class="line">author zouxxyy &lt;xxxxxxxxx@qq.com&gt; 1575450654 +0800</span><br><span class="line">committer zouxxyy &lt;xxxxxxxxx@qq.com&gt; 1575450654 +0800</span><br><span class="line"></span><br><span class="line">first commit</span><br></pre></td></tr></table></figure><h2 id="文件相关-v2"><a class="header-anchor" href="#文件相关-v2">¶</a>文件相关</h2><ul><li><p><code>git add test.txt</code> 相当于先将文件做成<code>git对象</code>存入<code>object</code> ，再将它（指针信息）放入暂存区，等待提交。该操作是<strong>绝对安全</strong>的，因为数据已经写入文件中。所以可以理解为啥只有 <code>git add</code>，而没有什么<code>git change</code>之类的，因为修改也会加<code>git对象</code>。</p></li><li><p><code>git commit -m &quot;xx&quot;</code> 由暂存区生成树对象，再生成提交对象。<strong>注意暂存区的东西不删除</strong></p></li><li><p><code>git rm test.txt</code> <strong>相当于将该文件对应的（指针信息）从暂存区中删除，同时删除工作目录中的文件</strong></p></li></ul><h2 id="分支相关-v2"><a class="header-anchor" href="#分支相关-v2">¶</a>分支相关</h2><p><strong>分支切换会切换工作目录</strong>，所以切换前，先把当前分支该提交的提交了，<strong>保证工作目录干净</strong>。</p><ul><li><p>2个重要的东西：</p><ul><li><code>refs</code> 存放各个分支（和tags）的提交对象的指针（hashkey）</li><li><code>HEAD</code> 指明当前分支的指针位置</li></ul></li><li><p><code>git branch branchname</code> 相当于在 <code>refs/heads</code> 目录中新建以<strong>分支名</strong>命名的当前分支的提交对象的指针</p></li><li><p><code>git checkout branchname</code> 更改<code>HEAD</code>文件；<strong>切换工作目录</strong>；<strong>影响暂存区</strong></p></li><li><p><code>git commit -m &quot;commit message&quot;</code> 更新 <code>refs</code> 中对应分支的提交对象</p></li></ul><h2 id="回退相关-v2"><a class="header-anchor" href="#回退相关-v2">¶</a>回退相关</h2><p><code>git checkout branchName</code> 和 <code>git reset --hard commitHash</code> 的区别 ：</p><ul><li><p><code>checkout</code> 只改HEAD ；<code>reset</code> HEAD 和 分支指针的一起改</p></li><li><p><code>checkout</code> 对工作目录是安全的；<code>reset --hard</code> 会<strong>完全强制覆盖工作目录</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要包括Git的用法加底层&lt;/p&gt;
&lt;h1&gt;使用&lt;/h1&gt;
&lt;h2 id=&quot;初始化相关&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#初始化相关&quot;&gt;¶&lt;/a&gt;初始化相关&lt;/h2&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# --system 操作系统级别； --global 用户级别； 不加： 项目级别 &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ git config --global user.name &lt;span class=&quot;string&quot;&gt;&quot;zouxxyy&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ git config --global user.email &lt;span class=&quot;string&quot;&gt;&quot;xxxxxxxxxxxxxx&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 查看配置&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ git config --list&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 创建git对象，会生成 .git 目录&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ git init&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ ls&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;HEAD                 指示当前分支的文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;config               配置文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hooks.  （目录）      存放钩子脚本，比如提交代码前或者后的自动执行操作&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;objects （目录）      存放所有历史记录的&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;branches             指示目前被检测出的分支&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;description          仓库描述性信息的文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;info    （目录）      存放全局性的排除文件。比如 mac 进去查看就有.DS_Store&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;refs    （目录）      存放分支与tags的提交对象的指针&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Tools" scheme="http://yoursite.com/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>spark源码-shuffle之BlockStoreShuffleReader</title>
    <link href="http://yoursite.com/2019/12/30/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BBlockStoreShuffleReader/"/>
    <id>http://yoursite.com/2019/12/30/spark源码-shuffle之BlockStoreShuffleReader/</id>
    <published>2019-12-30T01:33:22.000Z</published>
    <updated>2019-12-30T01:42:59.921Z</updated>
    
    <content type="html"><![CDATA[<p>spark的唯一 ShuffleReader ：BlockStoreShuffleReader</p><a id="more"></a><p>spark 版本 2.4.3</p><h1>引入</h1><p><a href="https://zouxxyy.github.io/2019/11/29/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BShuffleManager/" target="_blank" rel="noopener">之前</a>提过，<code>getShuffleWrite</code> 只在 <code>ShuffleMapTask</code> 中出现。</p><p>那么<code>getShuffleRead</code>呢？由于不管哪个 Task 都需要读数据，于是就把该步骤封装在RDD的<code>computer</code>方法中。以下是<code>ShuffleRDD</code>中的<code>computer</code>方法。通过调用 <code>shuffleManager</code> 的 <code>getReader</code> 就获取了本文的主角<code>BlockStoreShuffleReader</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">  <span class="keyword">val</span> dep = dependencies.head.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]]</span><br><span class="line">  <span class="comment">// 是不是觉得 split.index, split.index + 1 很怪，后面会发现这是怎么回事</span></span><br><span class="line">  <span class="type">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class="number">1</span>, context)</span><br><span class="line">    .read()</span><br><span class="line">    .asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>大致流程</h1><ol><li><p><strong>获取初始迭代器<code>ShuffleBlockFetcherIterator</code></strong>（<code>Iterator[(BlockId, InputStream)</code>）</p></li><li><p><strong>反序列化出实例，并生成 k,v 迭代器</strong>（<code>Iterator[(key, value)]</code>）</p></li><li><p><strong>添加readMetrics，再封装成<code>InterruptibleIterator</code></strong></p></li><li><p><strong>进行聚合操作</strong>。分有无聚合，当有聚合时，又分是否执行过map聚合</p></li><li><p><strong>进行排序操作</strong>。分有无排序。</p></li></ol><p><code>BlockStoreShuffleReader </code> 要干的事其实很容易理解，就是把<strong>不同 block 中同一分区的record</strong>，拉到指定的 reducer 中，再对它进行<strong>聚合和排序</strong>即可。一个 reducer 处理一个分区。</p><h1>源码</h1><h2 id="获取初始迭代器ShuffleBlockFetcherIterator"><a class="header-anchor" href="#获取初始迭代器ShuffleBlockFetcherIterator">¶</a>获取初始迭代器<code>ShuffleBlockFetcherIterator</code></h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取初始迭代器 ShuffleBlockFetcherIterator</span></span><br><span class="line"><span class="keyword">val</span> wrappedStreams = <span class="keyword">new</span> <span class="type">ShuffleBlockFetcherIterator</span>(</span><br><span class="line">  context,</span><br><span class="line">  blockManager.shuffleClient, <span class="comment">// 默认是 NettyBlockTransferService，如果使用外部shuffle系统则使用 ExternalShuffleClient</span></span><br><span class="line">  blockManager,</span><br><span class="line">  <span class="comment">// 通过它得到 2元tuple : (BlockManagerId, Seq[(BlockId, BlockSize)])</span></span><br><span class="line">  mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),</span><br><span class="line">  serializerManager.wrapStream,</span><br><span class="line">  <span class="comment">// Note: we use getSizeAsMb when no suffix is provided for backwards compatibility</span></span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.getSizeAsMb(<span class="string">"spark.reducer.maxSizeInFlight"</span>, <span class="string">"48m"</span>) * <span class="number">1024</span> * <span class="number">1024</span>,</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.getInt(<span class="string">"spark.reducer.maxReqsInFlight"</span>, <span class="type">Int</span>.<span class="type">MaxValue</span>),</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.get(config.<span class="type">REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS</span>),</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.get(config.<span class="type">MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM</span>),</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.getBoolean(<span class="string">"spark.shuffle.detectCorrupt"</span>, <span class="literal">true</span>))</span><br></pre></td></tr></table></figure><p>这是 <code>mapOutputTracker.getMapSizesByExecutorId</code> 里的关键步骤</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convertMapStatuses</span></span>(</span><br><span class="line">    shuffleId: <span class="type">Int</span>,</span><br><span class="line">    startPartition: <span class="type">Int</span>,</span><br><span class="line">    endPartition: <span class="type">Int</span>,</span><br><span class="line">    statuses: <span class="type">Array</span>[<span class="type">MapStatus</span>]): <span class="type">Iterator</span>[(<span class="type">BlockManagerId</span>, <span class="type">Seq</span>[(<span class="type">BlockId</span>, <span class="type">Long</span>)])] = &#123;</span><br><span class="line">  assert (statuses != <span class="literal">null</span>)</span><br><span class="line">  <span class="keyword">val</span> splitsByAddress = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">BlockManagerId</span>, <span class="type">ListBuffer</span>[(<span class="type">BlockId</span>, <span class="type">Long</span>)]]</span><br><span class="line">  <span class="comment">// mapTask 的个数决定了 Seq[(BlockId, BlockSize)] 内元素的个数，很好理解</span></span><br><span class="line">  <span class="keyword">for</span> ((status, mapId) &lt;- statuses.iterator.zipWithIndex) &#123;</span><br><span class="line">    <span class="keyword">if</span> (status == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> errorMessage = <span class="string">s"Missing an output location for shuffle <span class="subst">$shuffleId</span>"</span></span><br><span class="line">      logError(errorMessage)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">MetadataFetchFailedException</span>(shuffleId, startPartition, errorMessage)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 左闭右开，因此前面的 （split.index, split.index + 1）中的 split.index + 1，并没有什么软用  </span></span><br><span class="line">      <span class="keyword">for</span> (part &lt;- startPartition until endPartition) &#123;</span><br><span class="line">        <span class="keyword">val</span> size = status.getSizeForBlock(part)</span><br><span class="line">        <span class="keyword">if</span> (size != <span class="number">0</span>) &#123;</span><br><span class="line">          splitsByAddress.getOrElseUpdate(status.location, <span class="type">ListBuffer</span>()) +=</span><br><span class="line">              ((<span class="type">ShuffleBlockId</span>(shuffleId, mapId, part), size))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  splitsByAddress.iterator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>ShuffleBlockFetcherIterator  </code>这个迭代器的具体实现比较复杂，简单介绍下：</p><ul><li><p>分本地数据块 和 远程数据块</p></li><li><p>本地数据块直接调用 <code>BlockManager.getBlockData</code></p></li><li><p>远程数据块采用Netty通过网络获取</p></li></ul><p>下面是 <code>IndexShuffleBlockResolver</code> 中的 <code>getBlockData</code>。我们的索引文件（indexFile）终于派上用场</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getBlockData</span></span>(blockId: <span class="type">ShuffleBlockId</span>): <span class="type">ManagedBuffer</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> indexFile = getIndexFile(blockId.shuffleId, blockId.mapId)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> channel = <span class="type">Files</span>.newByteChannel(indexFile.toPath)</span><br><span class="line">  <span class="comment">// 根据reduceId选择索引</span></span><br><span class="line">  channel.position(blockId.reduceId * <span class="number">8</span>L)</span><br><span class="line">  <span class="keyword">val</span> in = <span class="keyword">new</span> <span class="type">DataInputStream</span>(<span class="type">Channels</span>.newInputStream(channel))</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> offset = in.readLong()</span><br><span class="line">    <span class="keyword">val</span> nextOffset = in.readLong()</span><br><span class="line">    <span class="keyword">val</span> actualPosition = channel.position()</span><br><span class="line">    <span class="keyword">val</span> expectedPosition = blockId.reduceId * <span class="number">8</span>L + <span class="number">16</span></span><br><span class="line">    <span class="keyword">if</span> (actualPosition != expectedPosition) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">s"SPARK-22982: Incorrect channel position after index file reads: "</span> +</span><br><span class="line">        <span class="string">s"expected <span class="subst">$expectedPosition</span> but actual position was <span class="subst">$actualPosition</span>."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FileSegmentManagedBuffer</span>(</span><br><span class="line">      transportConf,</span><br><span class="line">      getDataFile(blockId.shuffleId, blockId.mapId),</span><br><span class="line">      offset,</span><br><span class="line">      nextOffset - offset)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    in.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="反序列化出实例，并生成-k-v-迭代器"><a class="header-anchor" href="#反序列化出实例，并生成-k-v-迭代器">¶</a>反序列化出实例，并生成 k,v 迭代器</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> serializerInstance = dep.serializer.newInstance()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a key/value iterator for each stream</span></span><br><span class="line"><span class="keyword">val</span> recordIter = wrappedStreams.flatMap &#123; <span class="keyword">case</span> (blockId, wrappedStream) =&gt;</span><br><span class="line">  serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="添加readMetrics，再封装成InterruptibleIterator"><a class="header-anchor" href="#添加readMetrics，再封装成InterruptibleIterator">¶</a>添加readMetrics，再封装成<code>InterruptibleIterator</code></h2><p>readMetrics 里都是些记录数据，用于监控展示</p><p><code>InterruptibleIterator</code> 封装了任务中断功能</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Update the context task metrics for each record read.</span></span><br><span class="line"><span class="keyword">val</span> readMetrics = context.taskMetrics.createTempShuffleReadMetrics()</span><br><span class="line"><span class="keyword">val</span> metricIter = <span class="type">CompletionIterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>), <span class="type">Iterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>)]](</span><br><span class="line">  recordIter.map &#123; record =&gt;</span><br><span class="line">    readMetrics.incRecordsRead(<span class="number">1</span>) <span class="comment">// sparkUI 里的 record 数</span></span><br><span class="line">    record</span><br><span class="line">  &#125;,</span><br><span class="line">  context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> interruptibleIter = <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>)](context, metricIter)</span><br></pre></td></tr></table></figure><h2 id="进行聚合操作"><a class="header-anchor" href="#进行聚合操作">¶</a>进行聚合操作</h2><p>分有无聚合，当有聚合时，又分是否在 mapTask 时执行过map聚合</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> aggregatedIter: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = <span class="keyword">if</span> (dep.aggregator.isDefined) &#123;</span><br><span class="line">  <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="comment">// We are reading values that are already combined</span></span><br><span class="line">    <span class="comment">// 有 mapSideCombine，就是聚合(K, C)</span></span><br><span class="line">    <span class="keyword">val</span> combinedKeyValuesIterator = interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">    dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 无 mapSideCombine，就是聚合(K, V)</span></span><br><span class="line">    <span class="keyword">val</span> keyValuesIterator = interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">Nothing</span>)]]</span><br><span class="line">    dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">// 不聚合</span></span><br><span class="line">  interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="进行排序操作"><a class="header-anchor" href="#进行排序操作">¶</a>进行排序操作</h2><p>分是否需要排序。如果需要，就使用<code>ExternalSorter</code>在分区内部进行排序</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultIter = dep.keyOrdering <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Some</span>(keyOrd: <span class="type">Ordering</span>[<span class="type">K</span>]) =&gt;</span><br><span class="line">    <span class="comment">// Create an ExternalSorter to sort the data.</span></span><br><span class="line">    <span class="keyword">val</span> sorter =</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">C</span>, <span class="type">C</span>](context, ordering = <span class="type">Some</span>(keyOrd), serializer = dep.serializer)</span><br><span class="line">    sorter.insertAll(aggregatedIter)</span><br><span class="line">    context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class="line">    context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class="line">    context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class="line">    <span class="comment">// Use completion callback to stop sorter if task was finished/cancelled.</span></span><br><span class="line">    context.addTaskCompletionListener[<span class="type">Unit</span>](_ =&gt; &#123;</span><br><span class="line">      sorter.stop()</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="type">CompletionIterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>], <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]](sorter.iterator, sorter.stop())</span><br><span class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">    aggregatedIter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark的唯一 ShuffleReader ：BlockStoreShuffleReader&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark杂谈-指定文件在HDFS中的写入节点</title>
    <link href="http://yoursite.com/2019/12/28/spark%E6%9D%82%E8%B0%88-%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6%E5%9C%A8HDFS%E4%B8%AD%E7%9A%84%E5%86%99%E5%85%A5%E8%8A%82%E7%82%B9/"/>
    <id>http://yoursite.com/2019/12/28/spark杂谈-指定文件在HDFS中的写入节点/</id>
    <published>2019-12-28T06:52:51.000Z</published>
    <updated>2019-12-28T06:56:53.632Z</updated>
    
    <content type="html"><![CDATA[<p>项目中需要指定文件写到HDFS的具体哪个节点，通过查找API，找到一种解决办法。以 <code>TextOutputFormat</code> 为例测试</p><a id="more"></a><h1>write</h1><p>观察 <code>TextOutputFormat</code> 里的 <code>write</code> 方法，它使用了<code>out.write</code>将 record 写入 HDFS。找到 <code>out</code> ，修改它</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> nullKey = key == <span class="keyword">null</span> || key <span class="keyword">instanceof</span> NullWritable;</span><br><span class="line">  <span class="keyword">boolean</span> nullValue = value == <span class="keyword">null</span> || value <span class="keyword">instanceof</span> NullWritable;</span><br><span class="line">  <span class="keyword">if</span> (nullKey &amp;&amp; nullValue) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!nullKey) &#123;</span><br><span class="line">    writeObject(key);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!(nullKey || nullValue)) &#123;</span><br><span class="line">    out.write(keyValueSeparator);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!nullValue) &#123;</span><br><span class="line">    writeObject(value);</span><br><span class="line">  &#125;</span><br><span class="line">  out.write(newline);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>getRecordWriter</h1><p><code>out</code> 是一个<code>DataOutputStream</code>，它通过 <code>getRecordWriter</code>得到的。</p><p>默认是由 <code>FSDataOutputStream fileOut = fs.create(file, progress;</code> 生成，但它无法指定节点。</p><p>作出修改：</p><p><strong>关键是把它替换成 <code>DistributedFileSystem</code> 里的 <code>create</code> 方法，其中有个参数是 <code>favoredNodes </code>，顾名思义数据将优先存到它指定节点</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRecordWriter</span></span>(ignored: <span class="type">FileSystem</span>, job: <span class="type">JobConf</span>, name: <span class="type">String</span>, progress: <span class="type">Progressable</span>): <span class="type">RecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> keyValueSeparator = job.get(<span class="string">"mapreduce.output.textoutputformat.separator"</span>, <span class="string">"\t"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> file = <span class="type">FileOutputFormat</span>.getTaskOutputPath(job, name)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将 OutputStream 转成 DistributedFileSystem</span></span><br><span class="line">  <span class="keyword">val</span> fs = file.getFileSystem(job).asInstanceOf[<span class="type">DistributedFileSystem</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> favoredNodes = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">InetSocketAddress</span>](<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这里自己指定</span></span><br><span class="line">  favoredNodes(<span class="number">0</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.12"</span>, <span class="number">9866</span>) <span class="comment">// 这个port是该节点dataNode的port</span></span><br><span class="line">  favoredNodes(<span class="number">1</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.13"</span>, <span class="number">9866</span>)</span><br><span class="line">  favoredNodes(<span class="number">2</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.17"</span>, <span class="number">9866</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fileOut: <span class="type">FSDataOutputStream</span> = fs.create(file,</span><br><span class="line">    <span class="type">FsPermission</span>.getFileDefault.applyUMask(<span class="type">FsPermission</span>.getUMask(fs.getConf)),</span><br><span class="line">    <span class="literal">true</span>,</span><br><span class="line">    fs.getConf.getInt(<span class="type">IO_FILE_BUFFER_SIZE_KEY</span>, <span class="type">IO_FILE_BUFFER_SIZE_DEFAULT</span>),</span><br><span class="line">    fs.getDefaultReplication(file),</span><br><span class="line">    fs.getDefaultBlockSize(file),</span><br><span class="line">    <span class="literal">null</span>,</span><br><span class="line">    favoredNodes)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">TextOutputFormat</span>.<span class="type">LineRecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>](fileOut, keyValueSeparator)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>完整测试代码</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.examples.zxyTest</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.<span class="type">InetSocketAddress</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.<span class="type">CommonConfigurationKeysPublic</span>.&#123;<span class="type">IO_FILE_BUFFER_SIZE_DEFAULT</span>, <span class="type">IO_FILE_BUFFER_SIZE_KEY</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.permission.<span class="type">FsPermission</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FSDataOutputStream</span>, <span class="type">FileSystem</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hdfs.<span class="type">DistributedFileSystem</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.&#123;<span class="type">IntWritable</span>, <span class="type">Text</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.&#123;<span class="type">FileOutputFormat</span>, <span class="type">JobConf</span>, <span class="type">RecordWriter</span>, <span class="type">TextOutputFormat</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.<span class="type">Progressable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 测试 指定 HDFS 存储节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FavoredNodesTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder</span><br><span class="line">      .appName(<span class="string">"FavoredNodesTest"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .config(<span class="string">"spark.driver.memory"</span>, <span class="string">"2g"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>((<span class="string">"A"</span>, <span class="number">1</span>), (<span class="string">"B"</span>, <span class="number">2</span>), (<span class="string">"C"</span>, <span class="number">3</span>), (<span class="string">"D"</span>, <span class="number">4</span>)), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    rdd.saveAsHadoopFile(<span class="string">"hdfs://dell-r720/zxyTest/output"</span>,</span><br><span class="line">      classOf[<span class="type">Text</span>],</span><br><span class="line">      classOf[<span class="type">IntWritable</span>],</span><br><span class="line">      classOf[<span class="type">MyTextOutputFormat</span>[<span class="type">Text</span>, <span class="type">IntWritable</span>]])</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"Result has been saved"</span>)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTextOutputFormat</span>[<span class="type">K</span>, <span class="type">V</span>] <span class="keyword">extends</span> <span class="title">TextOutputFormat</span>[<span class="type">K</span>, <span class="type">V</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRecordWriter</span></span>(ignored: <span class="type">FileSystem</span>, job: <span class="type">JobConf</span>, name: <span class="type">String</span>, progress: <span class="type">Progressable</span>): <span class="type">RecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> keyValueSeparator = job.get(<span class="string">"mapreduce.output.textoutputformat.separator"</span>, <span class="string">"\t"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> file = <span class="type">FileOutputFormat</span>.getTaskOutputPath(job, name)</span><br><span class="line">    <span class="keyword">val</span> fs = file.getFileSystem(job).asInstanceOf[<span class="type">DistributedFileSystem</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3 副本位置</span></span><br><span class="line">    <span class="keyword">val</span> favoredNodes = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">InetSocketAddress</span>](<span class="number">3</span>)</span><br><span class="line">    favoredNodes(<span class="number">0</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.12"</span>, <span class="number">9866</span>) <span class="comment">// 9866是该节点dataNode的port</span></span><br><span class="line">    favoredNodes(<span class="number">1</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.13"</span>, <span class="number">9866</span>)</span><br><span class="line">    favoredNodes(<span class="number">2</span>) = <span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="string">"10.0.0.17"</span>, <span class="number">9866</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileOut: <span class="type">FSDataOutputStream</span> = fs.create(file,</span><br><span class="line">      <span class="type">FsPermission</span>.getFileDefault.applyUMask(<span class="type">FsPermission</span>.getUMask(fs.getConf)),</span><br><span class="line">      <span class="literal">true</span>,</span><br><span class="line">      fs.getConf.getInt(<span class="type">IO_FILE_BUFFER_SIZE_KEY</span>, <span class="type">IO_FILE_BUFFER_SIZE_DEFAULT</span>),</span><br><span class="line">      fs.getDefaultReplication(file),</span><br><span class="line">      fs.getDefaultBlockSize(file),</span><br><span class="line">      <span class="literal">null</span>,</span><br><span class="line">      favoredNodes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">TextOutputFormat</span>.<span class="type">LineRecordWriter</span>[<span class="type">K</span>, <span class="type">V</span>](fileOut, keyValueSeparator)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;项目中需要指定文件写到HDFS的具体哪个节点，通过查找API，找到一种解决办法。以 &lt;code&gt;TextOutputFormat&lt;/code&gt; 为例测试&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="HDFS" scheme="http://yoursite.com/tags/HDFS/"/>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>自动化运维工具-Ansible</title>
    <link href="http://yoursite.com/2019/12/24/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7-Ansible/"/>
    <id>http://yoursite.com/2019/12/24/自动化运维工具-Ansible/</id>
    <published>2019-12-24T08:11:22.000Z</published>
    <updated>2019-12-24T08:34:27.351Z</updated>
    
    <content type="html"><![CDATA[<p>Ansible 是一个开源的基于 OpenSSH 的自动化配置管理工具。可以用它来配置系统、部署软件和编排更高级的 IT 任务，比如持续部署或零停机更新。管理员可以通过 Ansible 在成百上千台计算机上同时执行指令(任务)，这是<a href="https://ansible-tran.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">官方文档</a>。</p><a id="more"></a><h1>配置组</h1><p>Ansible 可同时操作属于一个组的多台主机，在 <code>/etc/ansible/hosts</code> 中配置组</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/ansible/hosts</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用法：[组名] + hostnames</span></span><br><span class="line">[spark]</span><br><span class="line">dell-r730-[1:4]</span><br><span class="line"></span><br><span class="line">[hdfs]</span><br><span class="line">dell-r730-[1:4]</span><br><span class="line">dell-r730-7</span><br></pre></td></tr></table></figure><p>使用时可分为 <strong>ad-hoc</strong> 和 <strong>playbook</strong></p><h1>ad-hoc</h1><p>在命令行敲入的shell命令，去执行些简单的任务：</p><p><code>ansible 组名 -m 模块名 -a 具体操作 [-u 用户名] [--sudo] [-f 10]</code></p><ul><li><code>-m</code> 模块名</li><li><code>-a</code> 具体操作，<strong>一般是k v结构</strong>。</li><li><code>-u</code> 默认以当前用户的身份执行命令，也可手动指定</li><li><code>--sudo</code> 通过 sudo 去执行命令（ passwordless 模式 ）</li><li><code>-f</code> 并发量</li></ul><p>下面介绍些常用模块</p><h2 id="commond-和-shell"><a class="header-anchor" href="#commond-和-shell">¶</a>commond 和 shell</h2><p><code>commond </code>是默认的模块</p><p><code>commond</code> 和 <code>shell</code> 都是直接敲命令的。<code>command</code> 更安全，但<strong>不支持 shell 变量，也不支持管道等 shell 相关的东西</strong></p><p>shell使用变量时也存在限制，所以尽量<strong>不要敲太复杂的命令</strong>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印当前时间</span></span><br><span class="line">$ ansible zxy -a <span class="string">"date"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单管道命令</span></span><br><span class="line">$ ansible zxy -m shell -a <span class="string">"cat /etc/hosts &gt; test.txt"</span></span><br></pre></td></tr></table></figure><h2 id="file-和-copy"><a class="header-anchor" href="#file-和-copy">¶</a>file 和 copy</h2><p><code>file</code> 用于创建（删除）文件或文件夹，也可更改所有者权限<br><code>copy</code> 用与复制文件<br>（我觉得有点麻烦，没有commnd来的快点）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件（如果存在，更新时间戳）</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testfile state=touch"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建目录（如果存在，不进行任何操作）</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testdir state=directory"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件或目录</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testdir state=absent"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可加 owner 定义所有者，mode 定义 权限，recurse 对目录递归。</span></span><br><span class="line">$ ansible zxy -m file -a <span class="string">"path=/home/cluster/testdir state=directory owner=cluster mode=0777 recurse=true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制文件</span></span><br><span class="line"><span class="comment"># force=no 不强制覆盖，backup=no 不备份旧文件</span></span><br><span class="line">$ ansible zxy -m copy -a <span class="string">"src=/home/cluster/testfile dest=/home/cluster/testfile2 owner=cluster mode=0777"</span></span><br></pre></td></tr></table></figure><h2 id="synchronize"><a class="header-anchor" href="#synchronize">¶</a>synchronize</h2><p>文件或文件夹同步，有 push（默认）和 pull 模式。</p><p>默认启用了<code>archive</code>参数，该参数默认开启了recursive, links, perms, times, owner，group和-D参数。</p><p>可加 <code>--exclude=xxx</code> 忽略指定文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ansible zxy -m synchronize -a <span class="string">"src=/home/cluster/testfile dest=/home/cluster/testfile"</span></span><br></pre></td></tr></table></figure><h2 id="ping"><a class="header-anchor" href="#ping">¶</a>ping</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ansible zxy -m ping</span><br></pre></td></tr></table></figure><h1>playbook</h1><p>将一系列有序任务保存成yml文件，方便多次使用和有序的执行指定的任务。</p><ul><li>执行</li></ul><p><code>ansible-playbook playbook.yml</code></p><ul><li>格式</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">- hosts:</span> <span class="string">组名</span></span><br><span class="line"><span class="attr">  vars:</span></span><br><span class="line">    <span class="string">变量名:</span> <span class="string">值</span></span><br><span class="line"><span class="attr">  remote_user:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">  tasks:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">描述任务（自定义）</span></span><br><span class="line">    <span class="string">模块名:</span> <span class="string">具体操作</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">xxxx</span></span><br><span class="line">    <span class="string">模块名:</span> <span class="string">xxxxx</span></span><br><span class="line"><span class="attr">    notify:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">钩子名(</span> <span class="string">task</span> <span class="string">结束且该</span> <span class="string">task</span> <span class="string">有意义（改变了东西）时被触发，只执行一次)</span></span><br><span class="line"><span class="attr">  handlers:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">钩子名</span></span><br><span class="line">      <span class="string">模块名:</span> <span class="string">具体操作</span></span><br></pre></td></tr></table></figure><ul><li>例子</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">- hosts:</span> <span class="string">webservers</span></span><br><span class="line"><span class="attr">  vars:</span></span><br><span class="line"><span class="attr">    http_port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    max_clients:</span> <span class="number">200</span></span><br><span class="line"><span class="attr">  remote_user:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">  tasks:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ensure</span> <span class="string">apache</span> <span class="string">is</span> <span class="string">at</span> <span class="string">the</span> <span class="string">latest</span> <span class="string">version</span></span><br><span class="line"><span class="attr">    yum:</span> <span class="string">pkg=httpd</span> <span class="string">state=latest</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">write</span> <span class="string">the</span> <span class="string">apache</span> <span class="string">config</span> <span class="string">file</span></span><br><span class="line"><span class="attr">    template:</span> <span class="string">src=/srv/httpd.j2</span> <span class="string">dest=/etc/httpd.conf</span></span><br><span class="line"><span class="attr">    notify:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">restart</span> <span class="string">apache</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ensure</span> <span class="string">apache</span> <span class="string">is</span> <span class="string">running</span></span><br><span class="line"><span class="attr">    service:</span> <span class="string">name=httpd</span> <span class="string">state=started</span></span><br><span class="line"><span class="attr">  handlers:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">restart</span> <span class="string">apache</span></span><br><span class="line"><span class="attr">      service:</span> <span class="string">name=httpd</span> <span class="string">state=restarted</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ansible 是一个开源的基于 OpenSSH 的自动化配置管理工具。可以用它来配置系统、部署软件和编排更高级的 IT 任务，比如持续部署或零停机更新。管理员可以通过 Ansible 在成百上千台计算机上同时执行指令(任务)，这是&lt;a href=&quot;https://ansible-tran.readthedocs.io/en/latest/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方文档&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Tools" scheme="http://yoursite.com/categories/Tools/"/>
    
    
      <category term="运维" scheme="http://yoursite.com/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>大数据bug记录</title>
    <link href="http://yoursite.com/2019/12/20/%E5%A4%A7%E6%95%B0%E6%8D%AEbug%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2019/12/20/大数据bug记录/</id>
    <published>2019-12-20T03:26:05.000Z</published>
    <updated>2019-12-20T06:38:10.753Z</updated>
    
    <content type="html"><![CDATA[<p>记录遇到的简单bug</p><a id="more"></a><h1>201911-201912</h1><h2 id="集群时间不同步"><a class="header-anchor" href="#集群时间不同步">¶</a>集群时间不同步</h2><p><strong>错误详情</strong></p><p>使用 spark 时，yarn 启动节点时报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Application application_1576029485466_0001 failed 2 times due to Error launching </span><br><span class="line">appattempt_1576029485466_0001_000002. Got exception: org.apache.hadoop.yarn.exceptions.YarnException: </span><br><span class="line">Unauthorized request to start container.</span><br><span class="line">This token is expired. current time is 1576059076270 found 1576030868681</span><br></pre></td></tr></table></figure><p><strong>解决办法</strong></p><p>让时间同步</p><h2 id="unable-to-create-new-native-thread"><a class="header-anchor" href="#unable-to-create-new-native-thread">¶</a>unable to create new native thread</h2><p><strong>错误详情</strong></p><p>使用 spark 时，程序报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">2019-12-11 11:34:58,366 INFO scheduler.DAGScheduler: ResultStage 0 (collect at SparkSharder.java:430) failed in 80.679 s due to Job aborted due to stage failure: Task 5428 in stage 0.0 failed 4 times, most recent failure: Lost task 5428.3 in stage 0.0 (TID 5440, dell-r730-4, executor 3): java.io.IOException: DestHost:destPort dell-r720:8020 , LocalHost:localPort dell-r730-4/10.0.0.14:0. Failed on local exception: java.io.IOException: Couldn&apos;t set up IO streams: java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)</span><br><span class="line">at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:808)</span><br><span class="line">at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)</span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1491)</span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1388)</span><br><span class="line">at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)</span><br><span class="line">at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)</span><br><span class="line">at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)</span><br><span class="line">at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:907)</span><br><span class="line">at sun.reflect.GeneratedMethodAccessor307.invoke(Unknown Source)</span><br></pre></td></tr></table></figure><p><strong>解决办法</strong></p><p>原因是超过了<code>unlimt -u</code>设定的最大线程数，把它增大即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改 limits.conf</span></span><br><span class="line">$ vim /etc/security/limits.conf</span><br><span class="line">...</span><br><span class="line">*       soft    nofile  65536      <span class="comment"># 文件打开数（以前改的）</span></span><br><span class="line">*       hard    nofile  65536</span><br><span class="line"></span><br><span class="line">*       hard    nproc     65536    <span class="comment"># 线程数（加上这两行）</span></span><br><span class="line">*       soft    nproc     65536</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果没有效果，则继续修改 20-nproc.conf（它限制了线程最大值）</span></span><br><span class="line">$ vim /etc/security/limits.d/20-nproc.conf</span><br><span class="line"><span class="comment"># *       soft    nproc     65536  # 修改它</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录遇到的简单bug&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="bug" scheme="http://yoursite.com/tags/bug/"/>
    
  </entry>
  
  <entry>
    <title>spark调试-常用代码</title>
    <link href="http://yoursite.com/2019/12/20/spark%E8%B0%83%E8%AF%95-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2019/12/20/spark调试-常用代码/</id>
    <published>2019-12-20T03:13:34.000Z</published>
    <updated>2019-12-20T03:16:15.473Z</updated>
    
    <content type="html"><![CDATA[<p>一些我用到的调试代码</p><a id="more"></a><h1>统计每个分区的record个数</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; numOfRecordPerPartition = javaRDD.mapPartitionsWithIndex(</span><br><span class="line">        ((Function2&lt;Integer, Iterator&lt;T&gt;, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt;) (Index, iterator) -&gt; &#123;</span><br><span class="line">            List&lt;Tuple2&lt;Integer, Integer&gt;&gt; numOfRecord = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">int</span> totalElement = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">                iterator.next();</span><br><span class="line">                totalElement++;</span><br><span class="line">            &#125;</span><br><span class="line">            numOfRecord.add(<span class="keyword">new</span> Tuple2&lt;&gt;(Index, totalElement));</span><br><span class="line">            <span class="keyword">return</span> numOfRecord.iterator();</span><br><span class="line">        &#125;), <span class="keyword">true</span>).collect();</span><br></pre></td></tr></table></figure><h1>收集每个分区的第一个record</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;Integer, T&gt;&gt; firstRecordPerPartition = javaRDD.mapPartitionsWithIndex(</span><br><span class="line">        ((Function2&lt;Integer, Iterator&lt;T&gt;, Iterator&lt;Tuple2&lt;Integer, T&gt;&gt;&gt;) (Index, iterator) -&gt; &#123;</span><br><span class="line">            List&lt;Tuple2&lt;Integer, T&gt;&gt; record = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">if</span> (iterator.hasNext())</span><br><span class="line">                record.add(<span class="keyword">new</span> Tuple2&lt;&gt;(Index, iterator.next()));</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                record.add(<span class="keyword">null</span>);</span><br><span class="line">            <span class="keyword">return</span> record.iterator();</span><br><span class="line">        &#125;), <span class="keyword">true</span>).collect();</span><br></pre></td></tr></table></figure><h1>查看当前RDD的PreferredLocations</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Partition[] partitions = javaRDD.rdd().getPartitions();</span><br><span class="line">List&lt;Seq&lt;String&gt;&gt; preferredLocationsList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (Partition partition : partitions) &#123;</span><br><span class="line">    preferredLocationsList.add(javaRDD.rdd().getPreferredLocations(partition));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一些我用到的调试代码&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>linux-常用命令</title>
    <link href="http://yoursite.com/2019/12/20/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>http://yoursite.com/2019/12/20/linux常用命令/</id>
    <published>2019-12-20T02:16:07.000Z</published>
    <updated>2020-01-02T11:46:26.814Z</updated>
    
    <content type="html"><![CDATA[<p>一些用过的linux命令</p><a id="more"></a><h1>文件</h1><ol><li><strong>查看目录内子目录所使用的空间</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ du -h  --max-depth=1`</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>改变文件（夹）拥有者</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ chown (-R) zxy filename</span><br></pre></td></tr></table></figure><ol start="3"><li><strong>改变文件权限</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ chmod 755 filename</span><br></pre></td></tr></table></figure><ol start="4"><li><strong>压缩与解压</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tar 压缩，-p 保留权限，-z 使用 gzip</span></span><br><span class="line">$ tar cvpfz /mnt/data/homeback.tgz /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># tar 解压</span></span><br><span class="line">$ tar xvpfz /mnt/data/homeback.tgz -C /</span><br></pre></td></tr></table></figure><ol start="5"><li><strong>查看磁盘使用情况</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ df -h</span><br></pre></td></tr></table></figure><h1>vim</h1><table><thead><tr><th>命令</th><th>功能</th></tr></thead><tbody><tr><td>gg</td><td>跳转到文件头</td></tr><tr><td>Shift+g</td><td>跳转到文件末尾</td></tr><tr><td>/字符串</td><td>从开头查找，n下一个，N上一个</td></tr><tr><td>?字符串</td><td>从底部查找</td></tr></tbody></table><h1>服务器</h1><ol><li><strong>添加用户</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ adduser username</span><br><span class="line">$ passwd username</span><br><span class="line"><span class="comment"># 赋予root权限</span></span><br><span class="line">$ vim /etc/sudoers</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>免密登陆</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub zxy@10.0.0.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line">$ ssh zxy@10.0.0.1 <span class="string">"cat &gt;&gt; ~/.ssh/authorized_keys"</span> &lt; ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><ol start="3"><li><strong>传输文件</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 源文件 -&gt; 目的地</span></span><br><span class="line">scp -r zxy@10.0.0.1:/home/zxy/filename  /Users/zxy/Desktop/</span><br></pre></td></tr></table></figure><ol start="4"><li><strong>配置网卡ip地址</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (1) 修改对应网卡配置文件（centos）</span></span><br><span class="line">$ sudo vim /etc/sysconfig/network-scripts/ifcfg-em1</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2) 重启生效</span></span><br><span class="line">$ sudo service network restart</span><br><span class="line"></span><br><span class="line"><span class="comment"># (3) 查看效果</span></span><br><span class="line">$ ifconfig</span><br></pre></td></tr></table></figure><ol start="5"><li><strong>使用 nfs 服务器</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解释：</span></span><br><span class="line"><span class="comment"># /mnt/storage2/data                     用于共享的目录</span></span><br><span class="line"><span class="comment"># *                                      客户端：所有主机</span></span><br><span class="line"><span class="comment"># rw                                     可读可写</span></span><br><span class="line"><span class="comment"># sync                                   数据同步，效率低，但可以保证数据的一致性</span></span><br><span class="line"><span class="comment"># no_root_squash                         让root保持权限，也就是让客户端的root相当于服务端的root</span></span><br><span class="line"><span class="comment"># all_squash,anonuid=1001,anongid=1001   客户端写数据时，普通用户名强转成指定名字（1001）</span></span><br><span class="line"><span class="comment"># no_all_squash 默认                      客户端写数据时，保持用户名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (1) 修改 exports </span></span><br><span class="line">$ sudo vim /etc/exports </span><br><span class="line"></span><br><span class="line"><span class="comment"># /mnt/storage2/data  该目录统一用户 1001（cluster集群使用）</span></span><br><span class="line">/mnt/storage2/data *(rw,sync,no_root_squash,all_squash,anonuid=1001,anongid=1001)</span><br><span class="line"><span class="comment"># /mnt/storage2/users 该目录用于共享，保持个人用户名</span></span><br><span class="line">/mnt/storage2/users *(rw,sync,no_root_squash)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2) 服务端 挂载（更新）</span></span><br><span class="line">$ exportfs -arv</span><br><span class="line"></span><br><span class="line"><span class="comment"># (3) 客户端 挂载</span></span><br><span class="line">$ mount -t nfs storage-2:/mnt/storage2/data /home/cluster/Storage2</span><br><span class="line">$ mount -t nfs storage-2:/mnt/storage2/users /mnt/users</span><br><span class="line"></span><br><span class="line"><span class="comment"># (4) 客户端 卸载</span></span><br><span class="line">$ umount /mnt/users</span><br><span class="line"></span><br><span class="line"><span class="comment"># (5) 服务端 卸载</span></span><br><span class="line">$ exportfs -auv</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一些用过的linux命令&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="shell" scheme="http://yoursite.com/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-shuffle之SortShuffleWriter</title>
    <link href="http://yoursite.com/2019/12/04/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BSortShuffleWriter/"/>
    <id>http://yoursite.com/2019/12/04/spark源码-shuffle之SortShuffleWriter/</id>
    <published>2019-12-04T06:36:32.000Z</published>
    <updated>2019-12-20T03:17:51.257Z</updated>
    
    <content type="html"><![CDATA[<p>spark 三大ShuffleWriter 之 SortShuffleWriter</p><a id="more"></a><p>spark 版本 2.4.3</p><h1>特点</h1><ul><li><p>最大特点就是<strong>支持 map-side aggregation</strong></p></li><li><p>最基础的 <code>ShuffleWriter</code>。当另外两种用不了时才选它～</p></li></ul><h1>大致流程</h1><p>首先记住有3种情况：<strong>含 aggregator 和 ordering</strong>；<strong>含 aggregator 但不含 ordering</strong>； <strong>前两种都不含</strong>。</p><p>为啥没有只含ordering的情况呢？因为不含aggregator就不做排序，永远记住<strong>ShuffleWriter阶段的排序只是为了使聚合更舒服</strong>！</p><ol><li><p><strong>选择 <code>sorter</code></strong>：情况1和2 选同一种<code>sorter</code>，情况3选另一种。我把它们称为 分支1 和 分支2</p></li><li><p><strong>读取数据</strong>：分支1把数据读进<code>PartitionedAppendOnlyMap</code>（把同一分区key相同的聚合），分支2把数据读进<code>PartitionedPairBuffer</code>（简单放入）</p></li><li><p><strong>数据数量达到阈值发生spill</strong>：这个spill文件整体是按<strong>分区顺序</strong>堆叠的。不同点是分区内部数据情况：情况1按 ordering 排序；情况2按 key 的 hash值 排序（这个排序只是为了方便聚合）；情况3 不排序</p></li><li><p><strong>合并spill文件和内存中未spill的文件，并返回分区长度数组</strong>：情况1先归并排序再聚合；情况2只聚合；情况3啥都不干</p></li><li><p><strong>根据分区长度数组生成索引文件</strong></p></li><li><p><strong>封装信息到<code>MapStatus</code>返回</strong></p></li></ol><p>总的来说就是一个 <strong>task</strong> 生成一个<strong>由 spill 文件合并形成的且聚合了的大文件</strong>和一个<strong>索引文件</strong>。</p><p>由于情况2更复杂点，以情况2为示例：<br><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/shuffle/SortShuffleWriter.jpg" alt></p><h1>源码</h1><h2 id="write-…"><a class="header-anchor" href="#write-…">¶</a>write(…)</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 流程1：根据是否需要 mapSideCombine 选择不同的 sorter</span></span><br><span class="line">  sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// In this case we pass neither an aggregator nor an ordering to the sorter, because we don't</span></span><br><span class="line">    <span class="comment">// care whether the keys get sorted in each partition; that will be done on the reduce side</span></span><br><span class="line">    <span class="comment">// if the operation being run is sortByKey.</span></span><br><span class="line">    <span class="comment">// 注意 ordering = None，官方解释的很清楚了，对吧</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](</span><br><span class="line">      context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 流程2-3：读取数据 与 spill</span></span><br><span class="line">  sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">  <span class="comment">// 文件名 "shuffle_" + shuffleId + "_" + mapId + "_" + reduceId + ".data"</span></span><br><span class="line">  <span class="keyword">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line">  <span class="keyword">val</span> tmp = <span class="type">Utils</span>.tempFileWith(output)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 流程4: 合并 spill文件 和 内存中未spill的文件，并返回分区长度数组</span></span><br><span class="line">    <span class="keyword">val</span> blockId = <span class="type">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class="type">IndexShuffleBlockResolver</span>.<span class="type">NOOP_REDUCE_ID</span>)</span><br><span class="line">    <span class="keyword">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class="line">    <span class="comment">// 流程5: 根据分区长度数组生成索引文件。这个步骤都是一样的，参考 BypassMergeSortShuffleWriter 篇</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class="line">    <span class="comment">// 流程6: 封装信息到 MapStatus 返回</span></span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logError(<span class="string">s"Error while deleting temp file <span class="subst">$&#123;tmp.getAbsolutePath&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="insertAll-…"><a class="header-anchor" href="#insertAll-…">¶</a>insertAll(…)</h2><p>流程2-3：读取数据 与 spill</p><p>主要关注 <code>PartitionedAppendOnlyMap </code> 和 <code>PartitionedPairBuffer </code></p><ul><li><p>相同点：都实现<code>WritablePartitionedPairCollection</code> trait。它们内部都是用 <code>Array</code> （key0, value0, key1, value1, key2, value2…）实现 Map 逻辑。<strong>key 是 （分区ID，原key）</strong></p></li><li><p>不同点：<code>PartitionedAppendOnlyMap </code><strong>支持添加于更新</strong> value：它使用<code>map.changeValue((getPartition(kv._1), kv._1), update)</code> 完成数据添加或者更新（聚合）。而<code>PartitionedPairBuffer </code><strong>仅支持添加</strong>。</p></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertAll</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> stop combining if we find that the reduction factor isn't high</span></span><br><span class="line">  <span class="keyword">val</span> shouldCombine = aggregator.isDefined</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 选择是否 combine</span></span><br><span class="line">  <span class="keyword">if</span> (shouldCombine) &#123;</span><br><span class="line">    <span class="comment">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class="line">    <span class="keyword">val</span> mergeValue = aggregator.get.mergeValue</span><br><span class="line">    <span class="keyword">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class="line">    <span class="keyword">var</span> kv: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="comment">// 从 aggregator 中取出 createCombiner 和 mergeValue，制作成update函数</span></span><br><span class="line">    <span class="keyword">val</span> update = (hadValue: <span class="type">Boolean</span>, oldValue: <span class="type">C</span>) =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class="keyword">else</span> createCombiner(kv._2)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;</span><br><span class="line">      <span class="comment">// 计数 + 1</span></span><br><span class="line">      addElementsRead()</span><br><span class="line">      kv = records.next()</span><br><span class="line">      <span class="comment">// combine 模式使用 PartitionedAppendOnlyMap</span></span><br><span class="line">      map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class="line">      <span class="comment">// 流程3: spill</span></span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Stick values into our buffer</span></span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;</span><br><span class="line">      addElementsRead()</span><br><span class="line">      <span class="keyword">val</span> kv = records.next()</span><br><span class="line">      <span class="comment">// 非 combine 模式使用 PartitionedPairBuffer</span></span><br><span class="line">      buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class="type">C</span>])</span><br><span class="line">      <span class="comment">// 流程3: spill</span></span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="maybeSpill-…"><a class="header-anchor" href="#maybeSpill-…">¶</a>maybeSpill(…)</h2><p>spill 的条件：内存申请没成功 或者 达到设定的阈值<code>numElementsForceSpillThreshold</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpill</span></span>(collection: <span class="type">C</span>, currentMemory: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> shouldSpill = <span class="literal">false</span></span><br><span class="line">  <span class="comment">// 元素个数是32的整数倍 且 大于 myMemoryThreshold</span></span><br><span class="line">  <span class="keyword">if</span> (elementsRead % <span class="number">32</span> == <span class="number">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class="line">    <span class="comment">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class="line">    <span class="keyword">val</span> amountToRequest = <span class="number">2</span> * currentMemory - myMemoryThreshold</span><br><span class="line">    <span class="comment">// 申请内存</span></span><br><span class="line">    <span class="keyword">val</span> granted = acquireMemory(amountToRequest)</span><br><span class="line">    myMemoryThreshold += granted</span><br><span class="line">    <span class="comment">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class="line">    <span class="comment">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class="line">    shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// spill条件：上面的申请没成功 或者  达到阈值</span></span><br><span class="line">  shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class="line">  <span class="comment">// Actually spill</span></span><br><span class="line">  <span class="keyword">if</span> (shouldSpill) &#123;</span><br><span class="line">    _spillCount += <span class="number">1</span></span><br><span class="line">    logSpillage(currentMemory)</span><br><span class="line">    <span class="comment">// spill 在这里发生</span></span><br><span class="line">    spill(collection)</span><br><span class="line">    _elementsRead = <span class="number">0</span></span><br><span class="line">    _memoryBytesSpilled += currentMemory</span><br><span class="line">    releaseMemory()</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="spill-…"><a class="header-anchor" href="#spill-…">¶</a>spill(…)</h2><p>先排序，后spill。排序方面，由于分支不同，有<strong>两个排序逻辑</strong>。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">protected</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">spill</span></span>(collection: <span class="type">WritablePartitionedPairCollection</span>[<span class="type">K</span>, <span class="type">C</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 排序</span></span><br><span class="line">  <span class="keyword">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class="line">  <span class="comment">// spill</span></span><br><span class="line">  <span class="keyword">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</span><br><span class="line">  spills += spillFile</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>PartitionedAppendOnlyMap </code>的排序逻辑：<strong>2重排序</strong>，先按分区ID排，再对分区内的数据排序（优先按 ordering 排序，否则hash）</p><p>注意这个hash排序，学过java中 == 和 equals 的区别的兄弟应该知道，hashcode 相等 是 两个对象 equals 的<strong>必要条件</strong>。这里只能保证 hashcode 相同的数据在一起，后续聚合时，<strong>还需经过比较后才能聚合</strong>（先打个预防针，后面源码会读到它）。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionKeyComparator</span></span>[<span class="type">K</span>](keyComparator: <span class="type">Comparator</span>[<span class="type">K</span>]): <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(a: (<span class="type">Int</span>, <span class="type">K</span>), b: (<span class="type">Int</span>, <span class="type">K</span>)): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> partitionDiff = a._1 - b._1</span><br><span class="line">      <span class="keyword">if</span> (partitionDiff != <span class="number">0</span>) &#123;</span><br><span class="line">        partitionDiff</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        keyComparator.compare(a._2, b._2)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// keyComparator：有 ordering 用 ordering，否则按 hash 排序。</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">val</span> keyComparator: <span class="type">Comparator</span>[<span class="type">K</span>] = ordering.getOrElse(<span class="keyword">new</span> <span class="type">Comparator</span>[<span class="type">K</span>] &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(a: <span class="type">K</span>, b: <span class="type">K</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> h1 = <span class="keyword">if</span> (a == <span class="literal">null</span>) <span class="number">0</span> <span class="keyword">else</span> a.hashCode()</span><br><span class="line">    <span class="keyword">val</span> h2 = <span class="keyword">if</span> (b == <span class="literal">null</span>) <span class="number">0</span> <span class="keyword">else</span> b.hashCode()</span><br><span class="line">    <span class="keyword">if</span> (h1 &lt; h2) <span class="number">-1</span> <span class="keyword">else</span> <span class="keyword">if</span> (h1 == h2) <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p><code>PartitionedPairBuffer</code>的排序逻辑：仅<strong>比较分区ID</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">partitionedDestructiveSortedIterator</span></span>(keyComparator: <span class="type">Option</span>[<span class="type">Comparator</span>[<span class="type">K</span>]])</span><br><span class="line">  : <span class="type">Iterator</span>[((<span class="type">Int</span>, <span class="type">K</span>), <span class="type">V</span>)] = &#123;</span><br><span class="line">  <span class="keyword">val</span> comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Sorter</span>(<span class="keyword">new</span> <span class="type">KVArraySortDataFormat</span>[(<span class="type">Int</span>, <span class="type">K</span>), <span class="type">AnyRef</span>]).sort(data, <span class="number">0</span>, curSize, comparator)</span><br><span class="line">  iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A comparator for (Int, K) pairs that orders them by only their partition ID.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionComparator</span></span>[<span class="type">K</span>]: <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] = <span class="keyword">new</span> <span class="type">Comparator</span>[(<span class="type">Int</span>, <span class="type">K</span>)] &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(a: (<span class="type">Int</span>, <span class="type">K</span>), b: (<span class="type">Int</span>, <span class="type">K</span>)): <span class="type">Int</span> = &#123;</span><br><span class="line">    a._1 - b._1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="writePartitionedFile-…"><a class="header-anchor" href="#writePartitionedFile-…">¶</a>writePartitionedFile(…)</h2><p>流程4: 合并spill文件和内存中未spill的文件，并返回分区长度数组</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writePartitionedFile</span></span>(</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    outputFile: <span class="type">File</span>): <span class="type">Array</span>[<span class="type">Long</span>] = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Track location of each range in the output file</span></span><br><span class="line">  <span class="keyword">val</span> lengths = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Long</span>](numPartitions)</span><br><span class="line">  <span class="keyword">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class="line">    context.taskMetrics().shuffleWriteMetrics)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 首先知道：collection.destructiveSortedWritablePartitionedIterator(comparator) 用这玩意获取内存中的数据（未被spill），后面多次用到它</span></span><br><span class="line">  <span class="keyword">if</span> (spills.isEmpty) &#123;</span><br><span class="line">    <span class="comment">// Case where we only have in-memory data</span></span><br><span class="line">    <span class="comment">// 只有内存文件，刷进内存即可（当然排序什么的还是要的，和上一步一样的规则）</span></span><br><span class="line">    <span class="keyword">val</span> collection = <span class="keyword">if</span> (aggregator.isDefined) map <span class="keyword">else</span> buffer</span><br><span class="line">    <span class="keyword">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class="line">    <span class="keyword">while</span> (it.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> partitionId = it.nextPartition()</span><br><span class="line">      <span class="keyword">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class="line">        it.writeNext(writer)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">      <span class="comment">// 记录分区长度</span></span><br><span class="line">      lengths(partitionId) = segment.length</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 合并操作在这里：this.partitionedIterator，内部调用 merge()</span></span><br><span class="line">    <span class="keyword">for</span> ((id, elements) &lt;- <span class="keyword">this</span>.partitionedIterator) &#123;</span><br><span class="line">      <span class="keyword">if</span> (elements.hasNext) &#123;</span><br><span class="line">        <span class="keyword">for</span> (elem &lt;- elements) &#123;</span><br><span class="line">          writer.write(elem._1, elem._2)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">        lengths(id) = segment.length</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  writer.close()</span><br><span class="line">  context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class="line">  context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class="line">  context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class="line"></span><br><span class="line">  lengths</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="merge-…"><a class="header-anchor" href="#merge-…">¶</a>merge(…)</h2><p>把 spill文件 和 内存文件 同分区的数据放到一起计算（3种计算情况）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(spills: <span class="type">Seq</span>[<span class="type">SpilledFile</span>], inMemory: <span class="type">Iterator</span>[((<span class="type">Int</span>, <span class="type">K</span>), <span class="type">C</span>)])</span><br><span class="line">    : <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]])] = &#123;</span><br><span class="line">  <span class="keyword">val</span> readers = spills.map(<span class="keyword">new</span> <span class="type">SpillReader</span>(_))</span><br><span class="line">  <span class="keyword">val</span> inMemBuffered = inMemory.buffered</span><br><span class="line">  (<span class="number">0</span> until numPartitions).iterator.map &#123; p =&gt;</span><br><span class="line">    <span class="comment">// 很明显这兄弟函数式编程写的很6</span></span><br><span class="line">    <span class="comment">// 主要就是把 spill文件 和 内存文件 同分区的数据放到一起计算（3种计算情况）。其实和以前的2重循环是一个意思</span></span><br><span class="line">    <span class="keyword">val</span> inMemIterator = <span class="keyword">new</span> <span class="type">IteratorForPartition</span>(p, inMemBuffered)</span><br><span class="line">    <span class="keyword">val</span> iterators = readers.map(_.readNextPartition()) ++ <span class="type">Seq</span>(inMemIterator)</span><br><span class="line">    <span class="keyword">if</span> (aggregator.isDefined) &#123;</span><br><span class="line">      <span class="comment">// Perform partial aggregation across partitions</span></span><br><span class="line">      <span class="comment">// 聚合：（分有无 ordering 两种情况）</span></span><br><span class="line">      (p, mergeWithAggregation(</span><br><span class="line">        iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ordering.isDefined) &#123;</span><br><span class="line">      <span class="comment">// No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);</span></span><br><span class="line">      <span class="comment">// sort the elements without trying to merge them</span></span><br><span class="line">      <span class="comment">// 只排序：对它们进行归并排序。</span></span><br><span class="line">      <span class="comment">// 说实话，我不觉得它会进这一分支。因为我多次强调过没有 aggregator 就必定没有 ordering</span></span><br><span class="line">      (p, mergeSort(iterators, ordering.get))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 啥都不要，直接把同分区文件 flatten</span></span><br><span class="line">      (p, iterators.iterator.flatten)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="mergeWithAggregation-…"><a class="header-anchor" href="#mergeWithAggregation-…">¶</a>mergeWithAggregation(…)</h2><p>聚合：（分有无 ordering 两种情况）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">mergeWithAggregation</span></span>(</span><br><span class="line">    iterators: <span class="type">Seq</span>[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]],</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    comparator: <span class="type">Comparator</span>[<span class="type">K</span>],</span><br><span class="line">    totalOrder: <span class="type">Boolean</span>)</span><br><span class="line">    : <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] =</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span> (!totalOrder) &#123;</span><br><span class="line">    <span class="comment">// We only have a partial ordering, e.g. comparing the keys by hash code, which means that</span></span><br><span class="line">    <span class="comment">// multiple distinct keys might be treated as equal by the ordering. To deal with this, we</span></span><br><span class="line">    <span class="comment">// need to read all keys considered equal by the ordering at once and compare them.</span></span><br><span class="line">    <span class="comment">// 无 ordering ，comparator 是 hash比较器。hash值相同的是key相同的必要条件</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]] &#123;</span><br><span class="line">      <span class="keyword">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Buffers reused across elements to decrease memory allocation</span></span><br><span class="line">      <span class="keyword">val</span> keys = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">K</span>]</span><br><span class="line">      <span class="keyword">val</span> combiners = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">C</span>]</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = sorted.hasNext</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = &#123;</span><br><span class="line">        <span class="keyword">if</span> (!hasNext) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoSuchElementException</span></span><br><span class="line">        &#125;</span><br><span class="line">        keys.clear()</span><br><span class="line">        combiners.clear()</span><br><span class="line">        <span class="keyword">val</span> firstPair = sorted.next()</span><br><span class="line">        keys += firstPair._1</span><br><span class="line">        combiners += firstPair._2</span><br><span class="line">        <span class="keyword">val</span> key = firstPair._1</span><br><span class="line">        <span class="comment">// hash值相等</span></span><br><span class="line">        <span class="keyword">while</span> (sorted.hasNext &amp;&amp; comparator.compare(sorted.head._1, key) == <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">val</span> pair = sorted.next()</span><br><span class="line">          <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">          <span class="keyword">var</span> foundKey = <span class="literal">false</span></span><br><span class="line">          <span class="keyword">while</span> (i &lt; keys.size &amp;&amp; !foundKey) &#123;</span><br><span class="line">            <span class="comment">// 注意 == 。 scala 就是用 == 比较对象相等的 </span></span><br><span class="line">            <span class="keyword">if</span> (keys(i) == pair._1) &#123;</span><br><span class="line">              <span class="comment">// key 相等 就合并</span></span><br><span class="line">              combiners(i) = mergeCombiners(combiners(i), pair._2)</span><br><span class="line">              foundKey = <span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (!foundKey) &#123;</span><br><span class="line">            keys += pair._1</span><br><span class="line">            combiners += pair._2</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Note that we return an iterator of elements since we could've had many keys marked</span></span><br><span class="line">        <span class="comment">// equal by the partial order; we flatten this below to get a flat iterator of (K, C).</span></span><br><span class="line">        keys.iterator.zip(combiners.iterator)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.flatMap(i =&gt; i)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// We have a total ordering, so the objects with the same key are sequential.</span></span><br><span class="line">    <span class="comment">// 有 ordering：先归并排序，再把有相同的key的元素聚合就行了</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] &#123;</span><br><span class="line">      <span class="comment">// 归并排序</span></span><br><span class="line">      <span class="keyword">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = sorted.hasNext</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">        <span class="keyword">if</span> (!hasNext) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoSuchElementException</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> elem = sorted.next()</span><br><span class="line">        <span class="keyword">val</span> k = elem._1</span><br><span class="line">        <span class="keyword">var</span> c = elem._2</span><br><span class="line">        <span class="comment">// key 相等 就合并</span></span><br><span class="line">        <span class="keyword">while</span> (sorted.hasNext &amp;&amp; sorted.head._1 == k) &#123;</span><br><span class="line">          <span class="keyword">val</span> pair = sorted.next()</span><br><span class="line">          c = mergeCombiners(c, pair._2)</span><br><span class="line">        &#125;</span><br><span class="line">        (k, c)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark 三大ShuffleWriter 之 SortShuffleWriter&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-shuffle之UnsafeShuffleWriter</title>
    <link href="http://yoursite.com/2019/12/02/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BUnsafeShuffleWriter/"/>
    <id>http://yoursite.com/2019/12/02/spark源码-shuffle之UnsafeShuffleWriter/</id>
    <published>2019-12-02T01:25:06.000Z</published>
    <updated>2019-12-20T03:17:57.715Z</updated>
    
    <content type="html"><![CDATA[<p>spark 三大ShuffleWriter 之 UnsafeShuffleWriter</p><a id="more"></a><p>spark 版本 2.4.3</p><h1>特点</h1><ul><li><p>它<strong>只适用不需要 map-side aggregation</strong> 的Shuffle操作</p></li><li><p>它使用UnSafe API操作序列化数据，而不是Java对象，减少了内存占用及因此导致的GC耗时(参考Spark 内存管理之Tungsten)，因此使用它时需要<strong>Serializer支持relocation</strong>。</p></li><li><p><strong>reduce端的分区数目小于等于 2^24</strong> (因为排序过程中需要使用是数据指针，它记录了数据的地址和分区ID，其中分区ID占24位)</p></li><li><p>溢写 &amp; 合并时使用UnSafe API直接操作序列化数据，<strong>合并时不需要反序列化数据</strong>。</p></li><li><p>溢写 &amp; 合并时<strong>可以使用fastMerge提升效率</strong>(调用NIO的transferTo方法)</p></li></ul><h1>大致流程</h1><ol><li><p>遍历数据，插入到<code>ShuffleExternalSorter</code>中。该过程完成许多事：<strong>将数据读入内存</strong> 同时 <strong>将数据指针不断写到指针数组</strong>中，当达到spill阈值，使用<code>writeSortedFile()</code>排序（排序排的是指针数组）并spill到磁盘。</p></li><li><p><code>sorter.closeAndGetSpills()</code>：spill 内存中剩余的文件，返回所有 spill 文件的元信息（重点是每个分区的长度）</p></li><li><p>合并所有 spill 文件成一个大文件（有3种合并工具选择），并返回分区长度数组</p></li><li><p>根据分区长度数组生成索引文件</p></li><li><p>封装信息到<code>MapStatus</code>返回</p></li></ol><p>总的来说就是一个 <strong>task</strong> 生成一个<strong>由 spill 文件合并形成的大文件（这玩意只是按分区排好，内部是无序的）<strong>和一个</strong>索引文件</strong>（上面流程是主要流程，重命名什么的就不写了）。<br><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/shuffle/UnsafeShuffleWriter.jpg" alt></p><h1>源码</h1><h2 id="write-…"><a class="header-anchor" href="#write-…">¶</a>write(…)</h2><p>这个<code>write(...)</code>方法就比较清爽了，因为步骤都封在方法里了</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">public void write(scala.collection.<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  boolean success = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">      <span class="comment">// 对应流程1</span></span><br><span class="line">      insertRecordIntoSorter(records.next());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 剩余流程</span></span><br><span class="line">    closeAndWriteOutput();</span><br><span class="line">    success = <span class="literal">true</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (sorter != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        sorter.cleanupResources();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (<span class="type">Exception</span> e) &#123;</span><br><span class="line">        <span class="comment">// Only throw this error if we won't be masking another</span></span><br><span class="line">        <span class="comment">// error.</span></span><br><span class="line">        <span class="keyword">if</span> (success) &#123;</span><br><span class="line">          <span class="keyword">throw</span> e;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          logger.error(<span class="string">"In addition to a failure during writing, we failed during "</span> +</span><br><span class="line">                       <span class="string">"cleanup."</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="insertRecordIntoSorter-…"><a class="header-anchor" href="#insertRecordIntoSorter-…">¶</a>insertRecordIntoSorter(…)</h2><p>其核心方法是<code>sorter.insertRecord()</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">void insertRecordIntoSorter(<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert(sorter != <span class="literal">null</span>);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">K</span> key = record._1();</span><br><span class="line">  <span class="keyword">final</span> int partitionId = partitioner.getPartition(key);</span><br><span class="line">  serBuffer.reset();</span><br><span class="line">  <span class="comment">// serOutputStream: 用于序列化对象的写入的流</span></span><br><span class="line">  serOutputStream.writeKey(key, <span class="type">OBJECT_CLASS_TAG</span>);</span><br><span class="line">  serOutputStream.writeValue(record._2(), <span class="type">OBJECT_CLASS_TAG</span>);</span><br><span class="line">  serOutputStream.flush();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> int serializedRecordSize = serBuffer.size();</span><br><span class="line">  assert (serializedRecordSize &gt; <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 都是它干的</span></span><br><span class="line">  sorter.insertRecord(</span><br><span class="line">    serBuffer.getBuf(), <span class="type">Platform</span>.<span class="type">BYTE_ARRAY_OFFSET</span>, serializedRecordSize, partitionId);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="sorter-insertRecord-…"><a class="header-anchor" href="#sorter-insertRecord-…">¶</a>sorter.insertRecord(…)</h2><p><strong>将数据读入内存</strong> 同时 <strong>将数据指针不断写到指针数组</strong>中，<strong>当达到spill阈值，发生 spill</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">public void insertRecord(<span class="type">Object</span> recordBase, long recordOffset, int length, int partitionId)</span><br><span class="line">  <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// for tests</span></span><br><span class="line">  assert(inMemSorter != <span class="literal">null</span>);</span><br><span class="line">  <span class="comment">// 如果 Sorter 内的数据超过阈值，就发生 spill</span></span><br><span class="line">  <span class="keyword">if</span> (inMemSorter.numRecords() &gt;= numElementsForSpillThreshold) &#123;</span><br><span class="line">    logger.info(<span class="string">"Spilling data because number of spilledRecords crossed the threshold "</span> +</span><br><span class="line">      numElementsForSpillThreshold);</span><br><span class="line">    spill();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 检查 inMemSorter中 的数组是否满了，如果满了就扩容</span></span><br><span class="line">  growPointerArrayIfNecessary();</span><br><span class="line">  <span class="keyword">final</span> int uaoSize = <span class="type">UnsafeAlignedOffset</span>.getUaoSize();</span><br><span class="line">  <span class="comment">// Need 4 or 8 bytes to store the record length.</span></span><br><span class="line">  <span class="keyword">final</span> int required = length + uaoSize;</span><br><span class="line">  acquireNewPageIfNecessary(required);</span><br><span class="line"></span><br><span class="line">  assert(currentPage != <span class="literal">null</span>);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">Object</span> base = currentPage.getBaseObject();</span><br><span class="line">  <span class="keyword">final</span> long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);</span><br><span class="line">  <span class="type">UnsafeAlignedOffset</span>.putSize(base, pageCursor, length);</span><br><span class="line">  pageCursor += uaoSize;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 将数据写入 MemoryBlock 中</span></span><br><span class="line">  <span class="type">Platform</span>.copyMemory(recordBase, recordOffset, base, pageCursor, length);</span><br><span class="line">  pageCursor += length;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 将数据指针信息写入inMemSorter的数组中</span></span><br><span class="line">  inMemSorter.insertRecord(recordAddress, partitionId);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="spill"><a class="header-anchor" href="#spill">¶</a>spill()</h2><p><code>spill()</code> 的核心是<code> writeSortedFile(false)</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public long spill(long size, <span class="type">MemoryConsumer</span> trigger) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (trigger != <span class="keyword">this</span> || inMemSorter == <span class="literal">null</span> || inMemSorter.numRecords() == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>L;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  logger.info(<span class="string">"Thread &#123;&#125; spilling sort data of &#123;&#125; to disk (&#123;&#125; &#123;&#125; so far)"</span>,</span><br><span class="line">    <span class="type">Thread</span>.currentThread().getId(),</span><br><span class="line">    <span class="type">Utils</span>.bytesToString(getMemoryUsage()),</span><br><span class="line">    spills.size(),</span><br><span class="line">    spills.size() &gt; <span class="number">1</span> ? <span class="string">" times"</span> : <span class="string">" time"</span>);</span><br><span class="line"></span><br><span class="line">  writeSortedFile(<span class="literal">false</span>);</span><br><span class="line">  <span class="keyword">final</span> long spillSize = freeMemory();</span><br><span class="line">  <span class="comment">// spill 完整。重置 inMemSorter</span></span><br><span class="line">  inMemSorter.reset();</span><br><span class="line">  <span class="comment">// Reset the in-memory sorter's pointer array only after freeing up the memory pages holding the</span></span><br><span class="line">  <span class="comment">// records. Otherwise, if the task is over allocated memory, then without freeing the memory</span></span><br><span class="line">  <span class="comment">// pages, we might not be able to get memory for the pointer array.</span></span><br><span class="line">  taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);</span><br><span class="line">  <span class="keyword">return</span> spillSize;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="writeSortedFile"><a class="header-anchor" href="#writeSortedFile">¶</a>writeSortedFile()</h2><p>对内存中的数据进行排序（排序排的是指针数组）并 写到磁盘</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> void writeSortedFile(boolean isLastFile) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">ShuffleWriteMetrics</span> writeMetricsToUse;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (isLastFile) &#123;</span><br><span class="line">    writeMetricsToUse = writeMetrics;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    writeMetricsToUse = <span class="keyword">new</span> <span class="type">ShuffleWriteMetrics</span>();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// This call performs the actual sort.</span></span><br><span class="line">  <span class="comment">// 迭代器：包含分区有序的数据指针（排序在这里进行，有两种排序手段）</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">ShuffleInMemorySorter</span>.<span class="type">ShuffleSorterIterator</span> sortedRecords =</span><br><span class="line">    inMemSorter.getSortedIterator();</span><br><span class="line">  <span class="keyword">final</span> byte[] writeBuffer = <span class="keyword">new</span> byte[diskWriteBufferSize];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 文件名：temp_shuffle_ + block_id</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">Tuple2</span>&lt;<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>&gt; spilledFileInfo =</span><br><span class="line">    blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> file = spilledFileInfo._2();</span><br><span class="line">  <span class="keyword">final</span> <span class="type">TempShuffleBlockId</span> blockId = spilledFileInfo._1();</span><br><span class="line">  <span class="keyword">final</span> <span class="type">SpillInfo</span> spillInfo = <span class="keyword">new</span> <span class="type">SpillInfo</span>(numPartitions, file, blockId);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">SerializerInstance</span> ser = <span class="type">DummySerializerInstance</span>.<span class="type">INSTANCE</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">DiskBlockObjectWriter</span> writer =</span><br><span class="line">    blockManager.getDiskWriter(blockId, file, ser, fileBufferSizeBytes, writeMetricsToUse);</span><br><span class="line"></span><br><span class="line">  int currentPartition = <span class="number">-1</span>;</span><br><span class="line">  <span class="keyword">final</span> int uaoSize = <span class="type">UnsafeAlignedOffset</span>.getUaoSize();</span><br><span class="line">  <span class="keyword">while</span> (sortedRecords.hasNext()) &#123;</span><br><span class="line">    sortedRecords.loadNext();</span><br><span class="line">    <span class="keyword">final</span> int partition = sortedRecords.packedRecordPointer.getPartitionId();</span><br><span class="line">    assert (partition &gt;= currentPartition);</span><br><span class="line">    <span class="keyword">if</span> (partition != currentPartition) &#123;</span><br><span class="line">      <span class="comment">// Switch to the new partition</span></span><br><span class="line">      <span class="keyword">if</span> (currentPartition != <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileSegment</span> fileSegment = writer.commitAndGet();</span><br><span class="line">        <span class="comment">// 记录每个分区的长度</span></span><br><span class="line">        spillInfo.partitionLengths[currentPartition] = fileSegment.length();</span><br><span class="line">      &#125;</span><br><span class="line">      currentPartition = partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Object</span> recordPage = taskMemoryManager.getPage(recordPointer);</span><br><span class="line">    <span class="keyword">final</span> long recordOffsetInPage = taskMemoryManager.getOffsetInPage(recordPointer);</span><br><span class="line">    int dataRemaining = <span class="type">UnsafeAlignedOffset</span>.getSize(recordPage, recordOffsetInPage);</span><br><span class="line">    long recordReadPosition = recordOffsetInPage + uaoSize; <span class="comment">// skip over record length</span></span><br><span class="line">    <span class="keyword">while</span> (dataRemaining &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">final</span> int toTransfer = <span class="type">Math</span>.min(diskWriteBufferSize, dataRemaining);</span><br><span class="line">      <span class="type">Platform</span>.copyMemory(</span><br><span class="line">        recordPage, recordReadPosition, writeBuffer, <span class="type">Platform</span>.<span class="type">BYTE_ARRAY_OFFSET</span>, toTransfer);</span><br><span class="line">      writer.write(writeBuffer, <span class="number">0</span>, toTransfer);</span><br><span class="line">      recordReadPosition += toTransfer;</span><br><span class="line">      dataRemaining -= toTransfer;</span><br><span class="line">    &#125;</span><br><span class="line">    writer.recordWritten();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="type">FileSegment</span> committedSegment = writer.commitAndGet();</span><br><span class="line">  writer.close();</span><br><span class="line">  <span class="keyword">if</span> (currentPartition != <span class="number">-1</span>) &#123;</span><br><span class="line">    spillInfo.partitionLengths[currentPartition] = committedSegment.length();</span><br><span class="line">    spills.add(spillInfo);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!isLastFile) &#123;  <span class="comment">// i.e. this is a spill file</span></span><br><span class="line">    writeMetrics.incRecordsWritten(writeMetricsToUse.recordsWritten());</span><br><span class="line">    taskContext.taskMetrics().incDiskBytesSpilled(writeMetricsToUse.bytesWritten());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="closeAndWriteOutput"><a class="header-anchor" href="#closeAndWriteOutput">¶</a>closeAndWriteOutput();</h2><p>终于完成了流程1: <code>insertRecordIntoSorter</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">void closeAndWriteOutput() <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert(sorter != <span class="literal">null</span>);</span><br><span class="line">  updatePeakMemoryUsed();</span><br><span class="line">  serBuffer = <span class="literal">null</span>;</span><br><span class="line">  serOutputStream = <span class="literal">null</span>;</span><br><span class="line">  <span class="comment">// 流程2: spill 内存中剩余的文件，返回所有 spill 文件的元信息（重点是每个分区的长度）</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">SpillInfo</span>[] spills = sorter.closeAndGetSpills();</span><br><span class="line">  sorter = <span class="literal">null</span>;</span><br><span class="line">  <span class="keyword">final</span> long[] partitionLengths;</span><br><span class="line">  <span class="comment">// 文件名："shuffle_" + shuffleId + "_" + mapId + "_" + reduceId + ".data"</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> tmp = <span class="type">Utils</span>.tempFileWith(output);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 流程3: 合并 spill 文件，并返回 spill 文件的大小，用于计算索引文件</span></span><br><span class="line">      partitionLengths = mergeSpills(spills, tmp);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">SpillInfo</span> spill : spills) &#123;</span><br><span class="line">        <span class="keyword">if</span> (spill.file.exists() &amp;&amp; ! spill.file.delete()) &#123;</span><br><span class="line">          logger.error(<span class="string">"Error while deleting spill file &#123;&#125;"</span>, spill.file.getPath());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 流程4: 生成索引文件。这个步骤都是一样的，参考 BypassMergeSortShuffleWriter</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logger.error(<span class="string">"Error while deleting temp file &#123;&#125;"</span>, tmp.getAbsolutePath());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  流程<span class="number">5</span>: 封装信息到`<span class="type">MapStatus</span>`返回</span><br><span class="line">  mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="mergeSpills-…"><a class="header-anchor" href="#mergeSpills-…">¶</a>mergeSpills(…)</h3><p>合并 spill 文件，有3种合并手段：</p><ul><li><p>快合并：<strong>不使用压缩，或者特定的支持拼接的压缩格式</strong>：Snappy、LZF、LZ4、ZStd</p><ol><li><p>当使用nio的<code>transferTo</code>传输 且 不需要加密时，使用 <code>mergeSpillsWithTransferTo(spills, outputFile)</code></p></li><li><p>否则使用<code>mergeSpillsWithFileStream(spills, outputFile, null)</code></p></li></ol></li><li><p>慢合并：</p><ol><li>使用<code>mergeSpillsWithFileStream(spills, outputFile, compressionCodec)</code></li></ol></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> long[] mergeSpills(<span class="type">SpillInfo</span>[] spills, <span class="type">File</span> outputFile) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  <span class="comment">// 压缩，以及压缩格式</span></span><br><span class="line">  <span class="keyword">final</span> boolean compressionEnabled = sparkConf.getBoolean(<span class="string">"spark.shuffle.compress"</span>, <span class="literal">true</span>);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">CompressionCodec</span> compressionCodec = <span class="type">CompressionCodec</span>$.<span class="type">MODULE</span>$.createCodec(sparkConf);</span><br><span class="line">  <span class="keyword">final</span> boolean fastMergeEnabled =</span><br><span class="line">    sparkConf.getBoolean(<span class="string">"spark.shuffle.unsafe.fastMergeEnabled"</span>, <span class="literal">true</span>);</span><br><span class="line">  <span class="comment">// 支持快速合并的情况：不使用压缩，或者特定的支持拼接的压缩格式：Snappy、LZF、LZ4、ZStd</span></span><br><span class="line">  <span class="keyword">final</span> boolean fastMergeIsSupported = !compressionEnabled ||</span><br><span class="line">    <span class="type">CompressionCodec</span>$.<span class="type">MODULE</span>$.supportsConcatenationOfSerializedStreams(compressionCodec);</span><br><span class="line">  <span class="comment">// 是否加密</span></span><br><span class="line">  <span class="keyword">final</span> boolean encryptionEnabled = blockManager.serializerManager().encryptionEnabled();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (spills.length == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">FileOutputStream</span>(outputFile).close(); <span class="comment">// Create an empty file</span></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> long[partitioner.numPartitions()];</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (spills.length == <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="type">Files</span>.move(spills[<span class="number">0</span>].file, outputFile);</span><br><span class="line">      <span class="keyword">return</span> spills[<span class="number">0</span>].partitionLengths;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">final</span> long[] partitionLengths;</span><br><span class="line">      <span class="keyword">if</span> (fastMergeEnabled &amp;&amp; fastMergeIsSupported) &#123;</span><br><span class="line">        <span class="comment">// 当使用nio的transferTo传输 且 不需要加密时，使用 transferTo-based fast merge</span></span><br><span class="line">        <span class="keyword">if</span> (transferToEnabled &amp;&amp; !encryptionEnabled) &#123;</span><br><span class="line">          logger.debug(<span class="string">"Using transferTo-based fast merge"</span>);</span><br><span class="line">          partitionLengths = mergeSpillsWithTransferTo(spills, outputFile);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 否则使用 fileStream-based fast merge</span></span><br><span class="line">          logger.debug(<span class="string">"Using fileStream-based fast merge"</span>);</span><br><span class="line">          partitionLengths = mergeSpillsWithFileStream(spills, outputFile, <span class="literal">null</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logger.debug(<span class="string">"Using slow merge"</span>);</span><br><span class="line">        <span class="comment">// 使用 slow merge</span></span><br><span class="line">        partitionLengths = mergeSpillsWithFileStream(spills, outputFile, compressionCodec);</span><br><span class="line">      &#125;</span><br><span class="line">      writeMetrics.decBytesWritten(spills[spills.length - <span class="number">1</span>].file.length());</span><br><span class="line">      writeMetrics.incBytesWritten(outputFile.length());</span><br><span class="line">      <span class="keyword">return</span> partitionLengths;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (<span class="type">IOException</span> e) &#123;</span><br><span class="line">    <span class="keyword">if</span> (outputFile.exists() &amp;&amp; !outputFile.delete()) &#123;</span><br><span class="line">      logger.error(<span class="string">"Unable to delete output file &#123;&#125;"</span>, outputFile.getPath());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="mergeSpillsWithTransferTo-…"><a class="header-anchor" href="#mergeSpillsWithTransferTo-…">¶</a>mergeSpillsWithTransferTo(…)</h3><p>由于内部流程都差不多，就举一个为例，核心是个<strong>2重循环</strong>，将所有spill文件中同一分区的数据合并，并按分区号排列</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> long[] mergeSpillsWithTransferTo(<span class="type">SpillInfo</span>[] spills, <span class="type">File</span> outputFile) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert (spills.length &gt;= <span class="number">2</span>);</span><br><span class="line">  <span class="keyword">final</span> int numPartitions = partitioner.numPartitions();</span><br><span class="line">  <span class="keyword">final</span> long[] partitionLengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">  <span class="keyword">final</span> <span class="type">FileChannel</span>[] spillInputChannels = <span class="keyword">new</span> <span class="type">FileChannel</span>[spills.length];</span><br><span class="line">  <span class="keyword">final</span> long[] spillInputChannelPositions = <span class="keyword">new</span> long[spills.length];</span><br><span class="line">  <span class="type">FileChannel</span> mergedFileOutputChannel = <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">  boolean threwException = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 对每个 spill 文件产出输入流</span></span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; spills.length; i++) &#123;</span><br><span class="line">      spillInputChannels[i] = <span class="keyword">new</span> <span class="type">FileInputStream</span>(spills[i].file).getChannel();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 合并文件 的输出流</span></span><br><span class="line">    mergedFileOutputChannel = <span class="keyword">new</span> <span class="type">FileOutputStream</span>(outputFile, <span class="literal">true</span>).getChannel();</span><br><span class="line"></span><br><span class="line">    long bytesWrittenToMergedFile = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 2重循环，结果是将所有spill文件中同一分区的数据合并，并按分区号排列</span></span><br><span class="line">    <span class="keyword">for</span> (int partition = <span class="number">0</span>; partition &lt; numPartitions; partition++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; spills.length; i++) &#123;</span><br><span class="line">        <span class="keyword">final</span> long partitionLengthInSpill = spills[i].partitionLengths[partition];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileChannel</span> spillInputChannel = spillInputChannels[i];</span><br><span class="line">        <span class="keyword">final</span> long writeStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">        <span class="comment">// 将 spill 里的 指定分区数据 写入合并文件中</span></span><br><span class="line">        <span class="type">Utils</span>.copyFileStreamNIO(</span><br><span class="line">          spillInputChannel,             <span class="comment">// spill 文件输入流</span></span><br><span class="line">          mergedFileOutputChannel,       <span class="comment">// 合并文件输出流</span></span><br><span class="line">          spillInputChannelPositions[i], <span class="comment">// spill 中 该分区起始位置</span></span><br><span class="line">          partitionLengthInSpill);       <span class="comment">// spill 中 该分区长度</span></span><br><span class="line">        spillInputChannelPositions[i] += partitionLengthInSpill;</span><br><span class="line">        writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - writeStartTime);</span><br><span class="line">        bytesWrittenToMergedFile += partitionLengthInSpill;</span><br><span class="line">        partitionLengths[partition] += partitionLengthInSpill; <span class="comment">// 所有 spill 中该分区的总长度</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (mergedFileOutputChannel.position() != bytesWrittenToMergedFile) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(</span><br><span class="line">        <span class="string">"Current position "</span> + mergedFileOutputChannel.position() + <span class="string">" does not equal expected "</span> +</span><br><span class="line">          <span class="string">"position "</span> + bytesWrittenToMergedFile + <span class="string">" after transferTo. Please check your kernel"</span> +</span><br><span class="line">          <span class="string">" version to see if it is 2.6.32, as there is a kernel bug which will lead to "</span> +</span><br><span class="line">          <span class="string">"unexpected behavior when using transferTo. You can set spark.file.transferTo=false "</span> +</span><br><span class="line">          <span class="string">"to disable this NIO feature."</span></span><br><span class="line">      );</span><br><span class="line">    &#125;</span><br><span class="line">    threwException = <span class="literal">false</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; spills.length; i++) &#123;</span><br><span class="line">      assert(spillInputChannelPositions[i] == spills[i].file.length());</span><br><span class="line">      <span class="type">Closeables</span>.close(spillInputChannels[i], threwException);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Closeables</span>.close(mergedFileOutputChannel, threwException);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> partitionLengths;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark 三大ShuffleWriter 之 UnsafeShuffleWriter&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-shuffle之BypassMergeSortShuffleWriter</title>
    <link href="http://yoursite.com/2019/11/30/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BBypassMergeSortShuffleWriter/"/>
    <id>http://yoursite.com/2019/11/30/spark源码-shuffle之BypassMergeSortShuffleWriter/</id>
    <published>2019-11-30T10:54:59.000Z</published>
    <updated>2019-12-20T03:17:36.238Z</updated>
    
    <content type="html"><![CDATA[<p>spark 三大ShuffleWriter 之 BypassMergeSortShuffleWriter</p><a id="more"></a><p>spark 版本 2.4.3</p><h1>特点</h1><ul><li><p>它<strong>只适用不需要 map-side aggregation</strong>的Shuffle操作，且<strong>Reducer任务数量比较少</strong>（默认200）</p></li><li><p>数据是直接写入文件，数据量较大的时候，网络I/O和内存负担较重</p></li><li><p>写分区文件时开了多个<code>DiskBlockObjectWriter</code>，对<strong>内存消耗比较大</strong></p></li></ul><h1>大致流程</h1><ol><li><p>为每个分区都创建一个<code>DiskBlockObjectWriter</code></p></li><li><p>遍历数据，使用<code>DiskBlockObjectWriter</code>的<code>write</code>方法将数据写入到不同分区文件中</p></li><li><p>刷写分区文件到磁盘</p></li><li><p>合并分区文件成一个大文件，并返回记录每个分区文件的长度的数组</p></li><li><p>根据分区长度数组生成索引文件</p></li><li><p>封装信息到<code>MapStatus</code>返回</p></li></ol><p>总的来说就是生成了一个<strong>由分区文件合并形成的大文件</strong>和一个<strong>索引文件</strong>（上面流程是主要流程，重命名什么的就不写了）。</p><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/spark/shuffle/BypassMergeSortShuffleWriter.jpg" alt></p><h1>源码</h1><h2 id="write-…"><a class="header-anchor" href="#write-…">¶</a>write(…)</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">public void write(<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert (partitionWriters == <span class="literal">null</span>);</span><br><span class="line">  <span class="keyword">if</span> (!records.hasNext()) &#123;</span><br><span class="line">    partitionLengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, <span class="literal">null</span>);</span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">final</span> <span class="type">SerializerInstance</span> serInstance = serializer.newInstance();</span><br><span class="line">  <span class="keyword">final</span> long openStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">  <span class="comment">// partitionWriter 数组：分区号即是数组偏移量。它们将数据按分区号分别写入不同的个文件，有多少个分区就形成多少个文件</span></span><br><span class="line">  <span class="comment">// 这里的 numPartitions 是分区后的数量</span></span><br><span class="line">  partitionWriters = <span class="keyword">new</span> <span class="type">DiskBlockObjectWriter</span>[numPartitions];</span><br><span class="line">  partitionWriterSegments = <span class="keyword">new</span> <span class="type">FileSegment</span>[numPartitions];</span><br><span class="line">  <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Tuple2</span>&lt;<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>&gt; tempShuffleBlockIdPlusFile =</span><br><span class="line">      blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">File</span> file = tempShuffleBlockIdPlusFile._2();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">BlockId</span> blockId = tempShuffleBlockIdPlusFile._1();</span><br><span class="line">    partitionWriters[i] =</span><br><span class="line">      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Creating the file to write to and creating a disk writer both involve interacting with</span></span><br><span class="line">  <span class="comment">// the disk, and can take a long time in aggregate when we open many files, so should be</span></span><br><span class="line">  <span class="comment">// included in the shuffle write time.</span></span><br><span class="line">  writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - openStartTime);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record = records.next();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">K</span> key = record._1();</span><br><span class="line">    <span class="comment">// 按分区器的规则写入数据</span></span><br><span class="line">    partitionWriters[partitioner.getPartition(key)].write(key, record._2());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">DiskBlockObjectWriter</span> writer = partitionWriters[i];</span><br><span class="line">    <span class="comment">// 刷写数据到磁盘</span></span><br><span class="line">    partitionWriterSegments[i] = writer.commitAndGet();</span><br><span class="line">    writer.close();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 文件名："shuffle_" + shuffleId + "_" + mapId + "_" + reduceId + ".data"</span></span><br><span class="line">  <span class="type">File</span> output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">  <span class="type">File</span> tmp = <span class="type">Utils</span>.tempFileWith(output);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 合并生成的分区文件，并返回每个分区文件的大小，用于计算偏移量</span></span><br><span class="line">    partitionLengths = writePartitionedFile(tmp);</span><br><span class="line">    <span class="comment">// 生成索引文件</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="comment">// 这里可以删除是因为 writeIndexFileAndCommit 中重命名了它</span></span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logger.error(<span class="string">"Error while deleting temp file &#123;&#125;"</span>, tmp.getAbsolutePath());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="writePartitionedFile-…"><a class="header-anchor" href="#writePartitionedFile-…">¶</a>writePartitionedFile(…)</h2><p>合并生成的分区文件，并返回每个分区文件的大小，用于计算偏移量</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> long[] writePartitionedFile(<span class="type">File</span> outputFile) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  <span class="comment">// Track location of the partition starts in the output file</span></span><br><span class="line">  <span class="comment">// lengths数组：记录每个分区文件的大小</span></span><br><span class="line">  <span class="keyword">final</span> long[] lengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">  <span class="keyword">if</span> (partitionWriters == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="comment">// We were passed an empty iterator</span></span><br><span class="line">    <span class="keyword">return</span> lengths;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并文件</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">FileOutputStream</span> out = <span class="keyword">new</span> <span class="type">FileOutputStream</span>(outputFile, <span class="literal">true</span>);</span><br><span class="line">  <span class="keyword">final</span> long writeStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">  boolean threwException = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">File</span> file = partitionWriterSegments[i].file();</span><br><span class="line">      <span class="keyword">if</span> (file.exists()) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileInputStream</span> in = <span class="keyword">new</span> <span class="type">FileInputStream</span>(file);</span><br><span class="line">        boolean copyThrewException = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 把分区文件 拷贝到 合并文件 中，存入文件大小到lengths数组中</span></span><br><span class="line">          lengths[i] = <span class="type">Utils</span>.copyStream(in, out, <span class="literal">false</span>, transferToEnabled);</span><br><span class="line">          copyThrewException = <span class="literal">false</span>;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          <span class="type">Closeables</span>.close(in, copyThrewException);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 删除分区文件</span></span><br><span class="line">        <span class="keyword">if</span> (!file.delete()) &#123;</span><br><span class="line">          logger.error(<span class="string">"Unable to delete file for partition &#123;&#125;"</span>, i);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    threwException = <span class="literal">false</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="type">Closeables</span>.close(out, threwException);</span><br><span class="line">    writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - writeStartTime);</span><br><span class="line">  &#125;</span><br><span class="line">  partitionWriters = <span class="literal">null</span>;</span><br><span class="line">  <span class="keyword">return</span> lengths;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="writeIndexFileAndCommit-…"><a class="header-anchor" href="#writeIndexFileAndCommit-…">¶</a>writeIndexFileAndCommit(…)</h2><p>生成索引文件</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeIndexFileAndCommit</span></span>(</span><br><span class="line">    shuffleId: <span class="type">Int</span>,</span><br><span class="line">    mapId: <span class="type">Int</span>,</span><br><span class="line">    lengths: <span class="type">Array</span>[<span class="type">Long</span>],</span><br><span class="line">    dataTmp: <span class="type">File</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> indexFile = getIndexFile(shuffleId, mapId)</span><br><span class="line">  <span class="keyword">val</span> indexTmp = <span class="type">Utils</span>.tempFileWith(indexFile)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> dataFile = getDataFile(shuffleId, mapId)</span><br><span class="line">    <span class="comment">// There is only one IndexShuffleBlockResolver per executor, this synchronization make sure</span></span><br><span class="line">    <span class="comment">// the following check and rename are atomic.</span></span><br><span class="line">    synchronized &#123;</span><br><span class="line">      <span class="keyword">val</span> existingLengths = checkIndexAndDataFile(indexFile, dataFile, lengths.length)</span><br><span class="line">      <span class="keyword">if</span> (existingLengths != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Another attempt for the same task has already written our map outputs successfully,</span></span><br><span class="line">        <span class="comment">// so just use the existing partition lengths and delete our temporary map outputs.</span></span><br><span class="line">        <span class="type">System</span>.arraycopy(existingLengths, <span class="number">0</span>, lengths, <span class="number">0</span>, lengths.length)</span><br><span class="line">        <span class="keyword">if</span> (dataTmp != <span class="literal">null</span> &amp;&amp; dataTmp.exists()) &#123;</span><br><span class="line">          dataTmp.delete()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// This is the first successful attempt in writing the map outputs for this task,</span></span><br><span class="line">        <span class="comment">// so override any existing index and data files with the ones we wrote.</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">DataOutputStream</span>(<span class="keyword">new</span> <span class="type">BufferedOutputStream</span>(<span class="keyword">new</span> <span class="type">FileOutputStream</span>(indexTmp)))</span><br><span class="line">        <span class="type">Utils</span>.tryWithSafeFinally &#123;</span><br><span class="line">          <span class="comment">// We take in lengths of each block, need to convert it to offsets.</span></span><br><span class="line">          <span class="comment">// 索引其实就是按 分区文件大小 叠上去而已</span></span><br><span class="line">          <span class="keyword">var</span> offset = <span class="number">0</span>L</span><br><span class="line">          out.writeLong(offset)</span><br><span class="line">          <span class="keyword">for</span> (length &lt;- lengths) &#123;</span><br><span class="line">            offset += length</span><br><span class="line">            out.writeLong(offset)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; &#123;</span><br><span class="line">          out.close()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (indexFile.exists()) &#123;</span><br><span class="line">          indexFile.delete()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (dataFile.exists()) &#123;</span><br><span class="line">          dataFile.delete()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 重命名 indexTmp</span></span><br><span class="line">        <span class="keyword">if</span> (!indexTmp.renameTo(indexFile)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"fail to rename file "</span> + indexTmp + <span class="string">" to "</span> + indexFile)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 重命名 dataTmp</span></span><br><span class="line">        <span class="keyword">if</span> (dataTmp != <span class="literal">null</span> &amp;&amp; dataTmp.exists() &amp;&amp; !dataTmp.renameTo(dataFile)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"fail to rename file "</span> + dataTmp + <span class="string">" to "</span> + dataFile)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (indexTmp.exists() &amp;&amp; !indexTmp.delete()) &#123;</span><br><span class="line">      logError(<span class="string">s"Failed to delete temporary index file at <span class="subst">$&#123;indexTmp.getAbsolutePath&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark 三大ShuffleWriter 之 BypassMergeSortShuffleWriter&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-shuffle之ShuffleManager</title>
    <link href="http://yoursite.com/2019/11/29/spark%E6%BA%90%E7%A0%81-shuffle%E4%B9%8BShuffleManager/"/>
    <id>http://yoursite.com/2019/11/29/spark源码-shuffle之ShuffleManager/</id>
    <published>2019-11-29T06:45:04.000Z</published>
    <updated>2019-12-28T07:31:22.339Z</updated>
    
    <content type="html"><![CDATA[<p>spark shuffle 学习起始篇，涉及源码中shuffle的相关概念 加 SortShuffleManager</p><a id="more"></a><p>spark 版本 2.4.3</p><h1>Spark 里的 Shuffle</h1><p>shuffle 主要分为:</p><ul><li><strong>ShuffleWrite</strong> 阶段：上一个stage 的尾任务<code>ShuffleMapTask</code> 把数据写入磁盘，也叫<code>ShuffleMap</code></li><li><strong>ShuffleRead</strong> 阶段： 下一个stage 拉取数据，也叫<code>ShuffleReduce</code></li></ul><p>源码中的一些概念：</p><ul><li><p>如果把 spark 整个流程看成一辆火车，那么除了最后一节是<code>ResultStage </code>，其它每一节车厢就是一个<code>ShuffleMapStage </code>，连接车厢的部分就是<code>shuffle</code>。</p></li><li><p>车厢头进行 <strong>read</strong>，车厢尾进行 <strong>write</strong>。很容易理解<code>ShuffleMapStage</code>需要读前一个stage内容，也需要把输出写入下一个stage；而<code>ResultStage</code>只需要读。这些可以在源码中发现。</p></li><li><p><code>ShuffleMapStage</code>对应 <code>ShuffleMapTask</code>，<code>ResultStage</code> 对应 <code>ResultTask</code>。</p></li><li><p><strong>read</strong> 实质是<code>ShuffleReader</code>里的 <code>read()</code>方法；<strong>write</strong> 是实质是<code>ShuffleWriter</code>里的 <code>write()</code>方法。</p></li><li><p><code>ShuffleReader</code> 和 <code>ShuffleWriter</code> 这两大组件都由<code>ShuffleManager</code>进行选择。</p></li></ul><p>好了，脑子里有了这些概念，就可以对这3个模块进行仔细研究了。</p><h1>SortShuffleManager</h1><p>由于 Spark 2.0以后，<code>ShuffleManager</code>只提供一种实现：<code>SortShuffleManager</code>，因此只深入研究它。</p><p>以下是<code>ShuffleMapTask</code>中的写操作。可以发现 <code>shuffleManager</code> 是 <code>SparkEnv</code> 中的属性。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从 SparkEnv 中 得到 shuffleManager</span></span><br><span class="line"><span class="keyword">val</span> manager = <span class="type">SparkEnv</span>.get.shuffleManager</span><br><span class="line"><span class="comment">// 从 shuffleManager 中得到 ShuffleWriter</span></span><br><span class="line">writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class="line"><span class="comment">// 执行 ShuffleWriter 里的 write 方法</span></span><br><span class="line">writer.write(rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])</span><br><span class="line"><span class="comment">// 成功写入，收尾工作</span></span><br><span class="line">writer.stop(success = <span class="literal">true</span>).get</span><br></pre></td></tr></table></figure><p>这里事先预告下，有3种<code>ShuffleWriter</code>，1种<code>ShuffleReader</code>。</p><p>那么如何选择呢？注意上面，它取决于<code>dep.shuffleHandle</code>，而它来自<code>shuffleManager</code>的<code>registerShuffle()</code>方法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> shuffleHandle: <span class="type">ShuffleHandle</span> = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">  shuffleId, _rdd.partitions.length, <span class="keyword">this</span>)</span><br></pre></td></tr></table></figure><p>好的，让我们进入<code>SortShuffleManager</code>中一探究竟。</p><h2 id="registerShuffle-…"><a class="header-anchor" href="#registerShuffle-…">¶</a>registerShuffle(…)</h2><p>其实就是选择 <code>handle</code> 的过程</p><ol><li><p>如果 <strong>不需要map端的聚合操作</strong> 且 <strong>shuffle 后的分区数量小于等于200</strong>（<code>spark.shuffle.sort.bypassMergeThreshold</code>），就选择 <code>BypassMergeSortShuffleHandle</code>。否则进入第二步</p></li><li><p>如果 <strong>序列化器支持重定位</strong> 且 <strong>不需要map端聚合</strong> 且 <strong>shuffle 后的分区数目小于等于2^24)</strong>，就选择 <code>SerializedShuffleHandle</code>。否则进入第三步</p></li><li><p>选择 <code>BaseShuffleHandle</code></p></li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    shuffleId: <span class="type">Int</span>,</span><br><span class="line">    numMaps: <span class="type">Int</span>,</span><br><span class="line">    dependency: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]): <span class="type">ShuffleHandle</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, numMaps, dependency)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="getWriter-…"><a class="header-anchor" href="#getWriter-…">¶</a>getWriter(…)</h2><p>根据上面得到的<code>handle</code>，进行模式匹配选择<code>ShuffleWriter</code>，有3种：</p><p><code>BypassMergeSortHandle</code>  --&gt; <code>BypassMergeSortShuffleWriter</code></p><p><code>SerializedShuffleHandle</code> --&gt; <code>UnsafeShuffleWriter</code></p><p><code>other(BaseShuffleHandle)</code> --&gt; <code>SortShuffleWriter</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getWriter</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">    handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">    mapId: <span class="type">Int</span>,</span><br><span class="line">    context: <span class="type">TaskContext</span>): <span class="type">ShuffleWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">  numMapsForShuffle.putIfAbsent(</span><br><span class="line">    handle.shuffleId, handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[_, _, _]].numMaps)</span><br><span class="line">  <span class="keyword">val</span> env = <span class="type">SparkEnv</span>.get</span><br><span class="line">  handle <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> unsafeShuffleHandle: <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">UnsafeShuffleWriter</span>(</span><br><span class="line">        env.blockManager,</span><br><span class="line">        shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">        context.taskMemoryManager(),</span><br><span class="line">        unsafeShuffleHandle,</span><br><span class="line">        mapId,</span><br><span class="line">        context,</span><br><span class="line">        env.conf)</span><br><span class="line">    <span class="keyword">case</span> bypassMergeSortHandle: <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleWriter</span>(</span><br><span class="line">        env.blockManager,</span><br><span class="line">        shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">        bypassMergeSortHandle,</span><br><span class="line">        mapId,</span><br><span class="line">        context,</span><br><span class="line">        env.conf)</span><br><span class="line">    <span class="keyword">case</span> other: <span class="type">BaseShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>, _] =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">SortShuffleWriter</span>(shuffleBlockResolver, other, mapId, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="getReader-…"><a class="header-anchor" href="#getReader-…">¶</a>getReader(…)</h2><p>只有一种<code>ShuffleReader</code>：<code>BlockStoreShuffleReader</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getReader</span></span>[<span class="type">K</span>, <span class="type">C</span>](</span><br><span class="line">    handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">    startPartition: <span class="type">Int</span>,</span><br><span class="line">    endPartition: <span class="type">Int</span>,</span><br><span class="line">    context: <span class="type">TaskContext</span>): <span class="type">ShuffleReader</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">BlockStoreShuffleReader</span>(</span><br><span class="line">    handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[<span class="type">K</span>, _, <span class="type">C</span>]], startPartition, endPartition, context)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;spark shuffle 学习起始篇，涉及源码中shuffle的相关概念 加 SortShuffleManager&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark源码-Partitioner学习</title>
    <link href="http://yoursite.com/2019/11/28/spark%E6%BA%90%E7%A0%81-Partitioner%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/11/28/spark源码-Partitioner学习/</id>
    <published>2019-11-28T01:42:43.000Z</published>
    <updated>2019-12-20T03:17:30.074Z</updated>
    
    <content type="html"><![CDATA[<p>Partitioner（分区器）学习</p><a id="more"></a><p>spark 版本 2.4.3</p><h1>Partitioner</h1><p>分区器，RDD五大特性之五（只针对（k,v）类型的RDD）。它的核心作用是使用 <code>getPartition(key: Any)</code>对每条数据进行分区</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它有两大实现，<code>HashPartitioner</code> 和 <code>RangePartitioner</code>。分区是为了并行处理，所以让每个分区的大小差不多是首要目标。</p><h1>HashPartitioner</h1><p>这个是最简单的，直接通过 key 的 hashCode 取模分区，能让数据大致均匀地分布在各个分区。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class HashPartitioner(partitions: Int) extends Partitioner &#123;</span><br><span class="line">  require(partitions &gt;= 0, s&quot;Number of partitions ($partitions) cannot be negative.&quot;)</span><br><span class="line"></span><br><span class="line">  def numPartitions: Int = partitions</span><br><span class="line"></span><br><span class="line">  // 看这里</span><br><span class="line">  def getPartition(key: Any): Int = key match &#123;</span><br><span class="line">    case null =&gt; 0</span><br><span class="line">    case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def equals(other: Any): Boolean = other match &#123;</span><br><span class="line">    case h: HashPartitioner =&gt;</span><br><span class="line">      h.numPartitions == numPartitions</span><br><span class="line">    case _ =&gt;</span><br><span class="line">      false</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def hashCode: Int = numPartitions</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>RangePartitioner</h1><p>既然有了hash分区，为什么还要range分区呢。事想需要全局排序时，如果使用hash分区，排序后只是分区有序，然后再对分区进行归并排序，这样工作量是不是特别大。所以排序时一般用 <code>RangePartitioner</code> ，比如 <code>sortByKey</code>。它的效果让是一个分区中的元素肯定都是比另一个分区内的元素小或者大。这样分区排序后的数据就是全局有序的。并且它通过采样操作可以让数据比较均匀地分布到各个分区。</p><p>它的大致步骤是：对每个分区进行采样（蓄水池采样） -&gt; 判断每个分区的采样结果是否合格，如果不合格再次采样 -&gt; 把采样数据排序，每条采样数据都有权重，按权重，计算出分解边界数组<code>rangeBounds</code> -&gt; 按边界，把数据划分到不同分区<code>getPartition(key: Any)</code>。</p><h2 id="rangeBounds"><a class="header-anchor" href="#rangeBounds">¶</a>rangeBounds</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> rangeBounds: <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (partitions &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="type">Array</span>.empty</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// This is the sample size we need to have roughly balanced output partitions, capped at 1M.</span></span><br><span class="line">    <span class="comment">// Cast to double to avoid overflowing ints or longs</span></span><br><span class="line">    <span class="comment">// 总采样点的个数，不超过 1e6，注意 partitions 是分区后的分区个数</span></span><br><span class="line">    <span class="keyword">val</span> sampleSize = math.min(samplePointsPerPartitionHint.toDouble * partitions, <span class="number">1e6</span>)</span><br><span class="line">    <span class="comment">// Assume the input partitions are roughly balanced and over-sample a little bit.</span></span><br><span class="line">    <span class="comment">// 每个分区的采样点个数，并乘了3进行过采样</span></span><br><span class="line">    <span class="keyword">val</span> sampleSizePerPartition = math.ceil(<span class="number">3.0</span> * sampleSize / rdd.partitions.length).toInt</span><br><span class="line">    <span class="comment">// 使用蓄水池采样法（见下）进行采样，返回总数据个数，和每个分区的采样情况(partitionId, 该分区数据总个数, sample)</span></span><br><span class="line">    <span class="keyword">val</span> (numItems, sketched) = <span class="type">RangePartitioner</span>.sketch(rdd.map(_._1), sampleSizePerPartition)</span><br><span class="line">    <span class="keyword">if</span> (numItems == <span class="number">0</span>L) &#123;</span><br><span class="line">      <span class="type">Array</span>.empty</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// If a partition contains much more than the average number of items, we re-sample from it</span></span><br><span class="line">      <span class="comment">// to ensure that enough items are collected from that partition.</span></span><br><span class="line">      <span class="keyword">val</span> fraction = math.min(sampleSize / math.max(numItems, <span class="number">1</span>L), <span class="number">1.0</span>)</span><br><span class="line">      <span class="keyword">val</span> candidates = <span class="type">ArrayBuffer</span>.empty[(<span class="type">K</span>, <span class="type">Float</span>)]</span><br><span class="line">      <span class="keyword">val</span> imbalancedPartitions = mutable.<span class="type">Set</span>.empty[<span class="type">Int</span>]</span><br><span class="line">      sketched.foreach &#123; <span class="keyword">case</span> (idx, n, sample) =&gt;</span><br><span class="line">        <span class="comment">// 如果一个分区采样过多，就重新采样它</span></span><br><span class="line">        <span class="keyword">if</span> (fraction * n &gt; sampleSizePerPartition) &#123;</span><br><span class="line">          imbalancedPartitions += idx</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// The weight is 1 over the sampling probability.</span></span><br><span class="line">          <span class="comment">// 权重 = 该分区数据总个数 / 采样点数</span></span><br><span class="line">          <span class="keyword">val</span> weight = (n.toDouble / sample.length).toFloat</span><br><span class="line">          <span class="keyword">for</span> (key &lt;- sample) &#123;</span><br><span class="line">            candidates += ((key, weight))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (imbalancedPartitions.nonEmpty) &#123;</span><br><span class="line">        <span class="comment">// Re-sample imbalanced partitions with the desired sampling probability.</span></span><br><span class="line">        <span class="comment">// 重新采样</span></span><br><span class="line">        <span class="keyword">val</span> imbalanced = <span class="keyword">new</span> <span class="type">PartitionPruningRDD</span>(rdd.map(_._1), imbalancedPartitions.contains)</span><br><span class="line">        <span class="keyword">val</span> seed = byteswap32(-rdd.id - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> reSampled = imbalanced.sample(withReplacement = <span class="literal">false</span>, fraction, seed).collect()</span><br><span class="line">        <span class="comment">// 以采样率的倒数做权重</span></span><br><span class="line">        <span class="keyword">val</span> weight = (<span class="number">1.0</span> / fraction).toFloat</span><br><span class="line">        candidates ++= reSampled.map(x =&gt; (x, weight))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 返回每个分区边界数据的数组（见下），数组长度为分区个数 - 1 (很好理解，切4份西瓜，需要3刀)</span></span><br><span class="line">      <span class="type">RangePartitioner</span>.determineBounds(candidates, math.min(partitions, candidates.size))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="蓄水池采样算法（Reservoir-Sampling）"><a class="header-anchor" href="#蓄水池采样算法（Reservoir-Sampling）">¶</a>蓄水池采样算法（Reservoir Sampling）</h2><ul><li>场景：数据流长度N很大且不可知，不能一次性存入内存；保证时间复杂度为O(N)；随机选取k个数，每个数被选中的概率为 k/N。</li><li>步骤：<ol><li>如果数据总量小于k，则依次放入蓄水池。池子满了，进入步骤2。</li><li>当遍历到第i个数据时，在[0, i]范围内取以随机数d，若d的落在[0, k-1]范围内，则用该数据替换蓄水池中的第d个数据。</li><li>重复步骤2，直到遍历完。</li></ol></li></ul><p>想深究原理的看<a href="https://www.jianshu.com/p/7a9ea6ece2af" target="_blank" rel="noopener">这个</a>，下面是 spark 中对该算法是实现。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sketch</span></span>[<span class="type">K</span> : <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">K</span>],</span><br><span class="line">    sampleSizePerPartition: <span class="type">Int</span>): (<span class="type">Long</span>, <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Long</span>, <span class="type">Array</span>[<span class="type">K</span>])]) = &#123;</span><br><span class="line">  <span class="keyword">val</span> shift = rdd.id</span><br><span class="line">  <span class="comment">// val classTagK = classTag[K] // to avoid serializing the entire partitioner object</span></span><br><span class="line">  <span class="keyword">val</span> sketched = rdd.mapPartitionsWithIndex &#123; (idx, iter) =&gt;</span><br><span class="line">    <span class="keyword">val</span> seed = byteswap32(idx ^ (shift &lt;&lt; <span class="number">16</span>))</span><br><span class="line">    <span class="keyword">val</span> (sample, n) = <span class="type">SamplingUtils</span>.reservoirSampleAndCount(</span><br><span class="line">      iter, sampleSizePerPartition, seed)</span><br><span class="line">    <span class="type">Iterator</span>((idx, n, sample))</span><br><span class="line">  &#125;.collect()</span><br><span class="line">  <span class="keyword">val</span> numItems = sketched.map(_._2).sum</span><br><span class="line">  (numItems, sketched)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心步骤是通过 <code>SamplingUtils.reservoirSampleAndCount(xxx)</code> 得到采样结果</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reservoirSampleAndCount</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    input: <span class="type">Iterator</span>[<span class="type">T</span>],</span><br><span class="line">    k: <span class="type">Int</span>,</span><br><span class="line">    seed: <span class="type">Long</span> = <span class="type">Random</span>.nextLong())</span><br><span class="line">  : (<span class="type">Array</span>[<span class="type">T</span>], <span class="type">Long</span>) = &#123;</span><br><span class="line">  <span class="keyword">val</span> reservoir = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](k) <span class="comment">// 装采样点的蓄水池</span></span><br><span class="line">  <span class="comment">// Put the first k elements in the reservoir.</span></span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> (i &lt; k &amp;&amp; input.hasNext) &#123;</span><br><span class="line">    <span class="keyword">val</span> item = input.next()</span><br><span class="line">    reservoir(i) = item</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If we have consumed all the elements, return them. Otherwise do the replacement.</span></span><br><span class="line">  <span class="keyword">if</span> (i &lt; k) &#123;</span><br><span class="line">    <span class="comment">// If input size &lt; k, trim the array to return only an array of input size.</span></span><br><span class="line">    <span class="comment">// 如果数据总个数不足采样个数，那就全部采样了，然后返回</span></span><br><span class="line">    <span class="keyword">val</span> trimReservoir = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](i)</span><br><span class="line">    <span class="type">System</span>.arraycopy(reservoir, <span class="number">0</span>, trimReservoir, <span class="number">0</span>, i)</span><br><span class="line">    (trimReservoir, i)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// If input size &gt; k, continue the sampling process.</span></span><br><span class="line">    <span class="keyword">var</span> l = i.toLong</span><br><span class="line">    <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">XORShiftRandom</span>(seed)</span><br><span class="line">    <span class="comment">// 对整个 input 遍历一次</span></span><br><span class="line">    <span class="keyword">while</span> (input.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> item = input.next()</span><br><span class="line">      l += <span class="number">1</span></span><br><span class="line">      <span class="comment">// There are k elements in the reservoir, and the l-th element has been</span></span><br><span class="line">      <span class="comment">// consumed. It should be chosen with probability k/l. The expression</span></span><br><span class="line">      <span class="comment">// below is a random long chosen uniformly from [0,l)</span></span><br><span class="line">      <span class="keyword">val</span> replacementIndex = (rand.nextDouble() * l).toLong <span class="comment">// 取[0,l)的随机数d</span></span><br><span class="line">      <span class="comment">// 如果 d 在 k 的范围内，则用 item 替换池子里的第d个数据。</span></span><br><span class="line">      <span class="keyword">if</span> (replacementIndex &lt; k) &#123;</span><br><span class="line">        reservoir(replacementIndex.toInt) = item</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    (reservoir, l)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="按权重选择边界"><a class="header-anchor" href="#按权重选择边界">¶</a>按权重选择边界</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Determines the bounds for range partitioning from candidates with weights indicating how many</span></span><br><span class="line"><span class="comment">  * items each represents. Usually this is 1 over the probability used to sample this candidate.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param candidates unordered candidates with weights</span></span><br><span class="line"><span class="comment">  * @param partitions number of partitions</span></span><br><span class="line"><span class="comment">  * @return selected bounds</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">determineBounds</span></span>[<span class="type">K</span> : <span class="type">Ordering</span> : <span class="type">ClassTag</span>](</span><br><span class="line">     candidates: <span class="type">ArrayBuffer</span>[(<span class="type">K</span>, <span class="type">Float</span>)],</span><br><span class="line">     partitions: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">   <span class="keyword">val</span> ordering = implicitly[<span class="type">Ordering</span>[<span class="type">K</span>]]</span><br><span class="line">   <span class="keyword">val</span> ordered = candidates.sortBy(_._1) <span class="comment">// 把采样点排好序</span></span><br><span class="line">   <span class="keyword">val</span> numCandidates = ordered.size</span><br><span class="line">   <span class="keyword">val</span> sumWeights = ordered.map(_._2.toDouble).sum</span><br><span class="line">   <span class="keyword">val</span> step = sumWeights / partitions</span><br><span class="line">   <span class="keyword">var</span> cumWeight = <span class="number">0.0</span></span><br><span class="line">   <span class="keyword">var</span> target = step</span><br><span class="line">   <span class="keyword">val</span> bounds = <span class="type">ArrayBuffer</span>.empty[<span class="type">K</span>]</span><br><span class="line">   <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">   <span class="keyword">var</span> j = <span class="number">0</span></span><br><span class="line">   <span class="keyword">var</span> previousBound = <span class="type">Option</span>.empty[<span class="type">K</span>]</span><br><span class="line">   <span class="keyword">while</span> ((i &lt; numCandidates) &amp;&amp; (j &lt; partitions - <span class="number">1</span>)) &#123;</span><br><span class="line">     <span class="keyword">val</span> (key, weight) = ordered(i)</span><br><span class="line">     cumWeight += weight</span><br><span class="line">     <span class="keyword">if</span> (cumWeight &gt;= target) &#123;</span><br><span class="line">       <span class="comment">// Skip duplicate values.</span></span><br><span class="line">       <span class="keyword">if</span> (previousBound.isEmpty || ordering.gt(key, previousBound.get)) &#123;</span><br><span class="line">         bounds += key</span><br><span class="line">         target += step</span><br><span class="line">         j += <span class="number">1</span></span><br><span class="line">         previousBound = <span class="type">Some</span>(key)</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     i += <span class="number">1</span></span><br><span class="line">   &#125;</span><br><span class="line">   bounds.toArray</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h2 id="getPartition-key-Any"><a class="header-anchor" href="#getPartition-key-Any">¶</a>getPartition(key: Any)</h2><p>有了边界，<code>getPartition(key: Any)</code>就很好计算了，其实就是个在有序区间找位置的过程。分区少就一个个比过去，如果区间数大于128，就使用二分查找获取分区位置。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">K</span>]</span><br><span class="line">  <span class="keyword">var</span> partition = <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> (rangeBounds.length &lt;= <span class="number">128</span>) &#123;</span><br><span class="line">    <span class="comment">// If we have less than 128 partitions naive search</span></span><br><span class="line">    <span class="keyword">while</span> (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) &#123;</span><br><span class="line">      partition += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 二分查找</span></span><br><span class="line">    partition = binarySearch(rangeBounds, k)</span><br><span class="line">    <span class="comment">// binarySearch either returns the match location or -[insertion point]-1</span></span><br><span class="line">    <span class="keyword">if</span> (partition &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      partition = -partition<span class="number">-1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (partition &gt; rangeBounds.length) &#123;</span><br><span class="line">      partition = rangeBounds.length</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 根据升序还是降序，返回相应的PartitionId。</span></span><br><span class="line">  <span class="keyword">if</span> (ascending) &#123;</span><br><span class="line">    partition</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    rangeBounds.length - partition</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Partitioner（分区器）学习&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark杂谈-使用textFile读取HDFS的分区规则</title>
    <link href="http://yoursite.com/2019/11/27/spark%E6%9D%82%E8%B0%88-%E4%BD%BF%E7%94%A8textFile%E8%AF%BB%E5%8F%96HDFS%E7%9A%84%E5%88%86%E5%8C%BA%E8%A7%84%E5%88%99/"/>
    <id>http://yoursite.com/2019/11/27/spark杂谈-使用textFile读取HDFS的分区规则/</id>
    <published>2019-11-27T06:43:54.000Z</published>
    <updated>2019-12-20T03:18:10.428Z</updated>
    
    <content type="html"><![CDATA[<p>使用 textFile 读取HDFS的数据分区规则</p><a id="more"></a><p>spark 版本 2.4.3</p><h1>跟着源码走</h1><p>测试文件：大小 516.06 MB ，54个 block，blockSize 大小是128M，但每个 block 里面的数据只有10M 左右</p><p><strong>1. 进入 <code>sc.textFile()</code></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFile: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"hdfs://xxxx"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// textFile() 有个默认值：minPartitions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions)</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 它是取 defaultParallelism 和 2 的最小值 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">defaultMinPartitions</span></span>: <span class="type">Int</span> = math.min(defaultParallelism, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这个 defaultParallelism 不指定就是 totalCores，我这里是4</span></span><br><span class="line">scheduler.conf.getInt(<span class="string">"spark.default.parallelism"</span>, totalCores)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 所以 defaultMinPartitions 最终为2</span></span><br></pre></td></tr></table></figure><p><strong>2. 创建 <code>HadoopRDD</code></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">HadoopRDD</span>(</span><br><span class="line">  <span class="keyword">this</span>,</span><br><span class="line">  confBroadcast,</span><br><span class="line">  <span class="type">Some</span>(setInputPathsFunc),</span><br><span class="line">  inputFormatClass,</span><br><span class="line">  keyClass,</span><br><span class="line">  valueClass,</span><br><span class="line">  minPartitions).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3. 每个RDD 都有一个 <code>getPartitions</code> 函数，由它得到分区号</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">val</span> allInputSplits = getInputFormat(jobConf).getSplits(jobConf, minPartitions)</span><br><span class="line">  ...</span><br><span class="line">   <span class="keyword">val</span> array = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Partition</span>](inputSplits.size)</span><br><span class="line">   <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until inputSplits.size) &#123;</span><br><span class="line">   array(i) = <span class="keyword">new</span> <span class="type">HadoopPartition</span>(id, i, inputSplits(i))</span><br><span class="line">   &#125;</span><br><span class="line">   array</span><br><span class="line">   &#125;</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>4. 采用<code>FileInputFormat</code> 里的 <code>getSplits()</code> 划分分区，先计算 splitSize</strong></p><p><code>getPartitions</code> 的 核心是 <code>getSplits()</code>，下面是计算分区关键步骤</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 总大小除2，为258M</span></span><br><span class="line">long goalSize = totalSize / (long)(numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这是人为设定的分区最小值，这个很好理解</span></span><br><span class="line">long minSize = <span class="type">Math</span>.max(job.getLong(<span class="string">"mapreduce.input.fileinputformat.split.minsize"</span>, <span class="number">1</span>L), <span class="keyword">this</span>.minSplitSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">// HDFS 文件的块大小，128M</span></span><br><span class="line">long blockSize = file.getBlockSize();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算 splitSize</span></span><br><span class="line">long splitSize = <span class="keyword">this</span>.computeSplitSize(goalSize, minSize, blockSize);</span><br></pre></td></tr></table></figure><ul><li><p>假设文件大小为 20M： <code>splitSize = max（1，min(10,128)) = 10M</code></p></li><li><p>假设文件大小为 516M：<code>splitSize  = max(1, min(258,128)) = 128M </code>（本文）</p></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> long computeSplitSize(long goalSize, long minSize, long blockSize) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="type">Math</span>.max(minSize, <span class="type">Math</span>.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>5. 最后，按 splitSize 切分区</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 可以发现为了防止最后一个分区过小的问题，引入了数字 1.1，保证最后一个分区的大小大于 splitSize  的 10%</span></span><br><span class="line"><span class="keyword">for</span>(bytesRemaining = length; (double)bytesRemaining / (double)splitSize &gt; <span class="number">1.1</span>D; bytesRemaining -= splitSize) &#123;</span><br><span class="line">splitHosts = <span class="keyword">this</span>.getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);</span><br><span class="line">splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, splitSize, splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>L) &#123;</span><br><span class="line">splitHosts = <span class="keyword">this</span>.getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, bytesRemaining, clusterMap);</span><br><span class="line">splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, bytesRemaining, splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>6 分区结果</strong></p><p>每块128M，最后一块略大，符合预期。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs://xxxx:0+134217728</span><br><span class="line">hdfs://xxxx:134217728+134217728</span><br><span class="line">hdfs://xxxx:268435456+134217728</span><br><span class="line">hdfs://xxxx:402653184+138476793</span><br></pre></td></tr></table></figure><h1>小结</h1><ul><li><p>这里比较奇葩的是 minPartitions 这个设定，它最大只能是2。我觉得之所以这样设定，是防止文件切的过小。假设整个文件大小只有5M，公式：<code>Math.min(goalSize, blockSize)</code> blockSize假定128M，此时 splitSize 由 minPartitions 决定（不考虑人为设定的那个minSize）。那么它最多只能被切成2份。</p></li><li><p>当文件较大时（大于blockSize两倍），只和 blockSize 有关。尽管我的测试文件中每个 block 实际大小只有10M，然鹅这个并没有什么软用。</p></li><li><p>这是单文件情况，如果是读一个目录下的多文件，那就是单独对每个文件进行切分。（从源码可以发现，其中的 totalSize 是所有文件大小总和）。</p></li><li><p>当然这只是分区划分，实际读取数据没这么简单。假如我们是一条一条读，那么如果该分区最后一条数据没读完，它会接着向下一块继续读，参考<a href="https://hadoopi.wordpress.com/2013/05/27/understand-recordreader-inputsplit/" target="_blank" rel="noopener">它</a>。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用 textFile 读取HDFS的数据分区规则&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>CentOS根目录扩容</title>
    <link href="http://yoursite.com/2019/11/23/CentOS%E6%A0%B9%E7%9B%AE%E5%BD%95%E6%89%A9%E5%AE%B9/"/>
    <id>http://yoursite.com/2019/11/23/CentOS根目录扩容/</id>
    <published>2019-11-23T08:40:56.000Z</published>
    <updated>2019-11-23T09:13:42.301Z</updated>
    
    <content type="html"><![CDATA[<p>记录对 /root 目录的扩容</p><a id="more"></a><h1>问题：<code>/root</code> 的空间用满了</h1><p>本来打算直接动态扩容，也就是按鸟哥写的放大LV容量，把 <code>/home</code> 的空间分点给 <code>/root</code> 。结果发现 xfs 文件系统只支持动态增加，不能减少。因此咱只能备份重装了。</p><h1>解决：</h1><h2 id="1-我的版本"><a class="header-anchor" href="#1-我的版本">¶</a>1 我的版本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/redhat-release</span><br><span class="line">CentOS Linux release 7.6.1810 (Core)</span><br></pre></td></tr></table></figure><h2 id="2-分区情况"><a class="header-anchor" href="#2-分区情况">¶</a>2 分区情况</h2><p>CentOS 的 <code>/root</code> 和 <code>/home</code> 目录使用了LVM（逻辑卷分区）</p><p>我准备给 <code>/root</code> 加100G，把 <code>/home</code> 改为700G，预留 50G</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ df -hl</span><br><span class="line">Filesystem           Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/cl-root   50G   46G  5.0G  91% /</span><br><span class="line">...</span><br><span class="line">/dev/mapper/cl-home  849G  220G  629G  26% /home</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们要调的就是这个LV Size</span></span><br><span class="line">$ lvdisplay</span><br><span class="line">  ...</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/home</span><br><span class="line">  LV Name                home</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                &lt;849.07 GiB</span><br><span class="line">  Current LE             217361</span><br><span class="line"></span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/root</span><br><span class="line">  LV Name                root</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                50.00 GiB</span><br><span class="line">  Current LE             12800</span><br></pre></td></tr></table></figure><h2 id="3-备份-home"><a class="header-anchor" href="#3-备份-home">¶</a>3 备份 <code>/home</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tar 压缩，-p 保留权限，-z 使用 gzip</span></span><br><span class="line">$ tar cvpfz /mnt/data/homeback.tgz /home</span><br></pre></td></tr></table></figure><h2 id="4-删除-home"><a class="header-anchor" href="#4-删除-home">¶</a>4 删除 <code>/home</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 干掉/home文件系统的进程</span></span><br><span class="line">$ fuser -km /home/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载/home，如果没用，加 -l 强制卸载</span></span><br><span class="line">$ umount /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 /home 的lv</span></span><br><span class="line">$ lvremove /dev/mapper/cl-home</span><br></pre></td></tr></table></figure><h2 id="5-扩容-root"><a class="header-anchor" href="#5-扩容-root">¶</a>5 扩容 <code>/root</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我这里加 100G</span></span><br><span class="line">$ lvextend -L +100G /dev/mapper/cl-root</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刷新 /root文件系统</span></span><br><span class="line">$ xfs_growfs /dev/mapper/cl-root</span><br></pre></td></tr></table></figure><h2 id="6-恢复-home"><a class="header-anchor" href="#6-恢复-home">¶</a>6 恢复 <code>/home</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分 700G 给它。（预留 50G 的 空闲空间）</span></span><br><span class="line">$ lvcreate -L 700G -n /dev/mapper/cl-home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建文件系统</span></span><br><span class="line">$ mkfs.xfs /dev/mapper/cl-home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂载 /home</span></span><br><span class="line">$ mount /dev/mapper/cl-home /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件恢复</span></span><br><span class="line">$ tar xvpfz /mnt/data/homeback.tgz -C /</span><br></pre></td></tr></table></figure><h2 id="7-检查结果"><a class="header-anchor" href="#7-检查结果">¶</a>7 检查结果</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">$ df -hl</span><br><span class="line">Filesystem           Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/cl-root  150G   46G  105G  31% /</span><br><span class="line">...</span><br><span class="line">/dev/mapper/cl-home  700G  220G  480G  32% /home</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查VG，还有 50G 剩余，稳稳的</span></span><br><span class="line">$ vgdisplay</span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               cl</span><br><span class="line">  Cur LV                3</span><br><span class="line">  ...</span><br><span class="line">  VG Size               &lt;930.51 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              238210</span><br><span class="line">  Alloc PE / Size       225648 / &lt;881.44 GiB</span><br><span class="line">  Free  PE / Size       12562 / 49.07 GiB</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 检查LV，和预想一样</span></span><br><span class="line">$ lvdisplay</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/root</span><br><span class="line">  LV Name                root</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                150.00 GiB</span><br><span class="line">  Current LE             38400</span><br><span class="line"></span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/cl/home</span><br><span class="line">  LV Name                home</span><br><span class="line">  VG Name                cl</span><br><span class="line">  ...</span><br><span class="line">  LV Size                700.00 GiB</span><br><span class="line">  Current LE             179200</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录对 /root 目录的扩容&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="LVM" scheme="http://yoursite.com/tags/LVM/"/>
    
  </entry>
  
  <entry>
    <title>hbase-基本原理</title>
    <link href="http://yoursite.com/2019/09/29/hbase-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2019/09/29/hbase-基本原理/</id>
    <published>2019-09-29T09:20:52.000Z</published>
    <updated>2019-10-08T02:10:35.843Z</updated>
    
    <content type="html"><![CDATA[<p>简单介绍了hbase的基本原理：数据结构、读写操作、flush、合并与切分</p><a id="more"></a><h1>数据结构</h1><h2 id="逻辑结构"><a class="header-anchor" href="#逻辑结构">¶</a>逻辑结构</h2><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/luoji.png" alt></p><p>一个<strong>namespace</strong>可以有多个<strong>表</strong>，如<code>zxylearn:student</code>，代表<code>zxylearn</code>命名空间下的<code>student</code>表。</p><p>一个<strong>表</strong>可以有多个<strong>region</strong>，如上图有3个region。一个region一个文件夹。可以预分区（推荐）或者当一个region过大时会自动触发切分。</p><p>一个<strong>region</strong>可以有多个<strong>store</strong>（按列族划分），如上图有两个列族。一个store一个文件夹，文件夹名是列族名。</p><p>一个<strong>store</strong>可以有多个<strong>HFile</strong>，flush一次产生一个，可以合并（后面会讲）</p><p><strong>HFile</strong>中就是具体数据了，逻辑上是一行行序列化的数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># HDFS中的一个HFile的完整路径：</span><br><span class="line">/hbase/data/zxylearn/student/0fe474b3eb5fef1573d85acae8e5b787/info1/2dce02f62ea446f892a0b3cfc4e4db8a</span><br><span class="line"># namespace  zxylearn</span><br><span class="line"># 表名        student</span><br><span class="line"># region     0fe474b3eb5fef1573d85acae8e5b787</span><br><span class="line"># 列族名      info1</span><br><span class="line"># HFile      2dce02f62ea446f892a0b3cfc4e4db8a</span><br></pre></td></tr></table></figure><h2 id="物理结构"><a class="header-anchor" href="#物理结构">¶</a>物理结构</h2><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/wuli.png" alt></p><p>通过指定 <code>'namespace：表名'</code> 和 <code>'Row Key'</code>，可查找到数据的<code>列族</code>、<code>列名</code>、<code>timestamp</code>、<code>value</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; get &apos;zxylearn:student&apos;,&apos;1001&apos;</span><br><span class="line">COLUMN                  CELL</span><br><span class="line"> info1:name             timestamp=1569722805794, value=zhangsan</span><br></pre></td></tr></table></figure><h1>写数据流程</h1><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/write.png" alt></p><ol><li>客户端向ZK请求返回 元数据表所在的RegionServer地址。（我们可以在ZK客户端中用<code>get /hbase/meta-region-server</code>看到它）</li><li>客户端接收地址，向它请求返回 待写数据所在的RegionServer地址。（我们可以在hbase客户端用<code>scan 'hbase:meta'</code>看到它：<code>column=info:server, timestamp=1569658344714, value=localhost:16020</code>）</li><li>向该地址请求写数据。</li><li>先将写操作记录到操作日志（<strong>WAL</strong>）中，接着将数据写入<strong>内存</strong>中。</li></ol><h1>Flush</h1><p>可以看出，写数据是将数据写入到内存中。当执行Flush刷写操作时，才会将数据写入磁盘，也就是HDFS中，形成一个HFile。</p><p><strong>Flush时机</strong></p><ul><li>大小超过限制：如RegionServer的全局内存大小，默认是堆大小的40%时刷写；单个region大小，默认128M。</li><li>时间：从最后一条数据写入后，1h（默认）没有新数据。</li></ul><p><strong>Flush细节</strong></p><ul><li><strong>flush会删除过期的数据</strong>。假设建表时数据版本数设置为1，那么写入磁盘的的数据最多只有一个版本；如果删除标记是DeleteColumn，那么会删除比它低版本的数据。(<strong>删除标记有多种</strong>，如 Delete：只删自己；DeleteColumn：删除自己与比自己低的)</li><li><strong>flush不会删除带删除标记的数据</strong>。原因：设想，我们原本想删除一条数据，给它打上删除标记。如果flush删除了，假如磁盘中的其它HFile中有该数据的旧版本，那么它们在<strong>合并操作</strong>（后面会讲）时就不会被删除了。</li></ul><p>反正记住这里会干掉不要的数据（版本数与删除标记决定），但<strong>不会干掉带删除标记的数据</strong>。</p><h1>读数据流程</h1><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/hbase/read.png" alt></p><ol><li>找到数据所在RS服务器的流程和写数据基本一样。</li><li>向该地址请求读数据。</li><li>读取cache（缓存）和MemStore（内存），查找该数据。</li><li>如果cache中没有，再去StoreFile（磁盘）中找该数据。</li><li>取时间戳最新的数据返回，并记录到cache中。</li></ol><p><strong>为什么在内存中找到了，还要去磁盘中找？</strong></p><p><strong>时间戳的缘故</strong>：我们用户自然是查找最新的数据，内存中的数据的时间戳不能保证一定比磁盘中的新，所有要把它们都找到，然后比较返回最新的数据。因此，这导致了<strong>hbase读数据比写数据还慢</strong>。所以，<strong>用cache缓存查到的数据</strong>，可以一定程度提高读数据的速度。</p><h1>compact(合并)与split(切分)</h1><h2 id="compact-合并"><a class="header-anchor" href="#compact-合并">¶</a>compact(合并)</h2><p>合并是<strong>将若干个小的HFile，合并成一个大的HFile</strong>。分<strong>Minor Compaction</strong> 和 <strong>Major Compaction</strong>。</p><ul><li><p><strong>Minor Compaction</strong> <strong>不会</strong>清理不要的数据和带删除标记的数据</p></li><li><p><strong>Major Compaction</strong> <strong>会</strong>清理不要的数据和带标记删除的数据。当HFile数大于等于3时，执行<code>compact</code>时执行的是它。</p></li></ul><p><strong>清理细节</strong></p><p>与flush不同，这里的清理不仅会清除不要的数据，还会<strong>清理带删除标记的数据</strong>（干掉它，和比它旧的数据）。</p><h2 id="split-切分"><a class="header-anchor" href="#split-切分">¶</a>split(切分)</h2><p><strong>切分是将大的region切分成若干个小的region</strong>。可以预分区（推荐）或者当一个region过大时会自动触发切分。</p><p><strong>为什么推荐只用一个列族呢？</strong></p><p><strong>多个列族可能导致小文件过多。<strong>假设一个列族的数据的很密集，另一列族很稀疏，那么在触发</strong>flush或者split</strong>时，密集的列族形成的HFile文件足够大没问题，但是稀疏的生成的就是小文件了，久而久之会形成过多小文件使效率降低。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单介绍了hbase的基本原理：数据结构、读写操作、flush、合并与切分&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>hbase-安装与配置</title>
    <link href="http://yoursite.com/2019/09/28/hbase-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2019/09/28/hbase-安装与配置/</id>
    <published>2019-09-28T08:20:25.000Z</published>
    <updated>2019-09-29T09:23:27.889Z</updated>
    
    <content type="html"><![CDATA[<p>介绍hbase的安装与简单使用</p><a id="more"></a><h1>准备</h1><p>我的版本：</p><p><strong>hadoop 2.7.7</strong></p><p><strong>zookeeper 3.5.5</strong></p><p><strong>hbase 1.3.5</strong></p><p>先装好 hadoop 和 zookeeper</p><h1>配置 HBase</h1><h2 id="修改-hbase-env-sh"><a class="header-anchor" href="#修改-hbase-env-sh">¶</a>修改 <a href="http://hbase-env.sh" target="_blank" rel="noopener">hbase-env.sh</a></h2><p>修改<code>hbase-1.3.5/conf</code>下的<code>hbase-env.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=$(/usr/libexec/java_home)</span><br><span class="line"><span class="comment"># 使用自己安装的ZK</span></span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure><h2 id="修改-hbase-site-xml"><a class="header-anchor" href="#修改-hbase-site-xml">¶</a>修改 hbase-site.xml</h2><p>修改<code>hbase-1.3.5/conf</code>下的<code>hbase-site.xml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.master&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:60000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;localhost:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/<span class="built_in">local</span>/apache-zookeeper-3.5.5-bin/zkData&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="添加-core-site-xml-和-hdfs-site-xml"><a class="header-anchor" href="#添加-core-site-xml-和-hdfs-site-xml">¶</a>添加 core-site.xml 和 hdfs-site.xml</h2><p>直接在<code>hbase-1.3.5/conf</code>下创建<strong>软连接</strong>即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/<span class="built_in">local</span>/hadoop-2.7.7/etc/hadoop/core-site.xml /usr/<span class="built_in">local</span>/hbase-1.3.5/conf/core-site.xml</span><br><span class="line"></span><br><span class="line">ln -s /usr/<span class="built_in">local</span>/hadoop-2.7.7/etc/hadoop/hdfs-site.xml /usr/<span class="built_in">local</span>/hbase-1.3.5/conf/hdfs-site.xml</span><br></pre></td></tr></table></figure><h2 id="修改-regionservers"><a class="header-anchor" href="#修改-regionservers">¶</a>修改 regionservers</h2><p>在<code>/hbase-1.3.5/conf/regionservers</code> 中添加<strong>集群节点的名字</strong>，用于群启与群关</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure><h1>使用</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先开HDFS和ZK</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 单节点启动与关闭</span></span><br><span class="line">bin/hbase-daemon.sh start master</span><br><span class="line">bin/hbase-daemon.sh start regionserver</span><br><span class="line">bin/hbase-daemon.sh stop master</span><br><span class="line">bin/hbase-daemon.sh stop regionserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># 群启与群关</span></span><br><span class="line">bin/start-hbase.sh</span><br><span class="line">bin/stop-hbase.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化网页</span></span><br><span class="line">http://localhost:16010</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正常启动后的JPS</span></span><br><span class="line">977 ResourceManager</span><br><span class="line">865 NameNode</span><br><span class="line">4289 Jps</span><br><span class="line">4146 HRegionServer</span><br><span class="line">915 DataNode</span><br><span class="line">1028 NodeManager</span><br><span class="line">2360 QuorumPeerMain</span><br><span class="line">4062 HMaster</span><br></pre></td></tr></table></figure><h1>简单shell操作</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入shell</span></span><br><span class="line">bin/hbase shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># 命名空间的相关操作</span></span><br><span class="line">create_namespace <span class="string">'zxylearn'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 表的相关操作</span></span><br><span class="line"><span class="comment"># 建表</span></span><br><span class="line">create <span class="string">'zxylearn:student'</span>,<span class="string">'info1'</span>,<span class="string">'info2'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 看表</span></span><br><span class="line">describe <span class="string">'zxylearn:student'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删表</span></span><br><span class="line"><span class="built_in">disable</span> <span class="string">'zxylearn:student'</span></span><br><span class="line">drop <span class="string">'zxylearn:student'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据的相关操作</span></span><br><span class="line"><span class="comment"># 增</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:name'</span>,<span class="string">'zhangsan'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:sex'</span>,<span class="string">'man'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1002'</span>,<span class="string">'info1:age'</span>,<span class="string">'22'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1002'</span>,<span class="string">'info2:addr'</span>,<span class="string">'wuhan'</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1003'</span>,<span class="string">'info2:addr'</span>,<span class="string">'beijing'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查</span></span><br><span class="line">scan <span class="string">'zxylearn:student'</span>,&#123;STARTROW=&gt;<span class="string">'1001'</span>,STOPROW=&gt;<span class="string">'1003'</span>&#125;</span><br><span class="line">get <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span></span><br><span class="line">get <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:name'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 改</span></span><br><span class="line">put <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:name'</span>,<span class="string">'zxy'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删</span></span><br><span class="line">delete <span class="string">'zxylearn:student'</span>,<span class="string">'1001'</span>,<span class="string">'info1:sex'</span></span><br><span class="line">deleteall <span class="string">'zxylearn:student'</span>, <span class="string">'1002'</span></span><br><span class="line">truncate <span class="string">'zxylearn:student'</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍hbase的安装与简单使用&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper-基本原理</title>
    <link href="http://yoursite.com/2019/09/24/zookeeper-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2019/09/24/zookeeper-基本原理/</id>
    <published>2019-09-24T08:24:14.000Z</published>
    <updated>2019-09-24T08:36:42.582Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了zookeeper的基本原理和使用</p><a id="more"></a><h1>简介</h1><p>ZooKeeper是个分布式的服务协调框架。具体用途如：<strong>统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理</strong>等。它基于<strong>观察者</strong>的设计模式；zookeeper = 文件系统 + 监听通知机制。</p><p>特点：</p><ol><li>最终一致性：Client不论连接到哪个Server，得到的数据都是一样的。</li><li>原子性：事务中的所有操作全部执行或者全部不执行。</li><li>顺序性：同一个client的请求顺序执行。</li><li>分区容错性：zookeeper是分布式的，所以在部分节点出现故障时，可以自己恢复。</li></ol><h1>数据结构</h1><p><strong>znode</strong>：zk是层级树状结构，一个节点就是znode。默认可存<strong>1M数据</strong>。</p><ul><li>操作节点时可设置<strong>watcher</strong>。当节点状态发生变化时，就会触发watcher对应的操作，只触发一次。</li><li><strong>永久节点</strong>：创建后永久存在，除非主动删除；<strong>临时节点</strong>：临时创建的，会话结束节点自动被删除</li><li><strong>顺序节点</strong>：节点名称后面自动增加一个10位数字的序列号；<strong>非顺序节点</strong>：不加序列号</li></ul><p>节点衍生出分类是为了迎合需求。临时节点可以记录服务器是否上线：当服务器下线，临时节点的数据消除。顺序节点可用于同一服务器多次上下线，每次名字都不同。很明显2个2分类，两两组合有4种节点。</p><p>下面是一个znode的数据结构：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个znode的数据结构</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] <span class="built_in">stat</span> /zxy</span><br><span class="line">cZxid = 0x39                                <span class="comment"># 节点创建时的zxid</span></span><br><span class="line">ctime = Mon Sep 23 16:38:23 CST 2019        <span class="comment"># 节点创建时的时间戳</span></span><br><span class="line">mZxid = 0x3d                                <span class="comment"># 节点修改时的zxid</span></span><br><span class="line">mtime = Mon Sep 23 16:38:44 CST 2019        <span class="comment"># 节点修改时的时间戳</span></span><br><span class="line">pZxid = 0x39                                <span class="comment"># 改变子节点时的zxid</span></span><br><span class="line">cversion = 0                                <span class="comment"># 子节点的版本号</span></span><br><span class="line">dataVersion = 3                             <span class="comment"># 数据版本：初始0，改变一次加1</span></span><br><span class="line">aclVersion = 0                              <span class="comment"># ACL版本（权限）</span></span><br><span class="line">ephemeralOwner = 0x0                        <span class="comment"># 数据拥有者：永久节点是0；临时节点是创建者的id</span></span><br><span class="line">dataLength = 13                             <span class="comment"># 数据长度</span></span><br><span class="line">numChildren = 0                             <span class="comment"># 子节点个数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对节点的操作无非是增删改查等，这里就不写了</span></span><br></pre></td></tr></table></figure><p>补充：</p><p><strong>zxid</strong>（ZooKeeper Transaction Id）：ZooKeeper每次状态变化将会产生一个叫zxid的时间戳。</p><h1>原理</h1><p>基本流程：</p><p>客户端发请求（可带watcher） -&gt;  zk 选举与恢复 (没leader时) -&gt; zk 读写数据 -&gt; 返回数据（可触发watcher）</p><p>两端主要是<strong>监听器的原理</strong>，中间主要用到<strong>ZAB协议</strong>。</p><h2 id="监听器原理"><a class="header-anchor" href="#监听器原理">¶</a>监听器原理</h2><p>watcher 相当于一个的炸弹，客户端发请求时：如<code>ls，get，set</code>等，可以给节点绑上炸弹。</p><p>如果触发了爆炸条件：<code>ls</code>就是该节点有增加删减子节点；<code>get set</code>就是该节点数据改变。</p><p>炸弹爆炸（只炸一次）也就是执行里面的回调函数。</p><p>很明显这里用户端至少需要启动两个线程：connect线程负责网络连接通信：绑炸弹并把任务发给zk与后续数据互传；listener线程监听炸弹爆炸信号。</p><p><img src="https://zouxxyy.s3-us-west-2.amazonaws.com/blog/zookeeper/zk.png" alt></p><h2 id="ZAB协议"><a class="header-anchor" href="#ZAB协议">¶</a>ZAB协议</h2><p>Zab协议（Zookeeper Atomic Broadcast），通过它来保证分布式事务的最终一致性。<br>这个内容很多，详细的可以参考：</p><p><a href="https://www.jianshu.com/p/2bceacd60b8a" target="_blank" rel="noopener">Zookeeper——一致性协议:Zab协议</a></p><p><a href="https://www.cnblogs.com/felixzh/p/5869212.html" target="_blank" rel="noopener">Zookeeper的功能以及工作原理</a></p><p>主要功能：崩溃恢复(选举、数据恢复) 的 原子广播 (数据读写)</p><h3 id="崩溃恢复"><a class="header-anchor" href="#崩溃恢复">¶</a>崩溃恢复</h3><p>集群中必须有一个leader，leader出现故障时，采用投票选举新leader，它需要满足以下条件：</p><ul><li>新选举出来的 Leader 不能包含未提交的 Proposal 。</li><li>新选举的 Leader 节点中含有最大的 zxid 。</li><li>得到超过一半选票者称为 Leader，因此<strong>zk集群个数为奇数</strong></li></ul><p>并不是所有节点都是 leader 和 follower ，还有observer，它不参与选举。作用是：可以增加集群数量，又减少投票选举时间。</p><p>选出leader后，进行数据恢复也就是同步，这个没啥，就是让它们其它节点数据都和leader同步，毕竟咱要确保<strong>最终一致性</strong>。恢复完毕后，就可以处理客户端的请求了。</p><h3 id="读写数据"><a class="header-anchor" href="#读写数据">¶</a>读写数据</h3><p><strong>一般流程：</strong></p><p><strong>读请求</strong>，就是直接从当前节点中读取数据</p><p><strong>写请求</strong></p><ol><li>客户端发起一个写操作请求。</li><li>Leader 将客户端的请求转化为事务（Proposal），每个 Proposal 分配一个全局的ID，即zxid。</li><li>Leader 为每个 Follower 分配一个单独的队列，然后将需要广播的 Proposal 依次放到队列中取，并且根据 FIFO 策略进行消息发送。</li><li>Follower 接收到 Proposal 后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 Ack 响应消息。</li><li>Leader 接收到<strong>超过半数以上</strong> Follower 的 Ack 响应消息后，即认为消息发送成功，可以发送 commit 消息。</li><li>Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交。Follower 接收到 commit 消息后，会将上一条事务提交。</li></ol><p><strong>有意思的点</strong></p><ul><li><p>**如何保证消息有序：**在整个消息广播中，Leader会将每一个事务请求转换成对应的 proposal 来进行广播，并且在广播 事务Proposal 之前，Leader服务器会首先为这个事务Proposal分配一个全局单递增的唯一ID，称之为事务ID（即zxid），由于Zab协议需要保证每一个消息的严格的顺序关系，因此必须将每一个proposal按照其zxid的先后顺序进行排序和处理。</p></li><li><p>**用队列提高效率：**Leader 服务器与每一个 Follower 服务器之间都维护了一个单独的 FIFO 消息队列进行收发消息，使用队列消息可以做到异步解耦。 Leader 和 Follower 之间只需要往队列中发消息即可。如果使用完全同步的方式会引起阻塞，性能要下降很多。(我感觉这里应该不是FIFO 消息队列，应该是最小队列吧)</p></li><li><p><strong>记住超过半数</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍了zookeeper的基本原理和使用&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="zookeeper" scheme="http://yoursite.com/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>hive-DML</title>
    <link href="http://yoursite.com/2019/09/19/hive-DML/"/>
    <id>http://yoursite.com/2019/09/19/hive-DML/</id>
    <published>2019-09-19T09:02:24.000Z</published>
    <updated>2019-09-21T03:54:42.372Z</updated>
    
    <content type="html"><![CDATA[<p>hive中一些DML的操作</p><a id="more"></a><p>DML（数据操纵语言）主要指数据的增删查改</p><h1>数据导入</h1><p>有5种导入数据的方法，最常用的是 <strong>Load</strong> 和 <strong>Insert</strong></p><h2 id="Load"><a class="header-anchor" href="#Load">¶</a>Load</h2><p>从文件系统中导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> [<span class="keyword">local</span>] inpath <span class="string">'/xxxxxx'</span> </span><br><span class="line">[overwrite] <span class="keyword">into</span> <span class="keyword">table</span> tablename [<span class="keyword">partition</span> (partcol1=val1,…)];</span><br></pre></td></tr></table></figure><ul><li>local: 指本地文件系统，否则为HDFS</li><li>overwrite: 指覆盖表中已有数据，否则表示追加</li><li>partition: 表示上传到指定分区</li></ul><h2 id="Insert"><a class="header-anchor" href="#Insert">¶</a>Insert</h2><p>通过查询语句导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 覆盖</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tablename [<span class="keyword">partition</span>(partcol1=val1,partclo2=val2)] select_statement;</span><br><span class="line"><span class="comment"># 追加</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> tablename [<span class="keyword">partition</span>(partcol1=val1,partclo2=val2)] select_statement;</span><br></pre></td></tr></table></figure><h2 id="As-Select"><a class="header-anchor" href="#As-Select">¶</a>As Select</h2><p>查询语句中创建表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> tablename</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><h2 id="Location"><a class="header-anchor" href="#Location">¶</a>Location</h2><p>创建表时通过Location指定数据的路径，再直接put数据到hdfs上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /xxxxx /user/hive/warehouse/tablename;</span><br></pre></td></tr></table></figure><h2 id="Import"><a class="header-anchor" href="#Import">¶</a>Import</h2><p>只能导入export导出的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import table tablename partition(month='201909') from '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><h1>数据导出</h1><p>最常用的是 <strong>Insert</strong> 和 <strong>Hadoop</strong></p><h2 id="Insert-v2"><a class="header-anchor" href="#Insert-v2">¶</a>Insert</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据导入本地（并格式化处理），不加local就是导入HDFS</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/xxxxx'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tablename;</span><br></pre></td></tr></table></figure><h2 id="Hadoop"><a class="header-anchor" href="#Hadoop">¶</a>Hadoop</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hadoop命令导出</span></span><br><span class="line">hive &gt; dfs -get /user/hive/warehouse/student/month=201909/000000_0 /xxxxxxx;</span><br></pre></td></tr></table></figure><h2 id="Export"><a class="header-anchor" href="#Export">¶</a>Export</h2><p>这个导出的数据除了数据还有元数据，可用Import导入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive &gt; <span class="built_in">export</span> table tablename to <span class="string">'/user/hive/warehouse/export/student'</span>;</span><br></pre></td></tr></table></figure><h1>数据清除</h1><p>只能清除内部表的数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive &gt; truncate table tablename;</span><br></pre></td></tr></table></figure><h1>查询</h1><p>查询的关键字较多，要知道它们的<strong>顺序</strong>（重点）</p><p>写的顺序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> ... <span class="keyword">join</span> <span class="keyword">on</span> ... <span class="keyword">where</span> ... <span class="keyword">group</span> <span class="keyword">by</span> ... <span class="keyword">having</span> ... <span class="keyword">order</span> <span class="keyword">by</span> ... <span class="keyword">limit</span> ...</span><br></pre></td></tr></table></figure><p>执行顺序：大体思路是 限定(where)，分组，限定（having），选择，排序</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from -&gt; join on -&gt; where -&gt; group by -&gt; having -&gt; select -&gt; order by -&gt; limit</span><br></pre></td></tr></table></figure><h2 id="select…where…limit"><a class="header-anchor" href="#select…where…limit">¶</a>select…where…limit</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单查询</span></span><br><span class="line"><span class="keyword">select</span> empno, ename <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) cnt <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal &gt;<span class="number">1000</span> <span class="keyword">limit</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure><h2 id="group-by-和-having"><a class="header-anchor" href="#group-by-和-having">¶</a>group by 和 having</h2><p>group by 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算emp表每个部门的平均工资</span></span><br><span class="line"><span class="keyword">select</span> t.deptno, <span class="keyword">avg</span>(t.sal) avg_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br></pre></td></tr></table></figure><p>where 作用在 分组（group by）和聚合（sum等）计算之前，选取哪些行，也就是在查询前筛选；having 对分组后<strong>计算的</strong>数据进行过滤。它<strong>只用于</strong>group by分组统计语句。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求每个部门的平均薪水大于2000的部门</span></span><br><span class="line"><span class="keyword">select</span> deptno, <span class="keyword">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal &gt; <span class="number">2000</span>;</span><br></pre></td></tr></table></figure><h2 id="join"><a class="header-anchor" href="#join">¶</a>join</h2><p>Hive支持通常的SQL JOIN语句，但是<strong>只支持等值连接，不支持非等值连接</strong>。 且 连接谓词中<strong>不支持or</strong>。<br>这个非等值连接可以从以前学的<strong>reducejoin</strong>的流程思索原因，reducejoin是在shuffer时将数据按关联值相等的（on的条件）分为一组，再在reducer阶段进行处理。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 合并员工表和部门表</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h2 id="排序"><a class="header-anchor" href="#排序">¶</a>排序</h2><h3 id="全局排序-Order-By"><a class="header-anchor" href="#全局排序-Order-By">¶</a>全局排序 Order By</h3><p>全局排序，只一个Reducer。全排很明显最后生成一个总的排序文件，<strong>1个reducer</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询员工信息按工资降序排列</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="按reducer排序-sort-by"><a class="header-anchor" href="#按reducer排序-sort-by">¶</a>按reducer排序 sort by</h3><p>每个reducer端都会做排序，出来的数据是有序的。假如有n个Reducer，就会生成n个有序文件。当n=1时，它就是<code>Order By</code>。</p><p>扩展一波，Reducer个数默认按原始数据256M一个，当然也可手动设置其个数。</p><h3 id="分区排序-Distribute-By…Sort-By"><a class="header-anchor" href="#分区排序-Distribute-By…Sort-By">¶</a>分区排序 Distribute By…Sort By</h3><p>先分区，后排序。这个分区类型mapreduce的分区，多少个分区，就有多少个reduce任务，后面就生成多少个文件。说白了这个和上面的区别就是它通过<code>Distribute By</code>指定怎么分区，即指定怎么分reducer。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置reduce个数</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br><span class="line"><span class="comment"># 先按照部门编号分区，再按照员工编号降序排序</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/Users/zxy/IdeaProjects/bigdata-learning/hive-learning/data/output'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="分桶-Cluster-By"><a class="header-anchor" href="#分桶-Cluster-By">¶</a>分桶 Cluster By</h3><p>当distribute by 和 sorts by <strong>字段相同</strong>时，可以使用cluster by方式。但<strong>排序只能是升序排序</strong>。</p><p>可以从取名看出，我没用分桶排序。你可以理解 Cluster 就是把数据分区，然后每个分区生成一个文件，这样就好解释为啥只能升序排序，我理解它压根就不需要排序，只是把数据分到不同区就ok。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下两种写法等价</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><p><strong>细节一：来波小结理一下  分区表 分区排序 分桶(前两个分区意思截然不同)</strong></p><ul><li><code>partition(month='201909')</code> 这个是分区表，针对的是数据的存储路径</li><li><code>Distribute By...Sort By</code> 这个是分区排序，和<strong>MR中的分区</strong>概念一样，多少个分区，就有多少个reduce任务，后面就生成多少个文件，分区之后对区里的数据进行排序。</li><li><code>Cluster By</code> 分桶，针对的是数据文件，将大的数据集分区。</li></ul><p>所以将 Cluster By 理解为分区， Distribute By…Sort By 理解为分区排序，岂不美哉</p><p><strong>细节二：注意导入数据到分桶中，要用insert，且 设置<code>hive.enforce.bucketing=true</code>和<code> hive.enforce.bucketing=true</code></strong></p><p><strong>细节三：分桶抽样查询</strong></p><p><code>select * from tablename tablesample(bucket x out of y on id);</code></p><p>x 表示从哪个bucket开始抽取</p><p>y 表示抽样间隔，共抽取 总数/y 个桶，且x的值必须<strong>小于等于</strong>y的值</p><p>举例：如果 x = 1, y = 4 ，共16个桶，那么将抽取16/4个桶，分别是 1、5、9、13</p><h2 id="行转列、列转行"><a class="header-anchor" href="#行转列、列转行">¶</a>行转列、列转行</h2><p><strong>行转列</strong>：将不同行的聚合到一起</p><p><code>collect_set(col)</code>：函数<strong>只接受基本数据类型</strong>，它的主要作用是将某字段的值进行<strong>去重汇总</strong>(不去重用list)，产生array类型字段</p><p>例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    base,</span><br><span class="line">    <span class="keyword">concat_ws</span>(<span class="string">"|"</span>, collect_set(<span class="keyword">name</span>)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) base</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    constellation) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    base;</span><br></pre></td></tr></table></figure><p><strong>列转行</strong>：将列拆分成多行。</p><p><code>explode(col)</code> 将hive一列中复杂的array或者map结构拆分成多行</p><p><code>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</code> 用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p><p>例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure><h2 id="窗口函数"><a class="header-anchor" href="#窗口函数">¶</a>窗口函数</h2><p>基本结构：函数 over(范围)  。用前面的函数处理over中的规定的数据</p><p>除了count、sum等一些常用函数，还有<strong>只能配合over使用</strong>的函数：</p><ul><li><code>lag(col,n)</code>：往前第n行数据</li><li><code>lead(col,n)</code>：往后第n行数据</li><li><code>ntile(n)</code>：给数据编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。</li><li><code>rank()</code> 排序相同时会重复，总数不会变，如12225668</li><li><code>dense_rank()</code> 排序相同时会重复，总数会减少，如12223445</li><li><code>row_rank()</code> 单纯顺序计算，如12345678</li></ul><p>over里面可以规定窗口范围：</p><ul><li><code>()</code>：全部数据</li><li><code>(partition by xxx order by xxx)</code>：分区有序</li><li><code>(rows between xxxx and xxxx)</code>：手动指定范围<ul><li><code>current row</code>：当前行</li><li><code>n preceding</code>：往前n行数据</li><li><code>n following</code>：往后n行数据</li><li><code>unbounded preceding</code>： 从起点开始</li><li><code>unbounded following</code>： 到终点结束</li></ul></li></ul><p>一些例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">business.namebusiness.orderdatebusiness.cost</span><br><span class="line">jack2017-01-0110</span><br><span class="line">tony2017-01-0215</span><br><span class="line">jack2017-02-0323</span><br><span class="line">tony2017-01-0429</span><br><span class="line">jack2017-01-0546</span><br><span class="line">jack2017-04-0642</span><br><span class="line">tony2017-01-0750</span><br><span class="line">jack2017-01-0855</span><br><span class="line">mart2017-04-0862</span><br><span class="line">mart2017-04-0968</span><br><span class="line">neil2017-05-1012</span><br><span class="line">mart2017-04-1175</span><br><span class="line">neil2017-06-1280</span><br><span class="line">mart2017-04-1394</span><br><span class="line"></span><br><span class="line"><span class="comment">#（1）查询在2017年4月份购买过的顾客及总人数，over()针对groupby后的全部数据</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    <span class="keyword">count</span>(*) <span class="keyword">over</span> () </span><br><span class="line"><span class="keyword">from</span> business </span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">    <span class="keyword">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) = <span class="string">'2017-04'</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">    <span class="keyword">name</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（2）查询顾客的购买明细 并 让cost按照日期进行累加</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>,</span><br><span class="line">    <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) </span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    business;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（3）查看顾客上次的购买时间，在窗口中分区排序</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>, </span><br><span class="line">    lag(orderdate, <span class="number">1</span>, <span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate)</span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（4）查询前20%时间的订单信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加分组号</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>, </span><br><span class="line">    ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line"><span class="keyword">from</span> business; t1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过滤出组号为1的数据</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>, </span><br><span class="line">    ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line"><span class="keyword">from</span> business) t1</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">    sorted = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（5）计算每个人消费的排名</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    orderdate,</span><br><span class="line">    <span class="keyword">cost</span>,</span><br><span class="line">    <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">cost</span> <span class="keyword">desc</span>)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    business;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;hive中一些DML的操作&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>hive-DDL</title>
    <link href="http://yoursite.com/2019/09/17/hive-DDL/"/>
    <id>http://yoursite.com/2019/09/17/hive-DDL/</id>
    <published>2019-09-17T12:29:08.000Z</published>
    <updated>2019-09-21T03:54:54.275Z</updated>
    
    <content type="html"><![CDATA[<p>hive中一些DDL的操作</p><a id="more"></a><h1>DDL</h1><p>DDL（数据定义语言）用来处理数据库中的各种对象，如数据库、表等</p><h2 id="数据库-database"><a class="header-anchor" href="#数据库-database">¶</a>数据库(database)</h2><h3 id="增"><a class="header-anchor" href="#增">¶</a>增</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个数据库，它在HDFS上的默认存储路径是/user/hive/warehouse/db_hive.db</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> db_hive;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 并 指定数据库在HDFS上存放的位置</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db_hive2 location <span class="string">'/db_hive2.db'</span>;</span><br></pre></td></tr></table></figure><h3 id="查"><a class="header-anchor" href="#查">¶</a>查</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示数据库</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示数据库详细信息</span></span><br><span class="line">desc database extended db_hive;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换数据库</span></span><br><span class="line"><span class="keyword">use</span> db_hive;</span><br></pre></td></tr></table></figure><h3 id="改"><a class="header-anchor" href="#改">¶</a>改</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改数据库</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> db_hive <span class="keyword">set</span> dbproperties(<span class="string">'createtime'</span>=<span class="string">'20190917'</span>);</span><br></pre></td></tr></table></figure><p>注意：数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。这个修改只是修改<code>DBPROPERTIES</code>里的键值对。</p><h3 id="删"><a class="header-anchor" href="#删">¶</a>删</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> db_hive;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 强制删除非空数据库</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> db_hive <span class="keyword">cascade</span>;</span><br></pre></td></tr></table></figure><h2 id="表-table"><a class="header-anchor" href="#表-table">¶</a>表(table)</h2><h3 id="增-v2"><a class="header-anchor" href="#增-v2">¶</a>增</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] <span class="comment">-- 注释</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] <span class="comment">-- 分区表</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="comment">-- 分桶表</span></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] <span class="comment">-- 不常用</span></span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] <span class="comment">-- 定义每行的格式</span></span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] <span class="comment">-- 指定存储文件类型</span></span><br><span class="line">[LOCATION hdfs_path] <span class="comment">-- 指定表在HDFS上的存储位置</span></span><br></pre></td></tr></table></figure><p><strong>一些细节：</strong></p><ul><li><strong>内部表与外部表</strong></li></ul><p><code>CREATE EXTERNAL TABLE</code> 用于创建外部表，默认是内部表（管理表）。它们的区别是 删除外部表并不会删除HDFS中的的数据，只会删除mysql中的元数据；而删除内部表都会删除。</p><p>可以这样理解外部表：HDFS上的数据是公有的，某个客户端建一了个hive表关联使用它，生成元数据，当该客户不用时，只删除他的元数据和hive表就行，公有数据仍然存在。</p><ul><li><strong>分区表</strong></li></ul><p>每个分区 对应一个HDFS文件系统上的独立的文件夹，该文件夹里包含该分区所有的数据。在查询时，可通过 WHERE 指定查询所需要的分区，查询效率会提高很多。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建表：</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test1(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据到指定分区：</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/xxxx'</span> <span class="keyword">into</span> <span class="keyword">table</span> test1 <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201909'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时，会数据会保存在 /user/hive/warehouse/test1/month=201909 中</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询指定分区的数据（可以直接将分区作为字段用）</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> test1 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201909'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二级分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test2(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li><strong>元数据和真实数据</strong></li></ul><p>元数据存在mysql中，包含数据库信息：ID、描述、HDFS路径、数据库名、所有者；分区信息；字段信息等等。而真实数据都存在HDFS中。</p><p><strong>它们可以自由独立存在</strong>，如外部表可以直接删除元数据。因此，创建数据时，如果直接放入分区中，由于元数据中没有分区信息，无法用where查到它，虽然数据存在。可以通过补充分区信息 或者 执行修复命令，让分区表和数据产生关联。</p><p>我的理解：先有数据，后有hive。hive要做的事就是关联到数据（生成元数据），然后CRUD它。</p><h3 id="改-v2"><a class="header-anchor" href="#改-v2">¶</a>改</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重命名</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">rename</span> <span class="keyword">to</span> new_table_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test1 <span class="keyword">add</span> <span class="keyword">columns</span>(newdesc <span class="keyword">string</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test1 <span class="keyword">change</span> <span class="keyword">column</span> <span class="keyword">id</span> <span class="keyword">desc</span> <span class="built_in">int</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test1 <span class="keyword">replace</span> <span class="keyword">columns</span>(<span class="keyword">name</span> <span class="keyword">string</span>, loc <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure><h3 id="查-v2"><a class="header-anchor" href="#查-v2">¶</a>查</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示表</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示表信息</span></span><br><span class="line">desc tablename;</span><br></pre></td></tr></table></figure><h3 id="删-v2"><a class="header-anchor" href="#删-v2">¶</a>删</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> test1;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;hive中一些DDL的操作&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
</feed>
